{
  "key_insight": "Pretrained video diffusion models have implicitly learned optical rules of transparency (refraction, reflection, transmission); by repurposing these models and training on a synthetic transparent/reflective video corpus, we can recover accurate depth and surface normals for transparent objects.",
  "problem_solved": "Traditional depth sensors and discriminative monocular models fail on transparent and reflective objects—producing holes, wrong values and temporally unstable estimates—this work tackles reliable depth and normal estimation for transparent/reflective scenes.",
  "method": [
    "Construct TransPhy3D: a large synthetic video corpus of transparent and reflective scenes (≈11k sequences) to expose models to realistic transparent phenomena.",
    "Repurpose pretrained video diffusion models as geometric priors—extracting or conditioning on internal representations to infer dense depth and surface normals for transparent objects.",
    "Train/finetune a geometry prediction head with temporal consistency constraints so outputs are dense, accurate and temporally stable across video frames."
  ],
  "impact": "Makes robust 3D perception of transparent and reflective objects practical for applications like robotics, AR/VR and 3D reconstruction by filling gaps left by stereo/ToF and standard monocular methods, improving accuracy and temporal stability.",
  "visual_elements": [
    "Before/after comparison: input RGB video → baseline depth (holes/artifacts) → TransPhy3D+diffusion depth & normals",
    "Pipeline diagram: pretrained video diffusion model + TransPhy3D → geometry extraction/finetuning → depth & normal outputs",
    "Dataset snapshot: montage of synthetic scenes showing refraction/reflection effects and a small chart with dataset size (≈11k sequences)",
    "Qualitative temporal sequence: frame-by-frame depth/normal consistency on a moving transparent object"
  ],
  "hashtags": [
    "#TransparentObjects",
    "#DepthEstimation",
    "#VideoDiffusion",
    "#3DPerception",
    "#ComputerVision"
  ],
  "paper_id": "2025-12-29-arxiv-diffusion_knows_transparency_repurposing_video_diffusion_for_transparent_object",
  "paper_title": "Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation",
  "paper_category": "Model",
  "paper_date": "2025-12-29",
  "paper_authors": "Shaocong Xu, Songlin Wei, Qizhe Wei, Zheng Geng, Hong Li",
  "paper_links": [
    {
      "title": "PDF",
      "url": "https://arxiv.org/pdf/2512.23705v1"
    }
  ],
  "generated_at": "2025-12-31T06:22:49.320749"
}