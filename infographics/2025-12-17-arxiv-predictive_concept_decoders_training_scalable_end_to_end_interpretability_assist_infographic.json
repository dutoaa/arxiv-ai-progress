{
  "key_insight": "Train small end-to-end \"interpretability assistants\" (Predictive Concept Decoders) that decode model activations into human-interpretable concepts which directly predict the model's behavior, producing more scalable and faithful explanations than hand-designed hypothesis testers.",
  "problem_solved": "Scaling faithful interpretation of complex internal activations: replaces slow, manual hypothesis-generation and testing with a learnable, end-to-end objective that links activation space to actual model behavior.",
  "method": [
    "Introduce Predictive Concept Decoders (PCDs) that map internal activations to concept scores used to predict the model's outputs/behavior.",
    "Train PCDs end-to-end with an objective that aligns concept predictions with the model's behavior (fidelity to outputs and behaviorally-relevant metrics).",
    "Use the learned decoders for scalable analysis, faithfulness evaluation, and targeted interventions/attribution on the base model."
  ],
  "impact": "Provides practitioners a scalable, trainable tool to extract behaviorally-faithful concepts from internal activations, improving debugging, transparency, and the ability to perform meaningful interventions on large models.",
  "visual_elements": [
    "Pipeline diagram: base model → internal activations → Predictive Concept Decoder → predicted behavior/output",
    "Bar/line chart comparing fidelity (predictive accuracy of behavior) of PCDs vs hand-designed interpretability agents",
    "Concept activation heatmaps over inputs showing which concepts drive specific model outputs",
    "Intervention schematic visualizing concept-level intervention and its effect on model behavior"
  ],
  "hashtags": [
    "#Interpretability",
    "#ExplainableAI",
    "#PredictiveConceptDecoders",
    "#ConceptBottleneck",
    "#ModelTransparency"
  ],
  "paper_id": "2025-12-17-arxiv-predictive_concept_decoders_training_scalable_end_to_end_interpretability_assist",
  "paper_title": "Predictive Concept Decoders: Training Scalable End-to-End Interpretability Assistants",
  "paper_category": "Model",
  "paper_date": "2025-12-17",
  "paper_authors": "Vincent Huang, Dami Choi, Daniel D. Johnson, Sarah Schwettmann, Jacob Steinhardt",
  "paper_links": [
    {
      "title": "PDF",
      "url": "https://arxiv.org/pdf/2512.15712v1"
    }
  ],
  "generated_at": "2025-12-19T06:23:17.557187"
}