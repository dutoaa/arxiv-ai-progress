{
  "key_insight": "A practical method to convert pretrained autoregressive (AR) language models into diffusion-based vision-language models, unlocking diffusion decoding advantages while reusing strong AR capabilities. This bridges the performance gap between dVLMs and mainstream AR-based VLMs without training diffusion LMs from scratch.",
  "problem_solved": "dVLMs lag behind AR-based VLMs because base diffusion language models lack capability; the paper addresses how to leverage existing powerful AR models to build competitive diffusion vision-language models.",
  "method": [
    "Translate AR model knowledge into a diffusion decoder via weight transfer and targeted adapters/distillation so the AR semantics are preserved in a diffusion framework.",
    "Align multimodal latents by integrating a visual encoder with cross-attention and joint training to map image features into the diffusion text latent space.",
    "Optimize with diffusion-specific objectives and guidance (noise scheduling, classifier-free guidance) to maintain AR performance while enabling parallel/diverse diffusion decoding."
  ],
  "impact": "Enables practitioners to reuse strong pretrained AR LMs for diffusion VLMs, making diffusion-based multimodal models more competitive and practical — offering parallel decoding, greater output diversity, and simpler model reuse.",
  "visual_elements": [
    "Architecture diagram showing AR model → adapter/distillation → diffusion decoder with visual encoder and cross-attention",
    "Pipeline flowchart: pretrain AR model → translation step → diffusion VLM inference (parallel sampling)",
    "Bar chart comparing performance (accuracy/score) of baseline dVLM, translated dVLM, and AR VLM",
    "Sample outputs grid showing diversity/quality differences between AR and translated diffusion sampling"
  ],
  "hashtags": [
    "#DiffusionModels",
    "#VisionLanguage",
    "#ModelConversion",
    "#MultimodalAI",
    "#AIResearch"
  ],
  "paper_id": "2025-12-17-arxiv-diffusionvl_translating_any_autoregressive_models_into_diffusion_vision_language",
  "paper_title": "DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models",
  "paper_category": "Model",
  "paper_date": "2025-12-17",
  "paper_authors": "Lunbin Zeng, Jingfeng Yao, Bencheng Liao, Hongyuan Tao, Wenyu Liu",
  "paper_links": [
    {
      "title": "PDF",
      "url": "https://arxiv.org/pdf/2512.15713v1"
    }
  ],
  "generated_at": "2025-12-18T22:54:05.210405"
}