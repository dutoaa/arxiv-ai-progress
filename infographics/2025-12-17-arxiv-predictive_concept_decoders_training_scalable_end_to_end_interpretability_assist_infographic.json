{
  "key_insight": "Turn interpretability into a supervised learning problem: train small auxiliary models (Predictive Concept Decoders) end-to-end to predict a model's behavior from its internal activations, yielding more scalable and faithful explanations than hand-designed agents.",
  "problem_solved": "Interpreting high-dimensional neural activations is hard and current scalable approaches rely on manual, probe-and-test agents that don't generalize; this paper automates and scales that process by training interpretable assistants to directly predict model behavior from activations.",
  "method": [
    "Train Predictive Concept Decoders (PCDs) end-to-end to map internal activations to model outputs or human-understandable concepts, using the model's behavior as the supervision signal.",
    "Optimize decoders for accurate prediction of the target model's behavior to prioritize faithfulness over hand-crafted heuristics.",
    "Use learned concept mappings to generate explanations, test hypotheses, and scale interpretability across models and layers."
  ],
  "impact": "Provides a practical, scalable way for practitioners to extract faithful, testable explanations from internal activations—useful for debugging, auditing, and aligning large models without bespoke interpretability pipelines.",
  "visual_elements": [
    "Pipeline diagram: model activations → PCDs → predicted behaviors / concepts (show end-to-end training loop)",
    "Before vs After comparison: hand-designed agent workflow vs learned PCD workflow (emphasize scalability)",
    "Activation-to-concept heatmap or attention overlay showing which neurons/features map to predicted concepts",
    "Bar/line chart of faithfulness (prediction accuracy) and scalability metrics across layers or model sizes"
  ],
  "hashtags": [
    "#Interpretability",
    "#ExplainableAI",
    "#ModelUnderstanding",
    "#PredictiveConceptDecoders",
    "#AITransparency"
  ],
  "paper_id": "2025-12-17-arxiv-predictive_concept_decoders_training_scalable_end_to_end_interpretability_assist",
  "paper_title": "Predictive Concept Decoders: Training Scalable End-to-End Interpretability Assistants",
  "paper_category": "Model",
  "paper_date": "2025-12-17",
  "paper_authors": "Vincent Huang, Dami Choi, Daniel D. Johnson, Sarah Schwettmann, Jacob Steinhardt",
  "paper_links": [
    {
      "title": "PDF",
      "url": "https://arxiv.org/pdf/2512.15712v1"
    }
  ],
  "generated_at": "2025-12-19T03:56:14.405572"
}