{
  "key_insight": "Turn interpretability into a supervised prediction problem: train lightweight ‘‘Predictive Concept Decoders’’ that map internal activations to model behavior, enabling scalable, end-to-end learned explanations instead of hand-designed hypothesis tests.",
  "problem_solved": "Internal activation space is high-dimensional and structured in ways that make manual hypothesis generation and testing slow, brittle, and hard to scale; this work automates and scales the extraction of meaningful, behavior-predictive concepts from activations.",
  "method": [
    "Attach small concept decoders to internal layers and train them end-to-end to predict the model's behavior (outputs, intermediate decisions, or human-labeled concepts) from activations.",
    "Use a predictive objective (fidelity to model behavior) rather than hand-crafted probes, allowing decoders to discover concepts that are both interpretable and behaviorally relevant.",
    "Apply the approach across layers/models to produce a library of scalable interpretability assistants for hypothesis generation, debugging, and audits."
  ],
  "impact": "Provides practitioners a scalable, trainable toolkit for faithful explanations that speed debugging, model audits, and safety analyses by surfacing concepts directly tied to model behavior rather than relying on ad-hoc probes.",
  "visual_elements": [
    "Schematic: neural network with small decoder modules tapping activations and outputting predicted behaviors (showing end-to-end training arrow).",
    "Flowchart: training pipeline from activations → decoder → predictive loss (fidelity) → updated decoder weights.",
    "Comparison chart: fidelity/utility of learned decoders vs. hand-designed probes or baseline interpretability methods.",
    "Concept map or t-SNE: clusters of activation-space concepts discovered by decoders with example inputs/labels."
  ],
  "hashtags": [
    "#Interpretability",
    "#ExplainableAI",
    "#PredictiveConceptDecoders",
    "#ModelDebugging",
    "#XAI"
  ],
  "paper_id": "2025-12-17-arxiv-predictive_concept_decoders_training_scalable_end_to_end_interpretability_assist",
  "paper_title": "Predictive Concept Decoders: Training Scalable End-to-End Interpretability Assistants",
  "paper_category": "Model",
  "paper_date": "2025-12-17",
  "paper_authors": "Vincent Huang, Dami Choi, Daniel D. Johnson, Sarah Schwettmann, Jacob Steinhardt",
  "paper_links": [
    {
      "title": "PDF",
      "url": "https://arxiv.org/pdf/2512.15712v1"
    }
  ],
  "generated_at": "2025-12-18T22:18:18.658838"
}