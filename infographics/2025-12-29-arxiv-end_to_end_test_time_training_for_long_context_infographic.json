{
  "key_insight": "Treat long-context language modeling as a continual learning problem: a standard sliding-window Transformer continues learning at test time via next-token prediction to compress context into its weights, with meta-learning at training time to prepare the model for fast adaptation.",
  "problem_solved": "Enables models to effectively use arbitrarily long context without designing new attention architectures or blowing up memory/compute, by adapting model weights on-the-fly to absorb context information.",
  "method": [
    "Base model: standard Transformer with sliding-window attention to keep compute and memory bounded.",
    "Test-time training (TTT): continue training on the provided context via next-token prediction so the model ‘compresses’ context into its parameters.",
    "Meta-learning at training time: optimize initialization so the model can rapidly and stably adapt during TTT at inference."
  ],
  "impact": "Provides a practical, architecture-agnostic path to scale context length and improve inference-time adaptation using existing models and modest compute, useful for long-document understanding and dynamic contexts.",
  "visual_elements": [
    "Two-panel pipeline diagram: (A) meta-training loop optimizing initialization, (B) test-time training loop compressing incoming context into model weights before final prediction.",
    "Comparison chart: performance vs context length for baseline (no TTT) vs sliding-window + TTT, showing sustained gains at long ranges.",
    "Schematic showing sliding-window attention with a ‘weight compression’ arrow from context tokens into model parameters (iconic representation of continual learning).",
    "Timeline/flow graphic showing: incoming long document → sliding-window reads chunks → local TTT updates → final prediction, annotated with compute/memory notes."
  ],
  "hashtags": [
    "#TestTimeTraining",
    "#LongContext",
    "#ContinualLearning",
    "#MetaLearning",
    "#Transformers"
  ],
  "paper_id": "2025-12-29-arxiv-end_to_end_test_time_training_for_long_context",
  "paper_title": "End-to-End Test-Time Training for Long Context",
  "paper_category": "Model",
  "paper_date": "2025-12-29",
  "paper_authors": "Arnuv Tandon, Karan Dalal, Xinhao Li, Daniel Koceja, Marcel Rød",
  "paper_links": [
    {
      "title": "PDF",
      "url": "https://arxiv.org/pdf/2512.23675v1"
    }
  ],
  "generated_at": "2025-12-31T06:22:08.375894"
}