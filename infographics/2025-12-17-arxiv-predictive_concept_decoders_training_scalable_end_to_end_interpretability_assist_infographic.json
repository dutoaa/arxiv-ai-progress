{
  "key_insight": "Train end-to-end \"interpretability assistants\"—Predictive Concept Decoders (PCDs)—that learn to map internal activations to model behavior, producing scalable, faithful explanations without hand-designed hypothesis testers.",
  "problem_solved": "Automates and scales the hard task of relating complex internal activation space to external model behavior, replacing slow, hand-crafted interpretability agents with trained predictors that directly capture useful concepts.",
  "method": [
    "Introduce Predictive Concept Decoders (PCDs) that consume hidden activations and predict model behavior or human-understandable concept signals.",
    "Train PCDs end-to-end with objectives that maximize predictive fidelity to the base model, aligning learned concepts with downstream behavior.",
    "Evaluate assistants on fidelity, scalability, and usefulness compared to hand-designed interpretability agents."
  ],
  "impact": "Provides AI/ML practitioners a scalable, trainable tool to obtain more faithful internal explanations for debugging, auditing, and alignment workflows, reducing manual effort and improving interpretability fidelity.",
  "visual_elements": [
    "Architecture diagram: base model → hidden activations → PCD → predicted behavior/concepts",
    "Training flowchart showing end-to-end objective and data flow (activations + labels → loss → PCD updates)",
    "Comparison chart: fidelity/coverage of PCDs vs hand-designed agents across model sizes",
    "Activation-to-concept heatmaps or example concept tokens linked to model outputs"
  ],
  "hashtags": [
    "#Interpretability",
    "#ExplainableAI",
    "#ConceptLearning",
    "#ModelDebugging",
    "#AIAlignment"
  ],
  "paper_id": "2025-12-17-arxiv-predictive_concept_decoders_training_scalable_end_to_end_interpretability_assist",
  "paper_title": "Predictive Concept Decoders: Training Scalable End-to-End Interpretability Assistants",
  "paper_category": "Model",
  "paper_date": "2025-12-17",
  "paper_authors": "Vincent Huang, Dami Choi, Daniel D. Johnson, Sarah Schwettmann, Jacob Steinhardt",
  "paper_links": [
    {
      "title": "PDF",
      "url": "https://arxiv.org/pdf/2512.15712v1"
    }
  ],
  "generated_at": "2025-12-21T06:22:25.577010"
}