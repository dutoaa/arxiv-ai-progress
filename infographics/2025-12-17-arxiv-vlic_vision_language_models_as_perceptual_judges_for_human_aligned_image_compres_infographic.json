{
  "key_insight": "State-of-the-art vision-language models (VLMs) can reliably replicate binary human preference judgments about compressed images, enabling them to serve as perceptual judges for human-aligned image compression.",
  "problem_solved": "Bridging the gap between distortion-based compression objectives (e.g., MSE) and actual human visual preferences without requiring prohibitively large psychophysical datasets.",
  "method": [
    "Use a pretrained VLM to compare original vs. compressed images and predict binary human preference (A/B) or a preference score.",
    "Validate VLM predictions against human judgment datasets to measure agreement and calibrate the model as a perceptual metric.",
    "Incorporate the VLM-based perceptual signal as an objective or evaluation metric when training and selecting compression models to improve human alignment."
  ],
  "impact": "Provides a scalable, data-efficient route to evaluate and train image compression models that align with human perception, reducing reliance on expensive human studies and improving perceived quality at given bitrates.",
  "visual_elements": [
    "Pipeline diagram: input image → compressor → compressed image → VLM judge → preference score (with optional human-in-the-loop icon)",
    "Agreement chart: bar/line plot showing VLM vs. human A/B agreement across datasets and compression levels",
    "Before/after image grid: same scene compressed with MSE vs. VLM-guided model, annotated with preference percentages",
    "Tradeoff plot: bitrate vs. perceived quality (human and VLM curves) to show alignment gains"
  ],
  "hashtags": [
    "#VisionLanguageModels",
    "#ImageCompression",
    "#PerceptualLoss",
    "#HumanAlignedAI",
    "#VLIC"
  ],
  "paper_id": "2025-12-17-arxiv-vlic_vision_language_models_as_perceptual_judges_for_human_aligned_image_compres",
  "paper_title": "VLIC: Vision-Language Models As Perceptual Judges for Human-Aligned Image Compression",
  "paper_category": "Model",
  "paper_date": "2025-12-17",
  "paper_authors": "Kyle Sargent, Ruiqi Gao, Philipp Henzler, Charles Herrmann, Aleksander Holynski",
  "paper_links": [
    {
      "title": "PDF",
      "url": "https://arxiv.org/pdf/2512.15701v1"
    }
  ],
  "generated_at": "2025-12-18T23:39:22.933704"
}