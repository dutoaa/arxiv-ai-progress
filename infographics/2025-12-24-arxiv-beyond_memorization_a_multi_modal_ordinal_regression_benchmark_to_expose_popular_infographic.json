{
  "key_insight": "State-of-the-art vision-language models exhibit a strong popularity bias—achieving up to 34% higher accuracy on famous buildings than ordinary ones—indicating reliance on memorization rather than true, generalizable visual-language understanding.",
  "problem_solved": "Exposes and quantifies popularity-driven memorization in VLMs for ordinal regression tasks and provides a large, open benchmark (YearGuessr) to systematically evaluate and study this bias.",
  "method": [
    "Built YearGuessr: a benchmark of 55,546 building images from 157 countries with multi-modal metadata and continuous ordinal labels (e.g., year/age).",
    "Evaluated popular vision-language models on an ordinal regression task and contrasted performance between 'famous' (iconic) and 'ordinary' buildings.",
    "Performed targeted analyses to quantify the popularity gap (up to 34% accuracy difference) and attribute errors to memorization versus generalization failures."
  ],
  "impact": "Alerts AI/ML practitioners that VLM performance can be inflated by memorization of popular examples; YearGuessr enables auditing, more robust evaluation, and development of debiasing strategies for real-world deployment.",
  "visual_elements": [
    "Bar chart comparing model accuracy on famous vs ordinary buildings (highlighting the ~34% gap).",
    "World map heatmap showing YearGuessr coverage across 157 countries with image counts.",
    "Dataset summary card: total images (55,546), modal attributes, and example image grid (famous vs ordinary).",
    "Schematic contrasting memorization (model recognizes iconic landmarks) vs generalization (model infers attributes of non-iconic buildings)."
  ],
  "hashtags": [
    "#PopularityBias",
    "#VisionLanguage",
    "#Benchmark",
    "#OrdinalRegression",
    "#YearGuessr"
  ],
  "paper_id": "2025-12-24-arxiv-beyond_memorization_a_multi_modal_ordinal_regression_benchmark_to_expose_popular",
  "paper_title": "Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models",
  "paper_category": "Evaluation",
  "paper_date": "2025-12-24",
  "paper_authors": "Li-Zhong Szu-Tu, Ting-Lin Wu, Chia-Jui Chang, He Syu, Yu-Lun Liu",
  "paper_links": [
    {
      "title": "PDF",
      "url": "https://arxiv.org/pdf/2512.21337v1"
    }
  ],
  "generated_at": "2025-12-25T06:24:02.874648"
}