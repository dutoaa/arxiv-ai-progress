{
  "key_insight": "Edit3r is a feed-forward model that produces photorealistic, instruction-aligned 3D scene edits from a handful of unposed, view-inconsistent images in a single passâ€”no per-scene optimization or camera poses required.",
  "problem_solved": "How to reconstruct and edit coherent 3D scenes when only sparse, unposed, and view-inconsistent edited images are available, avoiding slow per-scene optimization and pose estimation.",
  "method": [
    "Feed-forward encoder-decoder that consumes unposed image set + edit instruction and directly predicts a 3D edit and renderable scene representation in one pass.",
    "Training strategies to overcome lack of multi-view edited supervision (e.g., segmentation-guided consistency, synthetic multi-view augmentation, and consistency/photorealism losses) so predicted edits are view-consistent.",
    "Render photorealistic multi-view outputs without requiring camera poses or per-scene optimization, enabling instant inference."
  ],
  "impact": "Enables fast, scalable 3D scene editing workflows for content creation and vision tasks by removing expensive optimization and pose-recovery steps; useful for real-time applications, AR/VR content pipelines, and rapid prototyping.",
  "visual_elements": [
    "Pipeline diagram: unposed image set + text instruction -> Edit3r feed-forward network -> edited 3D scene + multi-view renders",
    "Before / after gallery: sparse input views vs. photorealistic multi-view renders of the edited scene",
    "Comparison chart: latency and workflow steps (per-scene optimization + pose estimation) vs. Edit3r (single-pass zero-optimization)",
    "Architecture block: training tricks visual (SAM-guided masks, synthetic multi-view augmentation, consistency losses)"
  ],
  "hashtags": [
    "#Edit3r",
    "#3DEditing",
    "#NeuralRendering",
    "#ViewSynthesis",
    "#RealTime3D"
  ],
  "paper_id": "2025-12-31-arxiv-edit3r_instant_3d_scene_editing_from_sparse_unposed_images",
  "paper_title": "Edit3r: Instant 3D Scene Editing from Sparse Unposed Images",
  "paper_category": "Evaluation",
  "paper_date": "2025-12-31",
  "paper_authors": "Jiageng Liu, Weijie Lyu, Xueting Li, Yejie Guo, Ming-Hsuan Yang",
  "paper_links": [
    {
      "title": "PDF",
      "url": "https://arxiv.org/pdf/2512.25071v1"
    }
  ],
  "generated_at": "2026-01-02T06:23:10.930687"
}