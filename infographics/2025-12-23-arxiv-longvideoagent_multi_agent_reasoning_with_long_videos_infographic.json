{
  "key_insight": "A coordinated multi-agent architecture where a master LLM directs a grounding agent to localize relevant temporal segments and a vision agent to extract targeted textual observations, enabling precise, evidence-backed reasoning over hour-long videos without lossy summarization.",
  "problem_solved": "Addresses the inability of many long-video QA systems to maintain temporal grounding and fine-grained cues—current methods often compress videos into lossy summaries or use limited toolsets, which weakens answer fidelity for hour-long episodes.",
  "method": [
    "Master LLM coordinator orchestrates the workflow: interprets the question, issues sub-tasks, and composes the final answer with evidence.",
    "Grounding agent performs temporal localization to find short, question-relevant segments within long videos (avoids full-video summarization).",
    "Vision agent extracts targeted textual observations from localized clips (e.g., OCR, object/action captions, scene details) and returns concise evidence for the master to reason over."
  ],
  "impact": "Enables more accurate, temporally grounded QA and interpretable evidence retrieval on hour-long videos, improving reliability for applications like video analytics, media understanding, and automated summarization. Encourages modular multi-agent designs for multimodal LLM toolchains.",
  "visual_elements": [
    "System architecture diagram showing Master LLM → Grounding Agent (temporal localization) → Vision Agent (textual observation extraction) → Answer composition.",
    "Horizontal video timeline with highlighted localized segments corresponding to a sample question (visualizes temporal grounding).",
    "End-to-end example pipeline: question → localized clip thumbnails → extracted text snippets (OCR/captions) → final answer with cited timestamps.",
    "Comparison chart (bar or line) showing accuracy/temporal grounding vs. baseline summary-based methods."
  ],
  "hashtags": [
    "#LongVideoAgent",
    "#MultimodalLLM",
    "#VideoQA",
    "#MultiAgent",
    "#TemporalReasoning"
  ],
  "paper_id": "2025-12-23-arxiv-longvideoagent_multi_agent_reasoning_with_long_videos",
  "paper_title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos",
  "paper_category": "Project",
  "paper_date": "2025-12-23",
  "paper_authors": "Runtao Liu, Ziyi Liu, Jiaqi Tang, Yue Ma, Renjie Pi",
  "paper_links": [
    {
      "title": "PDF",
      "url": "https://arxiv.org/pdf/2512.20618v1"
    }
  ],
  "generated_at": "2025-12-27T06:24:11.670319"
}