{
  "key_insight": "State-of-the-art vision–language models exhibit a large popularity bias: they score up to 34% higher accuracy on famous buildings than ordinary ones, revealing reliance on memorization rather than generalizable understanding.",
  "problem_solved": "Identifies and quantifies popularity-driven memorization in VLMs for ordinal prediction tasks and provides a standardized benchmark and dataset to measure and expose this bias.",
  "method": [
    "Assemble YearGuessr: a large multi-modal dataset (55,546 building images, multi-modal attributes from 157 countries) annotated with continuous ordinal labels (e.g., year/age).",
    "Evaluate popular VLMs on an ordinal regression protocol, comparing performance on famous vs. ordinary buildings to measure accuracy gaps.",
    "Introduce benchmark metrics and an evaluation pipeline to quantify popularity bias in model predictions and generalization."
  ],
  "impact": "Provides a practical benchmark and evidence that popular VLMs may over-rely on memorized iconic examples, prompting researchers and practitioners to adopt evaluation protocols and training methods that improve fairness and true generalization.",
  "visual_elements": [
    "Bar chart comparing model accuracy on famous vs. ordinary buildings (highlighting the ~34% gap).",
    "World map heatmap showing geographic distribution of YearGuessr images across 157 countries.",
    "Dataset sample grid: paired examples of famous and ordinary buildings with their ordinal labels (e.g., year).",
    "Pipeline diagram: YearGuessr dataset → model evaluation (ordinal regression) → bias metric and analysis."
  ],
  "hashtags": [
    "#VisionLanguage",
    "#PopularityBias",
    "#OrdinalRegression",
    "#YearGuessr",
    "#Dataset"
  ],
  "paper_id": "2025-12-24-arxiv-beyond_memorization_a_multi_modal_ordinal_regression_benchmark_to_expose_popular",
  "paper_title": "Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models",
  "paper_category": "Evaluation",
  "paper_date": "2025-12-24",
  "paper_authors": "Li-Zhong Szu-Tu, Ting-Lin Wu, Chia-Jui Chang, He Syu, Yu-Lun Liu",
  "paper_links": [
    {
      "title": "PDF",
      "url": "https://arxiv.org/pdf/2512.21337v1"
    }
  ],
  "generated_at": "2025-12-28T06:21:59.563221"
}