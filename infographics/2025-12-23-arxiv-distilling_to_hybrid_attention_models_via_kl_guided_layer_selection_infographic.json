{
  "key_insight": "Use per-layer KL divergence as a cheap importance score to decide which Transformer layers can be converted from softmax attention to linear attention, enabling distilled hybrid models that keep accuracy while improving inference efficiency.",
  "problem_solved": "How to choose which attention layers in a pretrained Transformer to convert to cheaper linear-attention variants during distillation without expensive re-pretraining or large accuracy loss.",
  "method": [
    "Compute a KL-guided importance score per layer (e.g., KL between teacher and candidate-student layer outputs/attention) to rank layers by sensitivity.",
    "Convert low-importance layers to linear-attention variants while keeping high-importance layers as softmax attention, producing an interleaved hybrid architecture.",
    "Apply knowledge distillation / fine-tuning on the hybrid model to recover performance with reduced compute and memory at inference."
  ],
  "impact": "Provides a simple, scalable recipe to retrofit pretrained Transformers into faster, lower-memory hybrid attention models with minimal overhead — practical for deploying efficient LLMs without full re-pretraining.",
  "visual_elements": [
    "Architecture diagram: original all-softmax Transformer vs hybrid interleaved softmax+linear layers (color-coded layers).",
    "Per-layer bar chart: KL importance score per layer with a threshold indicating converted vs retained layers.",
    "Trade-off curve: inference latency / memory vs accuracy (or perplexity) comparing original, fully-linear, and KL-guided hybrid models.",
    "Flowchart: KL scoring → layer ranking → conversion → distillation/fine-tuning (simple 4-step pipeline)."
  ],
  "hashtags": [
    "#ModelCompression",
    "#KnowledgeDistillation",
    "#EfficientLLMs",
    "#AttentionMechanisms",
    "#KLGuided"
  ],
  "paper_id": "2025-12-23-arxiv-distilling_to_hybrid_attention_models_via_kl_guided_layer_selection",
  "paper_title": "Distilling to Hybrid Attention Models via KL-Guided Layer Selection",
  "paper_category": "Model",
  "paper_date": "2025-12-23",
  "paper_authors": "Yanhong Li, Songlin Yang, Shawn Tan, Mayank Mishra, Rameswar Panda",
  "paper_links": [
    {
      "title": "PDF",
      "url": "https://arxiv.org/pdf/2512.20569v1"
    }
  ],
  "generated_at": "2025-12-26T06:23:58.656156"
}