{
  "key_insight": "WorldCanvas fuses text, explicit trajectories (motion, timing, visibility), and reference images to produce promptable, visually grounded event simulations—giving fine-grained control over identity, motion, and scene consistency in generated videos.",
  "problem_solved": "Bridges the gap between text-only event generation and trajectory-only image-to-video methods by providing semantic intent, motion/timing control, and visual identity grounding, enabling coherent, controllable multi-frame event synthesis.",
  "method": [
    "Encode trajectories that specify motion paths, timing, and visibility cues for objects across frames.",
    "Fuse multimodal inputs—natural language for intent, reference images for appearance grounding, and trajectories for dynamic control—into a generative backbone.",
    "Decode into temporally consistent image sequences (events) that respect the provided identity, motion, and timing constraints."
  ],
  "impact": "Gives practitioners a practical interface for fine-grained, grounded video/event generation useful for simulation, content creation, data generation, and robotics—improving controllability and realism compared to single-modality approaches.",
  "visual_elements": [
    "Pipeline diagram showing inputs (text, trajectory, reference image) → fusion module → generated frames/video",
    "Before/after sequence: reference image + trajectory overlay next to the synthesized frame sequence to highlight identity and motion preservation",
    "Timeline visualization of trajectory encoding showing timing and visibility (appear/disappear) across frames",
    "Prompt UI mockup illustrating how a user composes text + selects reference image + draws trajectory"
  ],
  "hashtags": [
    "#Multimodal",
    "#GenerativeAI",
    "#VideoSynthesis",
    "#TrajectoryControl",
    "#Simulation"
  ],
  "paper_id": "2025-12-18-arxiv-the_world_is_your_canvas_painting_promptable_events_with_reference_images_trajec",
  "paper_title": "The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text",
  "paper_category": "Model",
  "paper_date": "2025-12-18",
  "paper_authors": "Hanlin Wang, Hao Ouyang, Qiuyu Wang, Yue Yu, Yihao Meng",
  "paper_links": [
    {
      "title": "PDF",
      "url": "https://arxiv.org/pdf/2512.16924v1"
    }
  ],
  "generated_at": "2025-12-19T06:22:04.769459"
}