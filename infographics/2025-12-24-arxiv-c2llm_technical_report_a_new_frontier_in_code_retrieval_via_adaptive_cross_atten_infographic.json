{
  "key_insight": "C2LLM applies an adaptive cross-attention / Pooling by Multihead Attention (PMA) module to convert LLM causal token representations into rich sequence embeddings, aggregating information from all tokens and removing the EOS-based bottleneck. The family includes 0.5B and 7B contrastively trained models built on Qwen-2.5-Coder for improved code retrieval.",
  "problem_solved": "Standard EOS pooling wastes rich token-level causal representations from pretrained LLMs and limits retrieval quality; C2LLM addresses the information bottleneck by directly pooling across tokens to produce stronger code embeddings for retrieval.",
  "method": [
    "Use Qwen-2.5-Coder backbones and provide model variants at 0.5B and 7B scales.",
    "Introduce Pooling by Multihead Attention (PMA) / adaptive cross-attention pooling to aggregate token embeddings into a single sequence embedding that attends to all tokens.",
    "Train embeddings contrastively so code and query vectors are aligned for retrieval tasks."
  ],
  "impact": "Produces denser, more informative code embeddings that improve retrieval accuracy while preserving pretrained LLM knowledge, enabling better code search, recommendation, and downstream code understanding tools. Scales affordably across model sizes for practical deployment.",
  "visual_elements": [
    "Architecture diagram: Qwen-2.5-Coder backbone → PMA (cross-attention pooling) → sequence embedding",
    "Token aggregation sketch: compare EOS pooling (single token) vs PMA (attention from pooling token to all tokens)",
    "Bar/chart: retrieval performance (e.g., recall@k) of C2LLM vs EOS-pooled baselines across 0.5B and 7B",
    "Flowchart: contrastive training loop for query-code pairs (positive/negative sampling)"
  ],
  "hashtags": [
    "#CodeRetrieval",
    "#ContrastiveLearning",
    "#CrossAttention",
    "#CodeEmbeddings",
    "#LLM"
  ],
  "paper_id": "2025-12-24-arxiv-c2llm_technical_report_a_new_frontier_in_code_retrieval_via_adaptive_cross_atten",
  "paper_title": "C2LLM Technical Report: A New Frontier in Code Retrieval via Adaptive Cross-Attention Pooling",
  "paper_category": "Model",
  "paper_date": "2025-12-24",
  "paper_authors": "Jin Qin, Zihan Liao, Ziyin Zhang, Hang Yu, Peng Di",
  "paper_links": [
    {
      "title": "PDF",
      "url": "https://arxiv.org/pdf/2512.21332v1"
    }
  ],
  "generated_at": "2025-12-28T06:21:16.154092"
}