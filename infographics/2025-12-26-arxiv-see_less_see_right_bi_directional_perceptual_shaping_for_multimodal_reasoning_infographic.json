{
  "key_insight": "BiPS converts question-conditioned masked visual views into bidirectional \"where-to-look\" signals that guide a VLM to focus on fine-grained visual evidence (e.g., chart polylines) during reasoning, improving accuracy and efficiency without heavy external tools.",
  "problem_solved": "Current VLM reasoning either misses fine-grained visual cues, generalizes poorly across domains, or relies on expensive external visual tools; BiPS addresses these by producing compact, task-conditioned perceptual signals that direct attention to the right visual evidence.",
  "method": [
    "Create question-conditioned masked views of the input image to highlight candidate regions and fine-grained structures.",
    "Generate bidirectional where-to-look signals (e.g., top-down task cues and bottom-up perceptual cues) that indicate which visual tokens/regions matter for the query.",
    "Fuse these signals into the VLM reasoning loop so the model selectively attends to fine-grained evidence, reducing dependence on external tools and inference overhead."
  ],
  "impact": "Makes multimodal reasoning more accurate and computationally efficient on tasks requiring fine visual details (charts, diagrams, small objects), enabling more robust cross-domain deployment of VLMs with lower inference cost.",
  "visual_elements": [
    "Pipeline diagram: masked view generation → bidirectional where-to-look signal computation → VLM fusion and reasoning.",
    "Before/after example: a chart image with attention/heatmap overlays showing improved focus on polylines or small cues.",
    "Performance vs. cost chart: accuracy or robustness gains plotted against inference-time/resource usage (BiPS vs. baselines).",
    "Illustration comparing top-down (question-driven) and bottom-up (perceptual) signals merging into a single attention map."
  ],
  "hashtags": [
    "#VisionLanguage",
    "#MultimodalAI",
    "#EfficientAI",
    "#Interpretability",
    "#ChartUnderstanding"
  ],
  "paper_id": "2025-12-26-arxiv-see_less_see_right_bi_directional_perceptual_shaping_for_multimodal_reasoning",
  "paper_title": "See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning",
  "paper_category": "Model",
  "paper_date": "2025-12-26",
  "paper_authors": "Shuoshuo Zhang, Yizhen Zhang, Jingjing Fu, Lei Song, Jiang Bian",
  "paper_links": [
    {
      "title": "PDF",
      "url": "https://arxiv.org/pdf/2512.22120v1"
    }
  ],
  "generated_at": "2026-01-01T06:25:29.422985"
}