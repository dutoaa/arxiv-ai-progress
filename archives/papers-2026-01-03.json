[
  {
    "id": "2025-12-31-arxiv-coordinated_humanoid_manipulation_with_choice_policies",
    "date": "2025-12-31",
    "title": "Coordinated Humanoid Manipulation with Choice Policies",
    "authors": "Haozhi Qi, Yen-Jen Wang, Toru Lin, Brent Yi, Yi Ma",
    "abstract": "Humanoid robots hold great promise for operating in human-centric environments, yet achieving robust whole-body coordination across the head, hands, and legs remains a major challenge. We present a system that combines a modular teleoperation interface with a scalable learning framework to address this problem. Our teleoperation design decomposes humanoid control into intuitive submodules, which include hand-eye coordination, grasp primitives, arm end-effector tracking, and locomotion. This modu",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.25072v1"
      }
    ],
    "source_file": "papers/weekly/2025-W52/2025-12-31-arxiv-coordinated_humanoid_manipulation_with_choice_policies.yaml"
  },
  {
    "id": "2025-12-31-arxiv-gamo_geometry_aware_multi_view_diffusion_outpainting_for_sparse_view_3d_reconstr",
    "date": "2025-12-31",
    "title": "GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction",
    "authors": "Yi-Chuan Huang, Hao-Jen Chien, Chin-Yang Lin, Ying-Huan Chen, Yu-Lun Liu",
    "abstract": "Recent advances in 3D reconstruction have achieved remarkable progress in high-quality scene capture from dense multi-view imagery, yet struggle when input views are limited. Various approaches, including regularization techniques, semantic priors, and geometric constraints, have been implemented to address this challenge. Latest diffusion-based methods have demonstrated substantial improvements by generating novel views from new camera poses to augment training data, surpassing earlier regulari",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.25073v1"
      }
    ],
    "source_file": "papers/weekly/2025-W52/2025-12-31-arxiv-gamo_geometry_aware_multi_view_diffusion_outpainting_for_sparse_view_3d_reconstr.yaml"
  },
  {
    "id": "2025-12-31-nature-inferring_spatial_single_cell_level_interactions_through_interpreting_cell_state",
    "date": "2025-12-31",
    "title": "Inferring spatial single-cell-level interactions through interpreting cell state and niche correlations learned by self-supervised graph transformer",
    "authors": "Xiao Xiao, Le Zhang, Hongyu Zhao, Zuoheng Wang",
    "abstract": "<p>Nature Machine Intelligence, Published online: 31 December 2025; <a href=\"https://www.nature.com/articles/s42256-025-01161-0\">doi:10.1038/s42256-025-01161-0</a></p>Xiao et al. present GITIII, a lightweight and interpretable graph transformer for inferring spatial single-cell-level interactions and quantifying the influence of neighbouring cells on the gene expression of receiver cells in spatial transcriptomics.",
    "category": "Model",
    "source": "nature",
    "score": 110,
    "tags": [
      "ai",
      "research",
      "nature",
      "model"
    ],
    "links": [
      {
        "title": "View Paper",
        "url": "https://www.nature.com/articles/s42256-025-01161-0"
      },
      {
        "title": "DOI",
        "url": "https://doi.org/10.1038/s42256-025-01161-0"
      }
    ],
    "source_file": "papers/weekly/2025-W52/2025-12-31-nature-inferring_spatial_single_cell_level_interactions_through_interpreting_cell_state.yaml"
  },
  {
    "id": "2025-12-31-nature-immunostruct_enables_multimodal_deep_learning_for_immunogenicity_prediction",
    "date": "2025-12-31",
    "title": "ImmunoStruct enables multimodal deep learning for immunogenicity prediction",
    "authors": "Kevin Bijan Givechian, João Felipe Rocha, Chen Liu, Edward Yang, Sidharth Tyagi",
    "abstract": "<p>Nature Machine Intelligence, Published online: 31 December 2025; <a href=\"https://www.nature.com/articles/s42256-025-01163-y\">doi:10.1038/s42256-025-01163-y</a></p>A multimodal deep learning model combines molecular sequence, structure and biochemical properties to predict immunogenicity in an interpretable way, providing a framework for smarter molecular prediction and hypothesis generation.",
    "category": "Model",
    "source": "nature",
    "score": 115,
    "tags": [
      "ai",
      "research",
      "nature",
      "model"
    ],
    "links": [
      {
        "title": "View Paper",
        "url": "https://www.nature.com/articles/s42256-025-01163-y"
      },
      {
        "title": "DOI",
        "url": "https://doi.org/10.1038/s42256-025-01163-y"
      }
    ],
    "source_file": "papers/weekly/2025-W52/2025-12-31-nature-immunostruct_enables_multimodal_deep_learning_for_immunogenicity_prediction.yaml"
  },
  {
    "id": "2025-12-31-nature-reusability_report_optimizing_t_count_in_general_quantum_circuits_with_alphatens",
    "date": "2025-12-31",
    "title": "Reusability report: Optimizing T count in general quantum circuits with AlphaTensor-Quantum",
    "authors": "Remmy Zen, Maximilian Nägele, Florian Marquardt",
    "abstract": "<p>Nature Machine Intelligence, Published online: 31 December 2025; <a href=\"https://www.nature.com/articles/s42256-025-01166-9\">doi:10.1038/s42256-025-01166-9</a></p>The reusability of AlphaTensor-Quantum is tested and the method is extended to optimize a broad range of quantum circuits without retraining, achieving greater T-count reductions and demonstrating generalizable and efficient quantum circuit optimization.",
    "category": "Hardware/Infra",
    "source": "nature",
    "score": 110,
    "tags": [
      "ai",
      "research",
      "nature",
      "hardware_infra"
    ],
    "links": [
      {
        "title": "View Paper",
        "url": "https://www.nature.com/articles/s42256-025-01166-9"
      },
      {
        "title": "DOI",
        "url": "https://doi.org/10.1038/s42256-025-01166-9"
      }
    ],
    "source_file": "papers/weekly/2025-W52/2025-12-31-nature-reusability_report_optimizing_t_count_in_general_quantum_circuits_with_alphatens.yaml"
  },
  {
    "id": "2025-12-31-nature-assessing_the_potential_of_deep_learning_for_protein_ligand_docking",
    "date": "2025-12-31",
    "title": "Assessing the potential of deep learning for protein–ligand docking",
    "authors": "Alex Morehead, Nabin Giri, Jian Liu, Pawan Neupane, Jianlin Cheng",
    "abstract": "<p>Nature Machine Intelligence, Published online: 31 December 2025; <a href=\"https://www.nature.com/articles/s42256-025-01160-1\">doi:10.1038/s42256-025-01160-1</a></p>Morehead et al. introduce the benchmark PoseBench and evaluate the strengths and limitations of current AI-based protein–ligand docking and structure prediction methods.",
    "category": "Model",
    "source": "nature",
    "score": 115,
    "tags": [
      "ai",
      "research",
      "nature",
      "model"
    ],
    "links": [
      {
        "title": "View Paper",
        "url": "https://www.nature.com/articles/s42256-025-01160-1"
      },
      {
        "title": "DOI",
        "url": "https://doi.org/10.1038/s42256-025-01160-1"
      }
    ],
    "source_file": "papers/weekly/2025-W52/2025-12-31-nature-assessing_the_potential_of_deep_learning_for_protein_ligand_docking.yaml"
  },
  {
    "id": "2025-12-31-nature-learning_intermediate_physical_states_for_inverse_metasurface_design",
    "date": "2025-12-31",
    "title": "Learning intermediate physical states for inverse metasurface design",
    "authors": "Chun-Teh Chen",
    "abstract": "<p>Nature Machine Intelligence, Published online: 31 December 2025; <a href=\"https://www.nature.com/articles/s42256-025-01167-8\">doi:10.1038/s42256-025-01167-8</a></p>Deep generative models that learn intermediate surface-current maps, rather than layouts directly, offer a more stable route to inverse design of tunable and stacked metasurfaces.",
    "category": "Model",
    "source": "nature",
    "score": 105,
    "tags": [
      "ai",
      "research",
      "nature",
      "model"
    ],
    "links": [
      {
        "title": "View Paper",
        "url": "https://www.nature.com/articles/s42256-025-01167-8"
      },
      {
        "title": "DOI",
        "url": "https://doi.org/10.1038/s42256-025-01167-8"
      }
    ],
    "source_file": "papers/weekly/2025-W52/2025-12-31-nature-learning_intermediate_physical_states_for_inverse_metasurface_design.yaml"
  },
  {
    "id": "2025-12-31-arxiv-mama_memeia_multi_aspect_multi_agent_collaboration_for_depressive_symptoms_ident",
    "date": "2025-12-31",
    "title": "MAMA-Memeia! Multi-Aspect Multi-Agent Collaboration for Depressive Symptoms Identification in Memes",
    "authors": "Siddhant Agarwal, Adya Dhuler, Polly Ruhnke, Melvin Speisman, Md Shad Akhtar",
    "abstract": "Over the past years, memes have evolved from being exclusively a medium of humorous exchanges to one that allows users to express a range of emotions freely and easily. With the ever-growing utilization of memes in expressing depressive sentiments, we conduct a study on identifying depressive symptoms exhibited by memes shared by users of online social media platforms. We introduce RESTOREx as a vital resource for detecting depressive symptoms in memes on social media through the Large Language ",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.25015v1"
      }
    ],
    "source_file": "papers/weekly/2025-W52/2025-12-31-arxiv-mama_memeia_multi_aspect_multi_agent_collaboration_for_depressive_symptoms_ident.yaml"
  },
  {
    "id": "2025-12-31-arxiv-scaling_open_ended_reasoning_to_predict_the_future",
    "date": "2025-12-31",
    "title": "Scaling Open-Ended Reasoning to Predict the Future",
    "authors": "Nikhil Chandak, Shashwat Goel, Ameya Prabhu, Moritz Hardt, Jonas Geiping",
    "abstract": "High-stakes decision making involves reasoning under uncertainty about the future. In this work, we train language models to make predictions on open-ended forecasting questions. To scale up training data, we synthesize novel forecasting questions from global events reported in daily news, using a fully automated, careful curation recipe. We train the Qwen3 thinking models on our dataset, OpenForesight. To prevent leakage of future information during training and evaluation, we use an offline ne",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.25070v1"
      }
    ],
    "source_file": "papers/weekly/2025-W52/2025-12-31-arxiv-scaling_open_ended_reasoning_to_predict_the_future.yaml"
  },
  {
    "id": "2025-12-31-arxiv-on_the_geometry_and_topology_of_representations_the_manifolds_of_modular_additio",
    "date": "2025-12-31",
    "title": "On the geometry and topology of representations: the manifolds of modular addition",
    "authors": "Gabriela Moisescu-Pareja, Gavin McCracken, Harley Wiltzer, Vincent Létourneau, Colin Daniels",
    "abstract": "The Clock and Pizza interpretations, associated with architectures differing in either uniform or learnable attention, were introduced to argue that different architectural designs can yield distinct circuits for modular addition. In this work, we show that this is not the case, and that both uniform attention and trainable attention architectures implement the same algorithm via topologically and geometrically equivalent representations. Our methodology goes beyond the interpretation of individ",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.25060v1"
      }
    ],
    "source_file": "papers/weekly/2025-W52/2025-12-31-arxiv-on_the_geometry_and_topology_of_representations_the_manifolds_of_modular_additio.yaml"
  },
  {
    "id": "2025-12-31-nature-harnessing_the_power_of_single_cell_large_language_models_with_parameter_efficie",
    "date": "2025-12-31",
    "title": "Harnessing the power of single-cell large language models with parameter-efficient fine-tuning using scPEFT",
    "authors": "Fei He, Ruixin Fei, Jordan E. Krull, Yang Yu, Xinyu Zhang",
    "abstract": "<p>Nature Machine Intelligence, Published online: 31 December 2025; <a href=\"https://www.nature.com/articles/s42256-025-01170-z\">doi:10.1038/s42256-025-01170-z</a></p>He et al. present a parameter-efficient fine-tuning method for single-cell language models that improves performance on unseen diseases, treatments and cell types.",
    "category": "Model",
    "source": "nature",
    "score": 115,
    "tags": [
      "ai",
      "research",
      "nature",
      "model"
    ],
    "links": [
      {
        "title": "View Paper",
        "url": "https://www.nature.com/articles/s42256-025-01170-z"
      },
      {
        "title": "DOI",
        "url": "https://doi.org/10.1038/s42256-025-01170-z"
      }
    ],
    "source_file": "papers/weekly/2025-W52/2025-12-31-nature-harnessing_the_power_of_single_cell_large_language_models_with_parameter_efficie.yaml"
  },
  {
    "id": "2025-12-31-arxiv-spacetimepilot_generative_rendering_of_dynamic_scenes_across_space_and_time",
    "date": "2025-12-31",
    "title": "SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time",
    "authors": "Zhening Huang, Hyeonho Jeong, Xuelin Chen, Yulia Gryaditskaya, Tuanfeng Y. Wang",
    "abstract": "We present SpaceTimePilot, a video diffusion model that disentangles space and time for controllable generative rendering. Given a monocular video, SpaceTimePilot can independently alter the camera viewpoint and the motion sequence within the generative process, re-rendering the scene for continuous and arbitrary exploration across space and time. To achieve this, we introduce an effective animation time-embedding mechanism in the diffusion process, allowing explicit control of the output video'",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.25075v1"
      }
    ],
    "source_file": "papers/weekly/2025-W52/2025-12-31-arxiv-spacetimepilot_generative_rendering_of_dynamic_scenes_across_space_and_time.yaml"
  },
  {
    "id": "2025-12-31-arxiv-edit3r_instant_3d_scene_editing_from_sparse_unposed_images",
    "date": "2025-12-31",
    "title": "Edit3r: Instant 3D Scene Editing from Sparse Unposed Images",
    "authors": "Jiageng Liu, Weijie Lyu, Xueting Li, Yejie Guo, Ming-Hsuan Yang",
    "abstract": "We present Edit3r, a feed-forward framework that reconstructs and edits 3D scenes in a single pass from unposed, view-inconsistent, instruction-edited images. Unlike prior methods requiring per-scene optimization, Edit3r directly predicts instruction-aligned 3D edits, enabling fast and photorealistic rendering without optimization or pose estimation. A key challenge in training such a model lies in the absence of multi-view consistent edited images for supervision. We address this with (i) a SAM",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.25071v1"
      }
    ],
    "source_file": "papers/weekly/2025-W52/2025-12-31-arxiv-edit3r_instant_3d_scene_editing_from_sparse_unposed_images.yaml"
  },
  {
    "id": "2025-12-31-nature-current_diffusion_model_for_metasurface_structure_discoveries_with_spatial_frequ",
    "date": "2025-12-31",
    "title": "Current-diffusion model for metasurface structure discoveries with spatial-frequency dynamics",
    "authors": "Erji Li, Yusong Wang, Lei Jin, Zheng Zong, Enze Zhu",
    "abstract": "<p>Nature Machine Intelligence, Published online: 31 December 2025; <a href=\"https://www.nature.com/articles/s42256-025-01162-z\">doi:10.1038/s42256-025-01162-z</a></p>Metasurface design driven by AI faces challenges, such as extrapolation to unexplored performance regimes. MetaAI, a physics-aware current-diffusion framework, is introduced to advance metamaterial discovery from interpolation to extrapolation.",
    "category": "Model",
    "source": "nature",
    "score": 115,
    "tags": [
      "ai",
      "research",
      "nature",
      "model"
    ],
    "links": [
      {
        "title": "View Paper",
        "url": "https://www.nature.com/articles/s42256-025-01162-z"
      },
      {
        "title": "DOI",
        "url": "https://doi.org/10.1038/s42256-025-01162-z"
      }
    ],
    "source_file": "papers/weekly/2025-W52/2025-12-31-nature-current_diffusion_model_for_metasurface_structure_discoveries_with_spatial_frequ.yaml"
  },
  {
    "id": "2025-12-31-arxiv-from_inpainting_to_editing_a_self_bootstrapping_framework_for_context_rich_visua",
    "date": "2025-12-31",
    "title": "From Inpainting to Editing: A Self-Bootstrapping Framework for Context-Rich Visual Dubbing",
    "authors": "Xu He, Haoxian Zhang, Hejia Chen, Changyuan Zheng, Liyang Chen",
    "abstract": "Audio-driven visual dubbing aims to synchronize a video's lip movements with new speech, but is fundamentally challenged by the lack of ideal training data: paired videos where only a subject's lip movements differ while all other visual conditions are identical. Existing methods circumvent this with a mask-based inpainting paradigm, where an incomplete visual conditioning forces models to simultaneously hallucinate missing content and sync lips, leading to visual artifacts, identity drift, and ",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.25066v1"
      }
    ],
    "source_file": "papers/weekly/2025-W52/2025-12-31-arxiv-from_inpainting_to_editing_a_self_bootstrapping_framework_for_context_rich_visua.yaml"
  },
  {
    "id": "2025-12-31-arxiv-vulcan_instance_optimal_systems_heuristics_through_llm_driven_search",
    "date": "2025-12-31",
    "title": "Vulcan: Instance-Optimal Systems Heuristics Through LLM-Driven Search",
    "authors": "Rohit Dwivedula, Divyanshu Saxena, Sujay Yadalam, Daehyeok Kim, Aditya Akella",
    "abstract": "Resource-management tasks in modern operating and distributed systems continue to rely primarily on hand-designed heuristics for tasks such as scheduling, caching, or active queue management. Designing performant heuristics is an expensive, time-consuming process that we are forced to continuously go through due to the constant flux of hardware, workloads and environments.\n  We propose a new alternative: synthesizing instance-optimal heuristics -- specialized for the exact workloads and hardware",
    "category": "Hardware/Infra",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "hardware_infra"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.25065v1"
      }
    ],
    "source_file": "papers/weekly/2025-W52/2025-12-31-arxiv-vulcan_instance_optimal_systems_heuristics_through_llm_driven_search.yaml"
  },
  {
    "id": "2025-12-30-nature-author_correction_scalable_and_robust_dna_based_storage_via_coding_theory_and_de",
    "date": "2025-12-30",
    "title": "Author Correction: Scalable and robust DNA-based storage via coding theory and deep learning",
    "authors": "Daniella Bar-Lev, Itai Orr, Omer Sabary, Tuvi Etzion, Eitan Yaakobi",
    "abstract": "<p>Nature Machine Intelligence, Published online: 30 December 2025; <a href=\"https://www.nature.com/articles/s42256-025-01175-8\">doi:10.1038/s42256-025-01175-8</a></p>Author Correction: Scalable and robust DNA-based storage via coding theory and deep learning",
    "category": "Hardware/Infra",
    "source": "nature",
    "score": 115,
    "tags": [
      "ai",
      "research",
      "nature",
      "hardware_infra"
    ],
    "links": [
      {
        "title": "View Paper",
        "url": "https://www.nature.com/articles/s42256-025-01175-8"
      },
      {
        "title": "DOI",
        "url": "https://doi.org/10.1038/s42256-025-01175-8"
      }
    ],
    "source_file": "papers/weekly/2025-W52/2025-12-30-nature-author_correction_scalable_and_robust_dna_based_storage_via_coding_theory_and_de.yaml"
  },
  {
    "id": "2025-12-29-nature-versatile_cardiovascular_signal_generation_with_a_unified_diffusion_transformer",
    "date": "2025-12-29",
    "title": "Versatile cardiovascular signal generation with a unified diffusion transformer",
    "authors": "Zehua Chen, Yuyang Miao, Liyuan Wang, Luyun Fan, Danilo P. Mandic",
    "abstract": "<p>Nature Machine Intelligence, Published online: 29 December 2025; <a href=\"https://www.nature.com/articles/s42256-025-01147-y\">doi:10.1038/s42256-025-01147-y</a></p>UniCardio is a unified framework for versatile multimodal cardiovascular signal generation, enabling robust signal restoration and cross-modal translation to detect abnormal conditions and estimate vital signs in real-time health monitoring.",
    "category": "Model",
    "source": "nature",
    "score": 115,
    "tags": [
      "ai",
      "research",
      "nature",
      "model"
    ],
    "links": [
      {
        "title": "View Paper",
        "url": "https://www.nature.com/articles/s42256-025-01147-y"
      },
      {
        "title": "DOI",
        "url": "https://doi.org/10.1038/s42256-025-01147-y"
      }
    ],
    "source_file": "papers/weekly/2025-W52/2025-12-29-nature-versatile_cardiovascular_signal_generation_with_a_unified_diffusion_transformer.yaml"
  },
  {
    "id": "2025-12-29-arxiv-ai_tutoring_can_safely_and_effectively_support_students_an_exploratory_rct_in_uk",
    "date": "2025-12-29",
    "title": "AI tutoring can safely and effectively support students: An exploratory RCT in UK classrooms",
    "authors": "LearnLM Team, Eedi, :, Albert Wang, Aliya Rysbek",
    "abstract": "One-to-one tutoring is widely considered the gold standard for personalized education, yet it remains prohibitively expensive to scale. To evaluate whether generative AI might help expand access to this resource, we conducted an exploratory randomized controlled trial (RCT) with $N = 165$ students across five UK secondary schools. We integrated LearnLM -- a generative AI model fine-tuned for pedagogy -- into chat-based tutoring sessions on the Eedi mathematics platform. In the RCT, expert tutors",
    "category": "Project",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "project"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.23633v1"
      }
    ],
    "source_file": "papers/weekly/2025-W52/2025-12-29-arxiv-ai_tutoring_can_safely_and_effectively_support_students_an_exploratory_rct_in_uk.yaml"
  },
  {
    "id": "2025-12-29-arxiv-web_world_models",
    "date": "2025-12-29",
    "title": "Web World Models",
    "authors": "Jichen Feng, Yifan Zhang, Chenggong Zhang, Yifu Lu, Shilong Liu",
    "abstract": "Language agents increasingly require persistent worlds in which they can act, remember, and learn. Existing approaches sit at two extremes: conventional web frameworks provide reliable but fixed contexts backed by databases, while fully generative world models aim for unlimited environments at the expense of controllability and practical engineering. In this work, we introduce the Web World Model (WWM), a middle ground where world state and ``physics'' are implemented in ordinary web code to ens",
    "category": "Project",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "project"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.23676v1"
      }
    ],
    "source_file": "papers/weekly/2025-W52/2025-12-29-arxiv-web_world_models.yaml"
  },
  {
    "id": "2025-12-29-arxiv-training_ai_co_scientists_using_rubric_rewards",
    "date": "2025-12-29",
    "title": "Training AI Co-Scientists Using Rubric Rewards",
    "authors": "Shashwat Goel, Rishi Hazra, Dulhan Jayalath, Timon Willi, Parag Jain",
    "abstract": "AI co-scientists are emerging as a tool to assist human researchers in achieving their research goals. A crucial feature of these AI co-scientists is the ability to generate a research plan given a set of aims and constraints. The plan may be used by researchers for brainstorming, or may even be implemented after further refinement. However, language models currently struggle to generate research plans that follow all constraints and implicit requirements. In this work, we study how to leverage ",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.23707v1"
      }
    ],
    "source_file": "papers/weekly/2025-W52/2025-12-29-arxiv-training_ai_co_scientists_using_rubric_rewards.yaml"
  },
  {
    "id": "2025-12-29-arxiv-omniagent_audio_guided_active_perception_agent_for_omnimodal_audio_video_underst",
    "date": "2025-12-29",
    "title": "OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding",
    "authors": "Keda Tao, Wenjie Du, Bohan Yu, Weiqiang Wang, Jian Liu",
    "abstract": "Omnimodal large language models have made significant strides in unifying audio and visual modalities; however, they often lack the fine-grained cross-modal understanding and have difficulty with multimodal alignment. To address these limitations, we introduce OmniAgent, a fully audio-guided active perception agent that dynamically orchestrates specialized tools to achieve more fine-grained audio-visual reasoning. Unlike previous works that rely on rigid, static workflows and dense frame-caption",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.23646v1"
      }
    ],
    "source_file": "papers/weekly/2025-W52/2025-12-29-arxiv-omniagent_audio_guided_active_perception_agent_for_omnimodal_audio_video_underst.yaml"
  },
  {
    "id": "2025-12-29-arxiv-robomirror_understand_before_you_imitate_for_video_to_humanoid_locomotion",
    "date": "2025-12-29",
    "title": "RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion",
    "authors": "Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Tao Huang",
    "abstract": "Humans learn locomotion through visual observation, interpreting visual content first before imitating actions. However, state-of-the-art humanoid locomotion systems rely on either curated motion capture trajectories or sparse text commands, leaving a critical gap between visual understanding and control. Text-to-motion methods suffer from semantic sparsity and staged pipeline errors, while video-based approaches only perform mechanical pose mimicry without genuine visual understanding. We propo",
    "category": "Project",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "project"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.23649v1"
      }
    ],
    "source_file": "papers/weekly/2025-W52/2025-12-29-arxiv-robomirror_understand_before_you_imitate_for_video_to_humanoid_locomotion.yaml"
  },
  {
    "id": "2025-12-29-arxiv-stream_diffvsr_low_latency_streamable_video_super_resolution_via_auto_regressive",
    "date": "2025-12-29",
    "title": "Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion",
    "authors": "Hau-Shiang Shiu, Chin-Yang Lin, Zhixiang Wang, Chi-Wei Hsiao, Po-Fan Yu",
    "abstract": "Diffusion-based video super-resolution (VSR) methods achieve strong perceptual quality but remain impractical for latency-sensitive settings due to reliance on future frames and expensive multi-step denoising. We propose Stream-DiffVSR, a causally conditioned diffusion framework for efficient online VSR. Operating strictly on past frames, it combines a four-step distilled denoiser for fast inference, an Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during laten",
    "category": "Hardware/Infra",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "hardware_infra"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.23709v1"
      }
    ],
    "source_file": "papers/weekly/2025-W52/2025-12-29-arxiv-stream_diffvsr_low_latency_streamable_video_super_resolution_via_auto_regressive.yaml"
  },
  {
    "id": "2025-12-29-arxiv-boad_discovering_hierarchical_software_engineering_agents_via_bandit_optimizatio",
    "date": "2025-12-29",
    "title": "BOAD: Discovering Hierarchical Software Engineering Agents via Bandit Optimization",
    "authors": "Iris Xu, Guangtao Zeng, Zexue He, Charles Jin, Aldo Pareja",
    "abstract": "Large language models (LLMs) have shown strong reasoning and coding capabilities, yet they struggle to generalize to real-world software engineering (SWE) problems that are long-horizon and out of distribution. Existing systems often rely on a single agent to handle the entire workflow-interpreting issues, navigating large codebases, and implementing fixes-within one reasoning chain. Such monolithic designs force the model to retain irrelevant context, leading to spurious correlations and poor g",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.23631v1"
      }
    ],
    "source_file": "papers/weekly/2025-W52/2025-12-29-arxiv-boad_discovering_hierarchical_software_engineering_agents_via_bandit_optimizatio.yaml"
  },
  {
    "id": "2025-12-29-arxiv-memorization_in_3d_shape_generation_an_empirical_study",
    "date": "2025-12-29",
    "title": "Memorization in 3D Shape Generation: An Empirical Study",
    "authors": "Shu Pu, Boya Zeng, Kaichen Zhou, Mengyu Wang, Zhuang Liu",
    "abstract": "Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to q",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.23628v1"
      }
    ],
    "source_file": "papers/weekly/2025-W52/2025-12-29-arxiv-memorization_in_3d_shape_generation_an_empirical_study.yaml"
  },
  {
    "id": "2025-12-29-arxiv-nested_browser_use_learning_for_agentic_information_seeking",
    "date": "2025-12-29",
    "title": "Nested Browser-Use Learning for Agentic Information Seeking",
    "authors": "Baixuan Li, Jialong Wu, Wenbiao Yin, Kuan Li, Zhongwang Zhang",
    "abstract": "Information-seeking (IS) agents have achieved strong performance across a range of wide and deep search tasks, yet their tool use remains largely restricted to API-level snippet retrieval and URL-based page fetching, limiting access to the richer information available through real browsing. While full browser interaction could unlock deeper capabilities, its fine-grained control and verbose page content returns introduce substantial complexity for ReAct-style function-calling agents. To bridge t",
    "category": "Project",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "project"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.23647v1"
      }
    ],
    "source_file": "papers/weekly/2025-W52/2025-12-29-arxiv-nested_browser_use_learning_for_agentic_information_seeking.yaml"
  },
  {
    "id": "2025-12-29-arxiv-rethinking_the_spatio_temporal_alignment_of_end_to_end_3d_perception",
    "date": "2025-12-29",
    "title": "Rethinking the Spatio-Temporal Alignment of End-to-End 3D Perception",
    "authors": "Xiaoyu Li, Peidong Li, Xian Wu, Long Shi, Dedong Liu",
    "abstract": "Spatio-temporal alignment is crucial for temporal modeling of end-to-end (E2E) perception in autonomous driving (AD), providing valuable structural and textural prior information. Existing methods typically rely on the attention mechanism to align objects across frames, simplifying the motion model with a unified explicit physical model (constant velocity, etc.). These approaches prefer semantic features for implicit alignment, challenging the importance of explicit motion modeling in the tradit",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.23635v1"
      }
    ],
    "source_file": "papers/weekly/2025-W52/2025-12-29-arxiv-rethinking_the_spatio_temporal_alignment_of_end_to_end_3d_perception.yaml"
  },
  {
    "id": "2025-12-29-arxiv-diffusion_knows_transparency_repurposing_video_diffusion_for_transparent_object",
    "date": "2025-12-29",
    "title": "Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation",
    "authors": "Shaocong Xu, Songlin Wei, Qizhe Wei, Zheng Geng, Hong Li",
    "abstract": "Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences r",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.23705v1"
      }
    ],
    "source_file": "papers/weekly/2025-W52/2025-12-29-arxiv-diffusion_knows_transparency_repurposing_video_diffusion_for_transparent_object.yaml"
  },
  {
    "id": "2025-12-29-arxiv-eliciting_behaviors_in_multi_turn_conversations",
    "date": "2025-12-29",
    "title": "Eliciting Behaviors in Multi-Turn Conversations",
    "authors": "Jing Huang, Shujian Zhang, Lun Wang, Andrew Hard, Rajiv Mathews",
    "abstract": "Identifying specific and often complex behaviors from large language models (LLMs) in conversational settings is crucial for their evaluation. Recent work proposes novel techniques to find natural language prompts that induce specific behaviors from a target model, yet they are mainly studied in single-turn settings. In this work, we study behavior elicitation in the context of multi-turn conversations. We first offer an analytical framework that categorizes existing methods into three families ",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.23701v1"
      }
    ],
    "source_file": "papers/weekly/2025-W52/2025-12-29-arxiv-eliciting_behaviors_in_multi_turn_conversations.yaml"
  },
  {
    "id": "2025-12-29-arxiv-end_to_end_test_time_training_for_long_context",
    "date": "2025-12-29",
    "title": "End-to-End Test-Time Training for Long Context",
    "authors": "Arnuv Tandon, Karan Dalal, Xinhao Li, Daniel Koceja, Marcel Rød",
    "abstract": "We formulate long-context language modeling as a problem in continual learning rather than architecture design. Under this formulation, we only use a standard architecture -- a Transformer with sliding-window attention. However, our model continues learning at test time via next-token prediction on the given context, compressing the context it reads into its weights. In addition, we improve the model's initialization for learning at test time via meta-learning at training time. Overall, our meth",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.23675v1"
      }
    ],
    "source_file": "papers/weekly/2025-W52/2025-12-29-arxiv-end_to_end_test_time_training_for_long_context.yaml"
  },
  {
    "id": "2025-12-26-arxiv-see_less_see_right_bi_directional_perceptual_shaping_for_multimodal_reasoning",
    "date": "2025-12-26",
    "title": "See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning",
    "authors": "Shuoshuo Zhang, Yizhen Zhang, Jingjing Fu, Lei Song, Jiang Bian",
    "abstract": "Large vision-language models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across domains, and incur high inference-time cost. In this paper, we propose Bi-directional Perceptual Shaping (BiPS), which transforms question-conditioned masked views into bidirectional where-to-look signals that ",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.22120v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-26-arxiv-see_less_see_right_bi_directional_perceptual_shaping_for_multimodal_reasoning.yaml"
  },
  {
    "id": "2025-12-26-arxiv-streamavatar_streaming_diffusion_models_for_real_time_interactive_human_avatars",
    "date": "2025-12-26",
    "title": "StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars",
    "authors": "Zhiyao Sun, Ziqiao Peng, Yifeng Ma, Yi Chen, Zhengguang Zhou",
    "abstract": "Real-time, streaming interactive avatars represent a critical yet challenging goal in digital human research. Although diffusion-based human avatar generation methods achieve remarkable success, their non-causal architecture and high computational costs make them unsuitable for streaming. Moreover, existing interactive approaches are typically limited to head-and-shoulder region, limiting their ability to produce gestures and body motions. To address these challenges, we propose a two-stage auto",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.22065v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-26-arxiv-streamavatar_streaming_diffusion_models_for_real_time_interactive_human_avatars.yaml"
  },
  {
    "id": "2025-12-26-arxiv-meta_learning_based_handover_management_in_nextg_o_ran",
    "date": "2025-12-26",
    "title": "Meta-Learning-Based Handover Management in NextG O-RAN",
    "authors": "Michail Kalntis, George Iosifidis, José Suárez-Varela, Andra Lutu, Fernando A. Kuipers",
    "abstract": "While traditional handovers (THOs) have served as a backbone for mobile connectivity, they increasingly suffer from failures and delays, especially in dense deployments and high-frequency bands. To address these limitations, 3GPP introduced Conditional Handovers (CHOs) that enable proactive cell reservations and user-driven execution. However, both handover (HO) types present intricate trade-offs in signaling, resource usage, and reliability. This paper presents unique, countrywide mobility mana",
    "category": "Project",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "project"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.22022v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-26-arxiv-meta_learning_based_handover_management_in_nextg_o_ran.yaml"
  },
  {
    "id": "2025-12-26-arxiv-longfly_long_horizon_uav_vision_and_language_navigation_with_spatiotemporal_cont",
    "date": "2025-12-26",
    "title": "LongFly: Long-Horizon UAV Vision-and-Language Navigation with Spatiotemporal Context Integration",
    "authors": "Wen Jiang, Li Wang, Kangyao Huang, Wei Fan, Jinyuan Liu",
    "abstract": "Unmanned aerial vehicles (UAVs) are crucial tools for post-disaster search and rescue, facing challenges such as high information density, rapid changes in viewpoint, and dynamic structures, especially in long-horizon navigation. However, current UAV vision-and-language navigation(VLN) methods struggle to model long-horizon spatiotemporal context in complex environments, resulting in inaccurate semantic alignment and unstable path planning. To this end, we propose LongFly, a spatiotemporal conte",
    "category": "Project",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "project"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.22010v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-26-arxiv-longfly_long_horizon_uav_vision_and_language_navigation_with_spatiotemporal_cont.yaml"
  },
  {
    "id": "2025-12-26-arxiv-duadeep_seqaffinity_dual_stream_deep_learning_framework_for_sequence_only_antige",
    "date": "2025-12-26",
    "title": "DuaDeep-SeqAffinity: Dual-Stream Deep Learning Framework for Sequence-Only Antigen-Antibody Affinity Prediction",
    "authors": "Aicha Boutorh, Soumia Bouyahiaoui, Sara Belhadj, Nour El Yakine Guendouz, Manel Kara Laouar",
    "abstract": "Predicting the binding affinity between antigens and antibodies is fundamental to drug discovery and vaccine development. Traditional computational approaches often rely on experimentally determined 3D structures, which are scarce and computationally expensive to obtain. This paper introduces DuaDeep-SeqAffinity, a novel sequence-only deep learning framework that predicts affinity scores solely from their amino acid sequences using a dual-stream hybrid architecture. Our approach leverages pre-tr",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.22007v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-26-arxiv-duadeep_seqaffinity_dual_stream_deep_learning_framework_for_sequence_only_antige.yaml"
  },
  {
    "id": "2025-12-26-arxiv-a_lightweight_multi_scale_attention_framework_for_real_time_spinal_endoscopic_in",
    "date": "2025-12-26",
    "title": "A Lightweight Multi-Scale Attention Framework for Real-Time Spinal Endoscopic Instance Segmentation",
    "authors": "Qi Lai, JunYan Li, Qiang Cai, Lei Wang, Tao Yan",
    "abstract": "Real-time instance segmentation for spinal endoscopy is important for identifying and protecting critical anatomy during surgery, but it is difficult because of the narrow field of view, specular highlights, smoke/bleeding, unclear boundaries, and large scale changes. Deployment is also constrained by limited surgical hardware, so the model must balance accuracy and speed and remain stable under small-batch (even batch-1) training. We propose LMSF-A, a lightweight multi-scale attention framework",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.21984v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-26-arxiv-a_lightweight_multi_scale_attention_framework_for_real_time_spinal_endoscopic_in.yaml"
  },
  {
    "id": "2025-12-26-arxiv-look_closer_an_adversarial_parametric_editing_framework_for_hallucination_mitiga",
    "date": "2025-12-26",
    "title": "Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs",
    "authors": "Jiayu Hu, Beibei Li, Jiangwei Xia, Yanjun Qin, Bing Ji",
    "abstract": "While Vision-Language Models (VLMs) have garnered increasing attention in the AI community due to their promising practical applications, they exhibit persistent hallucination issues, generating outputs misaligned with visual inputs. Recent studies attribute these hallucinations to VLMs' over-reliance on linguistic priors and insufficient visual feature integration, proposing heuristic decoding calibration strategies to mitigate them. However, the non-trainable nature of these strategies inheren",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.21999v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-26-arxiv-look_closer_an_adversarial_parametric_editing_framework_for_hallucination_mitiga.yaml"
  },
  {
    "id": "2025-12-26-arxiv-libcontinual_a_comprehensive_library_towards_realistic_continual_learning",
    "date": "2025-12-26",
    "title": "LibContinual: A Comprehensive Library towards Realistic Continual Learning",
    "authors": "Wenbin Li, Shangge Liu, Borui Kang, Yiyang Chen, KaXuan Lew",
    "abstract": "A fundamental challenge in Continual Learning (CL) is catastrophic forgetting, where adapting to new tasks degrades the performance on previous ones. While the field has evolved with diverse methods, this rapid surge in diverse methodologies has culminated in a fragmented research landscape. The lack of a unified framework, including inconsistent implementations, conflicting dependencies, and varying evaluation protocols, makes fair comparison and reproducible research increasingly difficult. To",
    "category": "Project",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "project"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.22029v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-26-arxiv-libcontinual_a_comprehensive_library_towards_realistic_continual_learning.yaml"
  },
  {
    "id": "2025-12-26-arxiv-direction_finding_with_sparse_arrays_based_on_variable_window_size_spatial_smoot",
    "date": "2025-12-26",
    "title": "Direction Finding with Sparse Arrays Based on Variable Window Size Spatial Smoothing",
    "authors": "Wesley S. Leite, Rodrigo C. de Lamare, Yuriy Zakharov, Wei Liu, Martin Haardt",
    "abstract": "In this work, we introduce a variable window size (VWS) spatial smoothing framework that enhances coarray-based direction of arrival (DOA) estimation for sparse linear arrays. By compressing the smoothing aperture, the proposed VWS Coarray MUSIC (VWS-CA-MUSIC) and VWS Coarray root-MUSIC (VWS-CA-rMUSIC) algorithms replace part of the perturbed rank-one outer products in the smoothed coarray data with unperturbed low-rank additional terms, increasing the separation between signal and noise subspac",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.22024v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-26-arxiv-direction_finding_with_sparse_arrays_based_on_variable_window_size_spatial_smoot.yaml"
  },
  {
    "id": "2025-12-26-arxiv-yume_1_5_a_text_controlled_interactive_world_generation_model",
    "date": "2025-12-26",
    "title": "Yume-1.5: A Text-Controlled Interactive World Generation Model",
    "authors": "Xiaofeng Mao, Zhen Li, Chuanhao Li, Xiaojie Xu, Kaining Ying",
    "abstract": "Recent approaches have demonstrated the promise of using diffusion models to generate interactive and explorable worlds. However, most of these methods face critical challenges such as excessively large parameter sizes, reliance on lengthy inference steps, and rapidly growing historical context, which severely limit real-time performance and lack text-controlled generation capabilities. To address these challenges, we propose \\method, a novel framework designed to generate realistic, interactive",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.22096v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-26-arxiv-yume_1_5_a_text_controlled_interactive_world_generation_model.yaml"
  },
  {
    "id": "2025-12-26-arxiv-backdoor_attacks_on_prompt_driven_video_segmentation_foundation_models",
    "date": "2025-12-26",
    "title": "Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models",
    "authors": "Zongmin Zhang, Zhen Sun, Yifan Liao, Wenhan Dong, Xinlei He",
    "abstract": "Prompt-driven Video Segmentation Foundation Models (VSFMs) such as SAM2 are increasingly deployed in applications like autonomous driving and digital pathology, raising concerns about backdoor threats. Surprisingly, we find that directly transferring classic backdoor attacks (e.g., BadNet) to VSFMs is almost ineffective, with ASR below 5\\%. To understand this, we study encoder gradients and attention maps and observe that conventional training keeps gradients for clean and triggered samples larg",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.22046v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-26-arxiv-backdoor_attacks_on_prompt_driven_video_segmentation_foundation_models.yaml"
  },
  {
    "id": "2025-12-26-arxiv-context_as_a_tool_context_management_for_long_horizon_swe_agents",
    "date": "2025-12-26",
    "title": "Context as a Tool: Context Management for Long-Horizon SWE-Agents",
    "authors": "Shukai Liu, Jian Yang, Bo Jiang, Yizhi Li, Jinyang Guo",
    "abstract": "Agents based on large language models have recently shown strong potential on real-world software engineering (SWE) tasks that require long-horizon interaction with repository-scale codebases. However, most existing agents rely on append-only context maintenance or passively triggered compression heuristics, which often lead to context explosion, semantic drift, and degraded reasoning in long-running interactions. We propose CAT, a new context management paradigm that elevates context maintenanc",
    "category": "Hardware/Infra",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "hardware_infra"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.22087v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-26-arxiv-context_as_a_tool_context_management_for_long_horizon_swe_agents.yaml"
  },
  {
    "id": "2025-12-26-arxiv-sketchplay_intuitive_creation_of_physically_realistic_vr_content_with_gesture_dr",
    "date": "2025-12-26",
    "title": "SketchPlay: Intuitive Creation of Physically Realistic VR Content with Gesture-Driven Sketching",
    "authors": "Xiangwen Zhang, Xiaowei Dai, Runnan Chen, Xiaoming Chen, Zeke Zexi Hu",
    "abstract": "Creating physically realistic content in VR often requires complex modeling tools or predefined 3D models, textures, and animations, which present significant barriers for non-expert users. In this paper, we propose SketchPlay, a novel VR interaction framework that transforms humans' air-drawn sketches and gestures into dynamic, physically realistic scenes, making content creation intuitive and playful like drawing. Specifically, sketches capture the structure and spatial arrangement of objects ",
    "category": "Project",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "project"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.22016v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-26-arxiv-sketchplay_intuitive_creation_of_physically_realistic_vr_content_with_gesture_dr.yaml"
  },
  {
    "id": "2025-12-26-arxiv-proedit_inversion_based_editing_from_prompts_done_right",
    "date": "2025-12-26",
    "title": "ProEdit: Inversion-based Editing From Prompts Done Right",
    "authors": "Zhi Ouyang, Dian Zheng, Xiao-Ming Wu, Jian-Jian Jiang, Kun-Yu Lin",
    "abstract": "Inversion-based visual editing provides an effective and training-free way to edit an image or a video based on user instructions. Existing methods typically inject source image information during the sampling process to maintain editing consistency. However, this sampling strategy overly relies on source information, which negatively affects the edits in the target image (e.g., failing to change the subject's atributes like pose, number, or color as instructed). In this work, we propose ProEdit",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.22118v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-26-arxiv-proedit_inversion_based_editing_from_prompts_done_right.yaml"
  },
  {
    "id": "2025-12-26-arxiv-mai_ui_technical_report_real_world_centric_foundation_gui_agents",
    "date": "2025-12-26",
    "title": "MAI-UI Technical Report: Real-World Centric Foundation GUI Agents",
    "authors": "Hanzhang Zhou, Xu Zhang, Panrong Tong, Jianan Zhang, Liangyu Chen",
    "abstract": "The development of GUI agents could revolutionize the next generation of human-computer interaction. Motivated by this vision, we present MAI-UI, a family of foundation GUI agents spanning the full spectrum of sizes, including 2B, 8B, 32B, and 235B-A22B variants. We identify four key challenges to realistic deployment: the lack of native agent-user interaction, the limits of UI-only operation, the absence of a practical deployment architecture, and brittleness in dynamic environments. MAI-UI add",
    "category": "Hardware/Infra",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "hardware_infra"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.22047v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-26-arxiv-mai_ui_technical_report_real_world_centric_foundation_gui_agents.yaml"
  },
  {
    "id": "2025-12-26-arxiv-patch_discontinuity_mining_for_generalized_deepfake_detection",
    "date": "2025-12-26",
    "title": "Patch-Discontinuity Mining for Generalized Deepfake Detection",
    "authors": "Huanhuan Yuan, Yang Ping, Zhengqin Xu, Junyi Cao, Shuai Jia",
    "abstract": "The rapid advancement of generative artificial intelligence has enabled the creation of highly realistic fake facial images, posing serious threats to personal privacy and the integrity of online information. Existing deepfake detection methods often rely on handcrafted forensic cues and complex architectures, achieving strong performance in intra-domain settings but suffering significant degradation when confronted with unseen forgery patterns. In this paper, we propose GenDF, a simple yet effe",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.22027v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-26-arxiv-patch_discontinuity_mining_for_generalized_deepfake_detection.yaml"
  },
  {
    "id": "2025-12-24-arxiv-autonomous_uncertainty_quantification_for_computational_point_of_care_sensors",
    "date": "2025-12-24",
    "title": "Autonomous Uncertainty Quantification for Computational Point-of-care Sensors",
    "authors": "Artem Goncharov, Rajesh Ghosh, Hyou-Arm Joung, Dino Di Carlo, Aydogan Ozcan",
    "abstract": "Computational point-of-care (POC) sensors enable rapid, low-cost, and accessible diagnostics in emergency, remote and resource-limited areas that lack access to centralized medical facilities. These systems can utilize neural network-based algorithms to accurately infer a diagnosis from the signals generated by rapid diagnostic tests or sensors. However, neural network-based diagnostic models are subject to hallucinations and can produce erroneous predictions, posing a risk of misdiagnosis and i",
    "category": "Project",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "project"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.21335v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-24-arxiv-autonomous_uncertainty_quantification_for_computational_point_of_care_sensors.yaml"
  },
  {
    "id": "2025-12-24-arxiv-transcriptome_conditioned_personalized_de_novo_drug_generation_for_aml_using_met",
    "date": "2025-12-24",
    "title": "Transcriptome-Conditioned Personalized De Novo Drug Generation for AML Using Metaheuristic Assembly and Target-Driven Filtering",
    "authors": "Abdullah G. Elafifi, Basma Mamdouh, Mariam Hanafy, Muhammed Alaa Eldin, Yosef Khaled",
    "abstract": "Acute Myeloid Leukemia (AML) remains a clinical challenge due to its extreme molecular heterogeneity and high relapse rates. While precision medicine has introduced mutation-specific therapies, many patients still lack effective, personalized options. This paper presents a novel, end-to-end computational framework that bridges the gap between patient-specific transcriptomics and de novo drug discovery. By analyzing bulk RNA sequencing data from the TCGA-LAML cohort, the study utilized Weighted G",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.21301v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-24-arxiv-transcriptome_conditioned_personalized_de_novo_drug_generation_for_aml_using_met.yaml"
  },
  {
    "id": "2025-12-24-arxiv-c2llm_technical_report_a_new_frontier_in_code_retrieval_via_adaptive_cross_atten",
    "date": "2025-12-24",
    "title": "C2LLM Technical Report: A New Frontier in Code Retrieval via Adaptive Cross-Attention Pooling",
    "authors": "Jin Qin, Zihan Liao, Ziyin Zhang, Hang Yu, Peng Di",
    "abstract": "We present C2LLM - Contrastive Code Large Language Models, a family of code embedding models in both 0.5B and 7B sizes. Building upon Qwen-2.5-Coder backbones, C2LLM adopts a Pooling by Multihead Attention (PMA) module for generating sequence embedding from token embeddings, effectively 1) utilizing the LLM's causal representations acquired during pretraining, while also 2) being able to aggregate information from all tokens in the sequence, breaking the information bottleneck in EOS-based seque",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.21332v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-24-arxiv-c2llm_technical_report_a_new_frontier_in_code_retrieval_via_adaptive_cross_atten.yaml"
  },
  {
    "id": "2025-12-24-arxiv-your_reasoning_benchmark_may_not_test_reasoning_revealing_perception_bottleneck",
    "date": "2025-12-24",
    "title": "Your Reasoning Benchmark May Not Test Reasoning: Revealing Perception Bottleneck in Abstract Reasoning Benchmarks",
    "authors": "Xinhe Wang, Jin Huang, Xingjian Zhang, Tianhao Wang, Jiaqi W. Ma",
    "abstract": "Reasoning benchmarks such as the Abstraction and Reasoning Corpus (ARC) and ARC-AGI are widely used to assess progress in artificial intelligence and are often interpreted as probes of core, so-called ``fluid'' reasoning abilities. Despite their apparent simplicity for humans, these tasks remain challenging for frontier vision-language models (VLMs), a gap commonly attributed to deficiencies in machine reasoning. We challenge this interpretation and hypothesize that the gap arises primarily from",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.21329v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-24-arxiv-your_reasoning_benchmark_may_not_test_reasoning_revealing_perception_bottleneck.yaml"
  },
  {
    "id": "2025-12-24-arxiv-fast_sam2_with_text_driven_token_pruning",
    "date": "2025-12-24",
    "title": "Fast SAM2 with Text-Driven Token Pruning",
    "authors": "Avilasha Mandal, Chaoning Zhang, Fachrina Dewi Puspitasari, Xudong Wang, Jiaquan Zhang",
    "abstract": "Segment Anything Model 2 (SAM2), a vision foundation model has significantly advanced in prompt-driven video object segmentation, yet their practical deployment remains limited by the high computational and memory cost of processing dense visual tokens across time. The SAM2 pipelines typically propagate all visual tokens produced by the image encoder through downstream temporal reasoning modules, regardless of their relevance to the target object, resulting in reduced scalability due to quadrati",
    "category": "Hardware/Infra",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "hardware_infra"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.21333v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-24-arxiv-fast_sam2_with_text_driven_token_pruning.yaml"
  },
  {
    "id": "2025-12-24-arxiv-streaming_video_instruction_tuning",
    "date": "2025-12-24",
    "title": "Streaming Video Instruction Tuning",
    "authors": "Jiaer Xia, Peixian Chen, Mengdan Zhang, Xing Sun, Kaiyang Zhou",
    "abstract": "We present Streamo, a real-time streaming video LLM that serves as a general-purpose interactive assistant. Unlike existing online video models that focus narrowly on question answering or captioning, Streamo performs a broad spectrum of streaming video tasks, including real-time narration, action understanding, event captioning, temporal event grounding, and time-sensitive question answering. To develop such versatility, we construct Streamo-Instruct-465K, a large-scale instruction-following da",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.21334v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-24-arxiv-streaming_video_instruction_tuning.yaml"
  },
  {
    "id": "2025-12-24-arxiv-parallel_token_prediction_for_language_models",
    "date": "2025-12-24",
    "title": "Parallel Token Prediction for Language Models",
    "authors": "Felix Draxler, Justus Will, Farrin Marouf Sofian, Theofanis Karaletsos, Sameer Singh",
    "abstract": "We propose Parallel Token Prediction (PTP), a universal framework for parallel sequence generation in language models. PTP jointly predicts multiple dependent tokens in a single transformer call by incorporating the sampling procedure into the model. This reduces the latency bottleneck of autoregressive decoding, and avoids the restrictive independence assumptions common in existing multi-token prediction methods. We prove that PTP can represent arbitrary autoregressive sequence distributions. P",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.21323v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-24-arxiv-parallel_token_prediction_for_language_models.yaml"
  },
  {
    "id": "2025-12-24-arxiv-dreamontage_arbitrary_frame_guided_one_shot_video_generation",
    "date": "2025-12-24",
    "title": "DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation",
    "authors": "Jiawei Liu, Junqiao Li, Jiangfan Deng, Gen Li, Siyu Zhou",
    "abstract": "The \"one-shot\" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrar",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.21252v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-24-arxiv-dreamontage_arbitrary_frame_guided_one_shot_video_generation.yaml"
  },
  {
    "id": "2025-12-24-arxiv-beyond_memorization_a_multi_modal_ordinal_regression_benchmark_to_expose_popular",
    "date": "2025-12-24",
    "title": "Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models",
    "authors": "Li-Zhong Szu-Tu, Ting-Lin Wu, Chia-Jui Chang, He Syu, Yu-Lun Liu",
    "abstract": "We expose a significant popularity bias in state-of-the-art vision-language models (VLMs), which achieve up to 34% higher accuracy on famous buildings compared to ordinary ones, indicating a reliance on memorization over generalizable understanding. To systematically investigate this, we introduce the largest open benchmark for this task: the YearGuessr dataset, a collection of 55,546 building images with multi-modal attributes from 157 countries, annotated with continuous ordinal labels of thei",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.21337v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-24-arxiv-beyond_memorization_a_multi_modal_ordinal_regression_benchmark_to_expose_popular.yaml"
  },
  {
    "id": "2025-12-24-arxiv-gridit_factorized_grid_based_diffusion_for_efficient_long_image_sequence_generat",
    "date": "2025-12-24",
    "title": "GriDiT: Factorized Grid-Based Diffusion for Efficient Long Image Sequence Generation",
    "authors": "Snehal Singh Tomar, Alexandros Graikos, Arjun Krishna, Dimitris Samaras, Klaus Mueller",
    "abstract": "Modern deep learning methods typically treat image sequences as large tensors of sequentially stacked frames. However, is this straightforward representation ideal given the current state-of-the-art (SoTA)? In this work, we address this question in the context of generative models and aim to devise a more effective way of modeling image sequence data. Observing the inefficiencies and bottlenecks of current SoTA image sequence generation methods, we showcase that rather than working with large te",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.21276v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-24-arxiv-gridit_factorized_grid_based_diffusion_for_efficient_long_image_sequence_generat.yaml"
  },
  {
    "id": "2025-12-24-arxiv-surgical_scene_segmentation_using_a_spike_driven_video_transformer_with_real_tim",
    "date": "2025-12-24",
    "title": "Surgical Scene Segmentation using a Spike-Driven Video Transformer with Real-Time Potential",
    "authors": "Shihao Zou, Jingjing Li, Wei Ji, Jincai Huang, Kai Wang",
    "abstract": "Modern surgical systems increasingly rely on intelligent scene understanding to provide timely situational awareness for enhanced intra-operative safety. Within this pipeline, surgical scene segmentation plays a central role in accurately perceiving operative events. Although recent deep learning models, particularly large-scale foundation models, achieve remarkable segmentation accuracy, their substantial computational demands and power consumption hinder real-time deployment in resource-constr",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.21284v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-24-arxiv-surgical_scene_segmentation_using_a_spike_driven_video_transformer_with_real_tim.yaml"
  },
  {
    "id": "2025-12-24-arxiv-improving_the_convergence_rate_of_ray_search_optimization_for_query_efficient_ha",
    "date": "2025-12-24",
    "title": "Improving the Convergence Rate of Ray Search Optimization for Query-Efficient Hard-Label Attacks",
    "authors": "Xinjie Xu, Shuyu Cheng, Dongwei Xu, Qi Xuan, Chen Ma",
    "abstract": "In hard-label black-box adversarial attacks, where only the top-1 predicted label is accessible, the prohibitive query complexity poses a major obstacle to practical deployment. In this paper, we focus on optimizing a representative class of attacks that search for the optimal ray direction yielding the minimum $\\ell_2$-norm perturbation required to move a benign image into the adversarial region. Inspired by Nesterov's Accelerated Gradient (NAG), we propose a momentum-based algorithm, ARS-OPT, ",
    "category": "Hardware/Infra",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "hardware_infra"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.21241v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-24-arxiv-improving_the_convergence_rate_of_ray_search_optimization_for_query_efficient_ha.yaml"
  },
  {
    "id": "2025-12-24-arxiv-ticon_a_slide_level_tile_contextualizer_for_histopathology_representation_learni",
    "date": "2025-12-24",
    "title": "TICON: A Slide-Level Tile Contextualizer for Histopathology Representation Learning",
    "authors": "Varun Belagali, Saarthak Kapse, Pierre Marza, Srijan Das, Zilinghan Li",
    "abstract": "The interpretation of small tiles in large whole slide images (WSI) often needs a larger image context. We introduce TICON, a transformer-based tile representation contextualizer that produces rich, contextualized embeddings for ''any'' application in computational pathology. Standard tile encoder-based pipelines, which extract embeddings of tiles stripped from their context, fail to model the rich slide-level information essential for both local and global tasks. Furthermore, different tile-enc",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.21331v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-24-arxiv-ticon_a_slide_level_tile_contextualizer_for_histopathology_representation_learni.yaml"
  },
  {
    "id": "2025-12-24-arxiv-androidlens_long_latency_evaluation_with_nested_sub_targets_for_android_gui_agen",
    "date": "2025-12-24",
    "title": "AndroidLens: Long-latency Evaluation with Nested Sub-targets for Android GUI Agents",
    "authors": "Yue Cao, Yingyao Wang, Pi Bu, Jingxuan Xing, Wei Jiang",
    "abstract": "Graphical user interface (GUI) agents can substantially improve productivity by automating frequently executed long-latency tasks on mobile devices. However, existing evaluation benchmarks are still constrained to limited applications, simple tasks, and coarse-grained metrics. To address this, we introduce AndroidLens, a challenging evaluation framework for mobile GUI agents, comprising 571 long-latency tasks in both Chinese and English environments, each requiring an average of more than 26 ste",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.21302v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-24-arxiv-androidlens_long_latency_evaluation_with_nested_sub_targets_for_android_gui_agen.yaml"
  },
  {
    "id": "2025-12-24-arxiv-anyad_unified_any_modality_anomaly_detection_in_incomplete_multi_sequence_mri",
    "date": "2025-12-24",
    "title": "AnyAD: Unified Any-Modality Anomaly Detection in Incomplete Multi-Sequence MRI",
    "authors": "Changwei Wu, Yifei Chen, Yuxin Du, Mingxuan Liu, Jinying Zong",
    "abstract": "Reliable anomaly detection in brain MRI remains challenging due to the scarcity of annotated abnormal cases and the frequent absence of key imaging modalities in real clinical workflows. Existing single-class or multi-class anomaly detection (AD) models typically rely on fixed modality configurations, require repetitive training, or fail to generalize to unseen modality combinations, limiting their clinical scalability. In this work, we present a unified Any-Modality AD framework that performs r",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.21264v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-24-arxiv-anyad_unified_any_modality_anomaly_detection_in_incomplete_multi_sequence_mri.yaml"
  },
  {
    "id": "2025-12-24-arxiv-histream_efficient_high_resolution_video_generation_via_redundancy_eliminated_st",
    "date": "2025-12-24",
    "title": "HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming",
    "authors": "Haonan Qiu, Shikun Liu, Zijian Zhou, Zhaochong An, Weiming Ren",
    "abstract": "High-resolution video generation, while crucial for digital media and film, is computationally bottlenecked by the quadratic complexity of diffusion models, making practical inference infeasible. To address this, we introduce HiStream, an efficient autoregressive framework that systematically reduces redundancy across three axes: i) Spatial Compression: denoising at low resolution before refining at high resolution with cached features; ii) Temporal Compression: a chunk-by-chunk strategy with a ",
    "category": "Hardware/Infra",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "hardware_infra"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.21338v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-24-arxiv-histream_efficient_high_resolution_video_generation_via_redundancy_eliminated_st.yaml"
  },
  {
    "id": "2025-12-24-arxiv-reaseq_unleashing_world_knowledge_via_reasoning_for_sequential_modeling",
    "date": "2025-12-24",
    "title": "ReaSeq: Unleashing World Knowledge via Reasoning for Sequential Modeling",
    "authors": "Chuan Wang, Gaoming Yang, Han Wu, Jiakai Tang, Jiahao Yu",
    "abstract": "Industrial recommender systems face two fundamental limitations under the log-driven paradigm: (1) knowledge poverty in ID-based item representations that causes brittle interest modeling under data sparsity, and (2) systemic blindness to beyond-log user interests that constrains model performance within platform boundaries. These limitations stem from an over-reliance on shallow interaction statistics and close-looped feedback while neglecting the rich world knowledge about product semantics an",
    "category": "Project",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "project"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.21257v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-24-arxiv-reaseq_unleashing_world_knowledge_via_reasoning_for_sequential_modeling.yaml"
  },
  {
    "id": "2025-12-23-arxiv-distilling_to_hybrid_attention_models_via_kl_guided_layer_selection",
    "date": "2025-12-23",
    "title": "Distilling to Hybrid Attention Models via KL-Guided Layer Selection",
    "authors": "Yanhong Li, Songlin Yang, Shawn Tan, Mayank Mishra, Rameswar Panda",
    "abstract": "Distilling pretrained softmax attention Transformers into more efficient hybrid architectures that interleave softmax and linear attention layers is a promising approach for improving the inference efficiency of LLMs without requiring expensive pretraining from scratch. A critical factor in the conversion process is layer selection, i.e., deciding on which layers to convert to linear attention variants. This paper describes a simple and efficient recipe for layer selection that uses layer import",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.20569v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-23-arxiv-distilling_to_hybrid_attention_models_via_kl_guided_layer_selection.yaml"
  },
  {
    "id": "2025-12-23-arxiv-lead_minimizing_learner_expert_asymmetry_in_end_to_end_driving",
    "date": "2025-12-23",
    "title": "LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving",
    "authors": "Long Nguyen, Micha Fauth, Bernhard Jaeger, Daniel Dauner, Maximilian Igl",
    "abstract": "Simulators can generate virtually unlimited driving data, yet imitation learning policies in simulation still struggle to achieve robust closed-loop performance. Motivated by this gap, we empirically study how misalignment between privileged expert demonstrations and sensor-based student observations can limit the effectiveness of imitation learning. More precisely, experts have significantly higher visibility (e.g., ignoring occlusions) and far lower uncertainty (e.g., knowing other vehicles' a",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.20563v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-23-arxiv-lead_minimizing_learner_expert_asymmetry_in_end_to_end_driving.yaml"
  },
  {
    "id": "2025-12-23-arxiv-longvideoagent_multi_agent_reasoning_with_long_videos",
    "date": "2025-12-23",
    "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos",
    "authors": "Runtao Liu, Ziyi Liu, Jiaqi Tang, Yue Ma, Renjie Pi",
    "abstract": "Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent pl",
    "category": "Project",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "project"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.20618v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-23-arxiv-longvideoagent_multi_agent_reasoning_with_long_videos.yaml"
  },
  {
    "id": "2025-12-23-arxiv-emergent_temporal_abstractions_in_autoregressive_models_enable_hierarchical_rein",
    "date": "2025-12-23",
    "title": "Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning",
    "authors": "Seijin Kobayashi, Yanick Schimpf, Maximilian Schlegel, Angelika Steger, Maciej Wolczyk",
    "abstract": "Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations o",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.20605v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-23-arxiv-emergent_temporal_abstractions_in_autoregressive_models_enable_hierarchical_rein.yaml"
  },
  {
    "id": "2025-12-23-arxiv-repurposing_video_diffusion_transformers_for_robust_point_tracking",
    "date": "2025-12-23",
    "title": "Repurposing Video Diffusion Transformers for Robust Point Tracking",
    "authors": "Soowon Son, Honggyu An, Chaehyun Kim, Hyunah Ko, Jisu Nam",
    "abstract": "Point tracking aims to localize corresponding points across video frames, serving as a fundamental task for 4D reconstruction, robotics, and video editing. Existing methods commonly rely on shallow convolutional backbones such as ResNet that process frames independently, lacking temporal coherence and producing unreliable matching costs under challenging conditions. Through systematic analysis, we find that video Diffusion Transformers (DiTs), pre-trained on large-scale real-world videos with sp",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.20606v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-23-arxiv-repurposing_video_diffusion_transformers_for_robust_point_tracking.yaml"
  },
  {
    "id": "2025-12-23-arxiv-semanticgen_video_generation_in_semantic_space",
    "date": "2025-12-23",
    "title": "SemanticGen: Video Generation in Semantic Space",
    "authors": "Jianhong Bai, Xiaoshi Wu, Xintao Wang, Fu Xiao, Yuanxing Zhang",
    "abstract": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, ",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.20619v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-23-arxiv-semanticgen_video_generation_in_semantic_space.yaml"
  },
  {
    "id": "2025-12-23-arxiv-spatialtree_how_spatial_abilities_branch_out_in_mllms",
    "date": "2025-12-23",
    "title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs",
    "authors": "Yuxi Xiao, Longfei Li, Shen Yan, Xinhang Liu, Sida Peng",
    "abstract": "Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capa",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.20617v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-23-arxiv-spatialtree_how_spatial_abilities_branch_out_in_mllms.yaml"
  },
  {
    "id": "2025-12-23-arxiv-flashvlm_text_guided_visual_token_selection_for_large_multimodal_models",
    "date": "2025-12-23",
    "title": "FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models",
    "authors": "Kaitong Cai, Jusheng Zhang, Jing Yang, Yijia Fan, Pengtao Xie",
    "abstract": "Large vision-language models (VLMs) typically process hundreds or thousands of visual tokens per image or video frame, incurring quadratic attention cost and substantial redundancy. Existing token reduction methods often ignore the textual query or rely on deep attention maps, whose instability under aggressive pruning leads to degraded semantic alignment.\n  We propose FlashVLM, a text guided visual token selection framework that dynamically adapts visual inputs to the query. Instead of relying ",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.20561v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-23-arxiv-flashvlm_text_guided_visual_token_selection_for_large_multimodal_models.yaml"
  },
  {
    "id": "2025-12-23-arxiv-automated_stereotactic_radiosurgery_planning_using_a_human_in_the_loop_reasoning",
    "date": "2025-12-23",
    "title": "Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent",
    "authors": "Humza Nusrat, Luke Francisco, Bing Luo, Hassan Bagher-Ebadian, Joshua Kim",
    "abstract": "Stereotactic radiosurgery (SRS) demands precise dose shaping around critical structures, yet black-box AI systems have limited clinical adoption due to opacity concerns. We tested whether chain-of-thought reasoning improves agentic planning in a retrospective cohort of 41 patients with brain metastases treated with 18 Gy single-fraction SRS. We developed SAGE (Secure Agent for Generative Dose Expertise), an LLM-based planning agent for automated SRS treatment planning. Two variants generated pla",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.20586v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-23-arxiv-automated_stereotactic_radiosurgery_planning_using_a_human_in_the_loop_reasoning.yaml"
  },
  {
    "id": "2025-12-23-arxiv-active_intelligence_in_video_avatars_via_closed_loop_world_modeling",
    "date": "2025-12-23",
    "title": "Active Intelligence in Video Avatars via Closed-loop World Modeling",
    "authors": "Xuanhua He, Tianyu Yang, Ke Cao, Ruiqi Wu, Cheng Meng",
    "abstract": "Current video avatar generation methods excel at identity preservation and motion alignment but lack genuine agency, they cannot autonomously pursue long-term goals through adaptive environmental interaction. We address this by introducing L-IVA (Long-horizon Interactive Visual Avatar), a task and benchmark for evaluating goal-directed planning in stochastic generative environments, and ORCA (Online Reasoning and Cognitive Architecture), the first framework enabling active intelligence in video ",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.20615v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-23-arxiv-active_intelligence_in_video_avatars_via_closed_loop_world_modeling.yaml"
  },
  {
    "id": "2025-12-22-arxiv-visual_aware_cot_achieving_high_fidelity_visual_consistency_in_unified_models",
    "date": "2025-12-22",
    "title": "Visual-Aware CoT: Achieving High-Fidelity Visual Consistency in Unified Models",
    "authors": "Zixuan Ye, Quande Liu, Cong Wei, Yuanxing Zhang, Xintao Wang",
    "abstract": "Recently, the introduction of Chain-of-Thought (CoT) has largely improved the generation ability of unified models. However, it is observed that the current thinking process during generation mainly focuses on the text consistency with the text prompt, ignoring the \\textbf{visual context consistency} with the visual reference images during the multi-modal generation, e.g., multi-reference generation. The lack of such consistency results in the failure in maintaining key visual features (like hum",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.19686v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-22-arxiv-visual_aware_cot_achieving_high_fidelity_visual_consistency_in_unified_models.yaml"
  },
  {
    "id": "2025-12-22-arxiv-efficient_vision_mamba_for_mri_super_resolution_via_hybrid_selective_scanning",
    "date": "2025-12-22",
    "title": "Efficient Vision Mamba for MRI Super-Resolution via Hybrid Selective Scanning",
    "authors": "Mojtaba Safari, Shansong Wang, Vanessa L Wildman, Mingzhe Hu, Zach Eidex",
    "abstract": "Background: High-resolution MRI is critical for diagnosis, but long acquisition times limit clinical use. Super-resolution (SR) can enhance resolution post-scan, yet existing deep learning methods face fidelity-efficiency trade-offs. Purpose: To develop a computationally efficient and accurate deep learning framework for MRI SR that preserves anatomical detail for clinical integration. Materials and Methods: We propose a novel SR framework combining multi-head selective state-space models (MHSSM",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.19676v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-22-arxiv-efficient_vision_mamba_for_mri_super_resolution_via_hybrid_selective_scanning.yaml"
  },
  {
    "id": "2025-12-22-arxiv-va_variational_policy_alignment_for_pixel_aware_autoregressive_generation",
    "date": "2025-12-22",
    "title": "VA-$π$: Variational Policy Alignment for Pixel-Aware Autoregressive Generation",
    "authors": "Xinyao Liao, Qiyuan He, Kai Xu, Xiaoye Qu, Yicong Li",
    "abstract": "Autoregressive (AR) visual generation relies on tokenizers to map images to and from discrete sequences. However, tokenizers are trained to reconstruct clean images from ground-truth tokens, while AR generators are optimized only for token likelihood. This misalignment leads to generated token sequences that may decode into low-quality images, without direct supervision from the pixel space. We propose VA-$π$, a lightweight post-training framework that directly optimizes AR models with a princip",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.19680v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-22-arxiv-va_variational_policy_alignment_for_pixel_aware_autoregressive_generation.yaml"
  },
  {
    "id": "2025-12-22-arxiv-scalably_enhancing_the_clinical_validity_of_a_task_benchmark_with_physician_over",
    "date": "2025-12-22",
    "title": "Scalably Enhancing the Clinical Validity of a Task Benchmark with Physician Oversight",
    "authors": "Junze Ye, Daniel Tawfik, Alex J. Goodell, Nikhil V. Kotha, Mark K. Buyyounouski",
    "abstract": "Automating the calculation of clinical risk scores offers a significant opportunity to reduce physician administrative burden and enhance patient care. The current standard for evaluating this capability is MedCalc-Bench, a large-scale dataset constructed using LLM-based feature extraction and rule-based aggregation. However, treating such model-generated benchmarks as static oracles risks enshrining historical model errors as evaluation gold standards, a problem dangerously amplified when these",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.19691v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-22-arxiv-scalably_enhancing_the_clinical_validity_of_a_task_benchmark_with_physician_over.yaml"
  },
  {
    "id": "2025-12-22-arxiv-bottom_up_policy_optimization_your_language_model_policy_secretly_contains_inter",
    "date": "2025-12-22",
    "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies",
    "authors": "Yuqiao Tan, Minzheng Wang, Shizhu He, Huanxuan Liao, Chengfeng Zhao",
    "abstract": "Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.19673v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-22-arxiv-bottom_up_policy_optimization_your_language_model_policy_secretly_contains_inter.yaml"
  },
  {
    "id": "2025-12-22-arxiv-from_indoor_to_open_world_revealing_the_spatial_reasoning_gap_in_mllms",
    "date": "2025-12-22",
    "title": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs",
    "authors": "Mingrui Wu, Zhaozhi Wang, Fangjinhua Wang, Jiaolong Yang, Marc Pollefeys",
    "abstract": "While Multimodal Large Language Models (MLLMs) have achieved impressive performance on semantic tasks, their spatial intelligence--crucial for robust and grounded AI systems--remains underdeveloped. Existing benchmarks fall short of diagnosing this limitation: they either focus on overly simplified qualitative reasoning or rely on domain-specific indoor data, constrained by the lack of outdoor datasets with verifiable metric ground truth. To bridge this gap, we introduce a large-scale benchmark ",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.19683v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-22-arxiv-from_indoor_to_open_world_revealing_the_spatial_reasoning_gap_in_mllms.yaml"
  },
  {
    "id": "2025-12-22-arxiv-beyond_clip_knowledge_enhanced_multimodal_transformers_for_cross_modal_alignment",
    "date": "2025-12-22",
    "title": "Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis",
    "authors": "Argha Kamal Samanta, Harshika Goyal, Vasudha Joshi, Tushar Mungle, Pabitra Mitra",
    "abstract": "Diabetic retinopathy (DR) is a leading cause of preventable blindness worldwide, demanding accurate automated diagnostic systems. While general-domain vision-language models like Contrastive Language-Image Pre-Training (CLIP) perform well on natural image tasks, they struggle in medical domain applications, particularly in cross-modal retrieval for ophthalmological images. We propose a novel knowledge-enhanced joint embedding framework that integrates retinal fundus images, clinical text, and st",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.19663v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-22-arxiv-beyond_clip_knowledge_enhanced_multimodal_transformers_for_cross_modal_alignment.yaml"
  },
  {
    "id": "2025-12-22-arxiv-pushing_the_frontier_of_audiovisual_perception_with_large_scale_multimodal_corre",
    "date": "2025-12-22",
    "title": "Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning",
    "authors": "Apoorv Vyas, Heng-Jui Chang, Cheng-Fu Yang, Po-Yao Huang, Luya Gao",
    "abstract": "We introduce Perception Encoder Audiovisual, PE-AV, a new family of encoders for audio and video understanding trained with scaled contrastive learning. Built on PE, PE-AV makes several key contributions to extend representations to audio, and natively support joint embeddings across audio-video, audio-text, and video-text modalities. PE-AV's unified cross-modal embeddings enable novel tasks such as speech retrieval, and set a new state of the art across standard audio and video benchmarks. We u",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.19687v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-22-arxiv-pushing_the_frontier_of_audiovisual_perception_with_large_scale_multimodal_corre.yaml"
  },
  {
    "id": "2025-12-22-arxiv-interact2ar_full_body_human_human_interaction_generation_via_autoregressive_diff",
    "date": "2025-12-22",
    "title": "Interact2Ar: Full-Body Human-Human Interaction Generation via Autoregressive Diffusion Models",
    "authors": "Pablo Ruiz-Ponce, Sergio Escalera, José García-Rodríguez, Jiankang Deng, Rolandos Alexandros Potamias",
    "abstract": "Generating realistic human-human interactions is a challenging task that requires not only high-quality individual body and hand motions, but also coherent coordination among all interactants. Due to limitations in available data and increased learning complexity, previous methods tend to ignore hand motions, limiting the realism and expressivity of the interactions. Additionally, current diffusion-based approaches generate entire motion sequences simultaneously, limiting their ability to captur",
    "category": "Hardware/Infra",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "hardware_infra"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.19692v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-22-arxiv-interact2ar_full_body_human_human_interaction_generation_via_autoregressive_diff.yaml"
  },
  {
    "id": "2025-12-22-arxiv-zero_shot_reconstruction_of_in_scene_object_manipulation_from_video",
    "date": "2025-12-22",
    "title": "Zero-shot Reconstruction of In-Scene Object Manipulation from Video",
    "authors": "Dixuan Lin, Tianyou Wang, Zhuoyang Pan, Yufu Wang, Lingjie Liu",
    "abstract": "We build the first system to address the problem of reconstructing in-scene object manipulation from a monocular RGB video. It is challenging due to ill-posed scene reconstruction, ambiguous hand-object depth, and the need for physically plausible interactions. Existing methods operate in hand centric coordinates and ignore the scene, hindering metric accuracy and practical use. In our method, we first use data-driven foundation models to initialize the core components, including the object mesh",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.19684v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-22-arxiv-zero_shot_reconstruction_of_in_scene_object_manipulation_from_video.yaml"
  },
  {
    "id": "2025-12-22-arxiv-the_prism_hypothesis_harmonizing_semantic_and_pixel_representations_via_unified",
    "date": "2025-12-22",
    "title": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding",
    "authors": "Weichen Fan, Haiwen Diao, Quan Wang, Dahua Lin, Ziwei Liu",
    "abstract": "Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that convey",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.19693v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-22-arxiv-the_prism_hypothesis_harmonizing_semantic_and_pixel_representations_via_unified.yaml"
  },
  {
    "id": "2025-12-22-arxiv-genenv_difficulty_aligned_co_evolution_between_llm_agents_and_environment_simula",
    "date": "2025-12-22",
    "title": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators",
    "authors": "Jiacheng Guo, Ling Yang, Peter Chen, Qixin Xiao, Yinjie Wang",
    "abstract": "Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating t",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.19682v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-22-arxiv-genenv_difficulty_aligned_co_evolution_between_llm_agents_and_environment_simula.yaml"
  },
  {
    "id": "2025-12-19-arxiv-both_semantics_and_reconstruction_matter_making_representation_encoders_ready_fo",
    "date": "2025-12-19",
    "title": "Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing",
    "authors": "Shilong Zhang, He Zhang, Zhifei Zhang, Chongjian Ge, Shuchen Xue",
    "abstract": "Modern Latent Diffusion Models (LDMs) typically operate in low-level Variational Autoencoder (VAE) latent spaces that are primarily optimized for pixel-level reconstruction. To unify vision generation and understanding, a burgeoning trend is to adopt high-dimensional features from representation encoders as generative latents. However, we empirically identify two fundamental obstacles in this paradigm: (1) the discriminative feature space lacks compact regularization, making diffusion models pro",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.17909v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-19-arxiv-both_semantics_and_reconstruction_matter_making_representation_encoders_ready_fo.yaml"
  },
  {
    "id": "2025-12-19-arxiv-visually_prompted_benchmarks_are_surprisingly_fragile",
    "date": "2025-12-19",
    "title": "Visually Prompted Benchmarks Are Surprisingly Fragile",
    "authors": "Haiwen Feng, Long Lian, Lisa Dunlap, Jiahao Shu, XuDong Wang",
    "abstract": "A key challenge in evaluating VLMs is testing models' ability to analyze visual content independently from their textual priors. Recent benchmarks such as BLINK probe visual perception through visual prompting, where questions about visual content are paired with coordinates to which the question refers, with the coordinates explicitly marked in the image itself. While these benchmarks are an important part of VLM evaluation, we find that existing models are surprisingly fragile to seemingly irr",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.17875v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-19-arxiv-visually_prompted_benchmarks_are_surprisingly_fragile.yaml"
  },
  {
    "id": "2025-12-19-arxiv-learning_vertical_coordinates_via_automatic_differentiation_of_a_dynamical_core",
    "date": "2025-12-19",
    "title": "Learning vertical coordinates via automatic differentiation of a dynamical core",
    "authors": "Tim Whittaker, Seth Taylor, Elsa Cardoso-Bihlo, Alejandro Di Luca, Alex Bihlo",
    "abstract": "Terrain-following coordinates in atmospheric models often imprint their grid structure onto the solution, particularly over steep topography, where distorted coordinate layers can generate spurious horizontal and vertical motion. Standard formulations, such as hybrid or SLEVE coordinates, mitigate these errors by using analytic decay functions controlled by heuristic scale parameters that are typically tuned by hand and fixed a priori. In this work, we propose a framework to define a parametric ",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.17877v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-19-arxiv-learning_vertical_coordinates_via_automatic_differentiation_of_a_dynamical_core.yaml"
  },
  {
    "id": "2025-12-19-arxiv-distributionally_robust_imitation_learning_layered_control_architecture_for_cert",
    "date": "2025-12-19",
    "title": "Distributionally Robust Imitation Learning: Layered Control Architecture for Certifiable Autonomy",
    "authors": "Aditya Gahlawat, Ahmed Aboudonia, Sandeep Banik, Naira Hovakimyan, Nikolai Matni",
    "abstract": "Imitation learning (IL) enables autonomous behavior by learning from expert demonstrations. While more sample-efficient than comparative alternatives like reinforcement learning, IL is sensitive to compounding errors induced by distribution shifts. There are two significant sources of distribution shifts when using IL-based feedback laws on systems: distribution shifts caused by policy error and distribution shifts due to exogenous disturbances and endogenous model errors due to lack of learning",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.17899v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-19-arxiv-distributionally_robust_imitation_learning_layered_control_architecture_for_cert.yaml"
  },
  {
    "id": "2025-12-19-arxiv-anytask_an_automated_task_and_data_generation_framework_for_advancing_sim_to_rea",
    "date": "2025-12-19",
    "title": "AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning",
    "authors": "Ran Gong, Xiaohan Zhang, Jinghuan Shang, Maria Vittoria Minniti, Jigarkumar Patel",
    "abstract": "Generalist robot learning remains constrained by data: large-scale, diverse, and high-quality interaction data are expensive to collect in the real world. While simulation has become a promising way for scaling up data collection, the related tasks, including simulation task design, task-aware scene generation, expert demonstration synthesis, and sim-to-real transfer, still demand substantial human effort. We present AnyTask, an automated framework that pairs massively parallel GPU simulation wi",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.17853v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-19-arxiv-anytask_an_automated_task_and_data_generation_framework_for_advancing_sim_to_rea.yaml"
  },
  {
    "id": "2025-12-19-arxiv-keypoint_counting_classifiers_turning_vision_transformers_into_self_explainable",
    "date": "2025-12-19",
    "title": "Keypoint Counting Classifiers: Turning Vision Transformers into Self-Explainable Models Without Training",
    "authors": "Kristoffer Wickstrøm, Teresa Dorszewski, Siyan Chen, Michael Kampffmeyer, Elisabeth Wetzer",
    "abstract": "Current approaches for designing self-explainable models (SEMs) require complicated training procedures and specific architectures which makes them impractical. With the advance of general purpose foundation models based on Vision Transformers (ViTs), this impracticability becomes even more problematic. Therefore, new methods are necessary to provide transparency and reliability to ViT-based foundation models. In this work, we present a new method for turning any well-trained ViT-based model int",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.17891v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-19-arxiv-keypoint_counting_classifiers_turning_vision_transformers_into_self_explainable.yaml"
  },
  {
    "id": "2025-12-19-arxiv-diffusion_forcing_for_multi_agent_interaction_sequence_modeling",
    "date": "2025-12-19",
    "title": "Diffusion Forcing for Multi-Agent Interaction Sequence Modeling",
    "authors": "Vongani H. Maluleke, Kie Horiuchi, Lea Wilken, Evonne Ng, Jitendra Malik",
    "abstract": "Understanding and generating multi-person interactions is a fundamental challenge with broad implications for robotics and social computing. While humans naturally coordinate in groups, modeling such interactions remains difficult due to long temporal horizons, strong inter-agent dependencies, and variable group sizes. Existing motion generation methods are largely task-specific and do not generalize to flexible multi-agent generation. We introduce MAGNet (Multi-Agent Diffusion Forcing Transform",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.17900v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-19-arxiv-diffusion_forcing_for_multi_agent_interaction_sequence_modeling.yaml"
  },
  {
    "id": "2025-12-19-arxiv-infsplign_inference_time_spatial_alignment_of_text_to_image_diffusion_models",
    "date": "2025-12-19",
    "title": "InfSplign: Inference-Time Spatial Alignment of Text-to-Image Diffusion Models",
    "authors": "Sarah Rastegar, Violeta Chatalbasheva, Sieger Falkena, Anuj Singh, Yanbo Wang",
    "abstract": "Text-to-image (T2I) diffusion models generate high-quality images but often fail to capture the spatial relations specified in text prompts. This limitation can be traced to two factors: lack of fine-grained spatial supervision in training data and inability of text embeddings to encode spatial semantics. We introduce InfSplign, a training-free inference-time method that improves spatial alignment by adjusting the noise through a compound loss in every denoising step. Proposed loss leverages dif",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.17851v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-19-arxiv-infsplign_inference_time_spatial_alignment_of_text_to_image_diffusion_models.yaml"
  },
  {
    "id": "2025-12-19-arxiv-radargen_automotive_radar_point_cloud_generation_from_cameras",
    "date": "2025-12-19",
    "title": "RadarGen: Automotive Radar Point Cloud Generation from Cameras",
    "authors": "Tomer Borreda, Fangqiang Ding, Sanja Fidler, Shengyu Huang, Or Litany",
    "abstract": "We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery. RadarGen adapts efficient image-latent diffusion to the radar domain by representing radar measurements in bird's-eye-view form that encodes spatial structure together with radar cross section (RCS) and Doppler attributes. A lightweight recovery step reconstructs point clouds from the generated maps. To better align generation with the visual scene, RadarGen incorporate",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.17897v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-19-arxiv-radargen_automotive_radar_point_cloud_generation_from_cameras.yaml"
  },
  {
    "id": "2025-12-19-arxiv-when_reasoning_meets_its_laws",
    "date": "2025-12-19",
    "title": "When Reasoning Meets Its Laws",
    "authors": "Junyu Zhang, Yifan Sun, Tianang Leng, Jingyan Shen, Liu Ziyin",
    "abstract": "Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we ext",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.17901v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-19-arxiv-when_reasoning_meets_its_laws.yaml"
  },
  {
    "id": "2025-12-18-arxiv-adatooler_v_adaptive_tool_use_for_images_and_videos",
    "date": "2025-12-18",
    "title": "AdaTooler-V: Adaptive Tool-Use for Images and Videos",
    "authors": "Chaoyang Wang, Kaituo Feng, Dongyang Chen, Zhongyu Wang, Zhixun Li",
    "abstract": "Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.16918v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-18-arxiv-adatooler_v_adaptive_tool_use_for_images_and_videos.yaml"
  },
  {
    "id": "2025-12-18-arxiv-the_world_is_your_canvas_painting_promptable_events_with_reference_images_trajec",
    "date": "2025-12-18",
    "title": "The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text",
    "authors": "Hanlin Wang, Hao Ouyang, Qiuyu Wang, Yue Yu, Yihao Meng",
    "abstract": "We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable even",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.16924v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-18-arxiv-the_world_is_your_canvas_painting_promptable_events_with_reference_images_trajec.yaml"
  },
  {
    "id": "2025-12-18-nature-multimodal_out_of_distribution_individual_uncertainty_quantification_enhances_bi",
    "date": "2025-12-18",
    "title": "Multimodal out-of-distribution individual uncertainty quantification enhances binding affinity prediction for polypharmacology",
    "authors": "Amitesh Badkul, Li Xie, Shuo Zhang, Lei Xie",
    "abstract": "<p>Nature Machine Intelligence, Published online: 18 December 2025; <a href=\"https://www.nature.com/articles/s42256-025-01151-2\">doi:10.1038/s42256-025-01151-2</a></p>Badkul et al. develop eMOSAIC, a method that improves drug discovery by accurately predicting the interaction mechanics of various compounds with multiple proteins.",
    "category": "Model",
    "source": "nature",
    "score": 110,
    "tags": [
      "ai",
      "research",
      "nature",
      "model"
    ],
    "links": [
      {
        "title": "View Paper",
        "url": "https://www.nature.com/articles/s42256-025-01151-2"
      },
      {
        "title": "DOI",
        "url": "https://doi.org/10.1038/s42256-025-01151-2"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-18-nature-multimodal_out_of_distribution_individual_uncertainty_quantification_enhances_bi.yaml"
  },
  {
    "id": "2025-12-18-nature-learning_cell_dynamics_with_neural_differential_equations",
    "date": "2025-12-18",
    "title": "Learning cell dynamics with neural differential equations",
    "authors": "Michael E. Vinyard, Anders W. Rasmussen, Ruitong Li, Allon M. Klein, Gad Getz",
    "abstract": "<p>Nature Machine Intelligence, Published online: 18 December 2025; <a href=\"https://www.nature.com/articles/s42256-025-01150-3\">doi:10.1038/s42256-025-01150-3</a></p>Vinyard et al. present a generative method to model cell dynamics using neural stochastic differential equations that learn state-dependent drift and diffusion, outperforming existing approaches and enabling perturbation studies of development and disease.",
    "category": "Model",
    "source": "nature",
    "score": 115,
    "tags": [
      "ai",
      "research",
      "nature",
      "model"
    ],
    "links": [
      {
        "title": "View Paper",
        "url": "https://www.nature.com/articles/s42256-025-01150-3"
      },
      {
        "title": "DOI",
        "url": "https://doi.org/10.1038/s42256-025-01150-3"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-18-nature-learning_cell_dynamics_with_neural_differential_equations.yaml"
  },
  {
    "id": "2025-12-18-arxiv-next_embedding_prediction_makes_strong_vision_learners",
    "date": "2025-12-18",
    "title": "Next-Embedding Prediction Makes Strong Vision Learners",
    "authors": "Sihan Xu, Ziqiao Ma, Wenhao Chai, Xuweiyi Chen, Weiyang Jin",
    "abstract": "Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gra",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.16922v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-18-arxiv-next_embedding_prediction_makes_strong_vision_learners.yaml"
  },
  {
    "id": "2025-12-18-nature-a_psychometric_framework_for_evaluating_and_shaping_personality_traits_in_large",
    "date": "2025-12-18",
    "title": "A psychometric framework for evaluating and shaping personality traits in large language models",
    "authors": "Gregory Serapio-García, Mustafa Safdari, Clément Crepy, Luning Sun, Stephen Fitz",
    "abstract": "<p>Nature Machine Intelligence, Published online: 18 December 2025; <a href=\"https://www.nature.com/articles/s42256-025-01115-6\">doi:10.1038/s42256-025-01115-6</a></p>Serapio-García, Safdari and colleagues develop a method based on psychometric tests to measure and validate personality-like traits in LLMs. Large, instruction-tuned models give reliable personality measurement results, and specific personality profiles can be mimicked in downstream tasks.",
    "category": "Evaluation",
    "source": "nature",
    "score": 115,
    "tags": [
      "ai",
      "research",
      "nature",
      "evaluation"
    ],
    "links": [
      {
        "title": "View Paper",
        "url": "https://www.nature.com/articles/s42256-025-01115-6"
      },
      {
        "title": "DOI",
        "url": "https://doi.org/10.1038/s42256-025-01115-6"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-18-nature-a_psychometric_framework_for_evaluating_and_shaping_personality_traits_in_large.yaml"
  },
  {
    "id": "2025-12-18-arxiv-stereopilot_learning_unified_and_efficient_stereo_conversion_via_generative_prio",
    "date": "2025-12-18",
    "title": "StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors",
    "authors": "Guibao Shen, Yihua Du, Wenhang Ge, Jing He, Chirui Chang",
    "abstract": "The rapid growth of stereoscopic displays, including VR headsets and 3D cinemas, has led to increasing demand for high-quality stereo video content. However, producing 3D videos remains costly and complex, while automatic Monocular-to-Stereo conversion is hindered by the limitations of the multi-stage ``Depth-Warp-Inpaint'' (DWI) pipeline. This paradigm suffers from error propagation, depth ambiguity, and format inconsistency between parallel and converged stereo configurations. To address these",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.16915v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-18-arxiv-stereopilot_learning_unified_and_efficient_stereo_conversion_via_generative_prio.yaml"
  },
  {
    "id": "2025-12-18-arxiv-easyv2v_a_high_quality_instruction_based_video_editing_framework",
    "date": "2025-12-18",
    "title": "EasyV2V: A High-quality Instruction-based Video Editing Framework",
    "authors": "Jinjie Mai, Chaoyang Wang, Guocheng Gordon Qian, Willi Menapace, Sergey Tulyakov",
    "abstract": "While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \\emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine mo",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.16920v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-18-arxiv-easyv2v_a_high_quality_instruction_based_video_editing_framework.yaml"
  },
  {
    "id": "2025-12-18-arxiv-differences_that_matter_auditing_models_for_capability_gap_discovery_and_rectifi",
    "date": "2025-12-18",
    "title": "Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification",
    "authors": "Qihao Liu, Chengzhi Mao, Yaojie Liu, Alan Yuille, Wen-Sheng Chu",
    "abstract": "Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once ",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.16921v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-18-arxiv-differences_that_matter_auditing_models_for_capability_gap_discovery_and_rectifi.yaml"
  },
  {
    "id": "2025-12-18-arxiv-exploration_v_s_exploitation_rethinking_rlvr_through_clipping_entropy_and_spurio",
    "date": "2025-12-18",
    "title": "Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward",
    "authors": "Peter Chen, Xiaopeng Li, Ziniu Li, Wotao Yin, Xi Chen",
    "abstract": "This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward ",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.16912v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-18-arxiv-exploration_v_s_exploitation_rethinking_rlvr_through_clipping_entropy_and_spurio.yaml"
  },
  {
    "id": "2025-12-18-arxiv-dvgt_driving_visual_geometry_transformer",
    "date": "2025-12-18",
    "title": "DVGT: Driving Visual Geometry Transformer",
    "authors": "Sicheng Zuo, Zixun Xie, Wenzhao Zheng, Shaoqing Xu, Fang Li",
    "abstract": "Perceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs. We first extract visual features for each image using a DINO backbone, and emp",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.16919v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-18-arxiv-dvgt_driving_visual_geometry_transformer.yaml"
  },
  {
    "id": "2025-12-18-arxiv-generative_adversarial_reasoner_enhancing_llm_reasoning_with_adversarial_reinfor",
    "date": "2025-12-18",
    "title": "Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning",
    "authors": "Qihao Liu, Luoxin Ye, Wufei Ma, Yu-Cheng Chou, Alan Yuille",
    "abstract": "Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule part",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.16917v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-18-arxiv-generative_adversarial_reasoner_enhancing_llm_reasoning_with_adversarial_reinfor.yaml"
  },
  {
    "id": "2025-12-18-arxiv-depth_any_panoramas_a_foundation_model_for_panoramic_depth_estimation",
    "date": "2025-12-18",
    "title": "Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation",
    "authors": "Xin Lin, Meixi Song, Dizhe Zhang, Wenxuan Lu, Haodong Li",
    "abstract": "In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.16913v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-18-arxiv-depth_any_panoramas_a_foundation_model_for_panoramic_depth_estimation.yaml"
  },
  {
    "id": "2025-12-17-arxiv-gatefusion_hierarchical_gated_cross_modal_fusion_for_active_speaker_detection",
    "date": "2025-12-17",
    "title": "GateFusion: Hierarchical Gated Cross-Modal Fusion for Active Speaker Detection",
    "authors": "Yu Wang, Juhyung Ha, Frangil M. Ramirez, Yuchen Wang, David J. Crandall",
    "abstract": "Active Speaker Detection (ASD) aims to identify who is currently speaking in each frame of a video. Most state-of-the-art approaches rely on late fusion to combine visual and audio features, but late fusion often fails to capture fine-grained cross-modal interactions, which can be critical for robust performance in unconstrained scenarios. In this paper, we introduce GateFusion, a novel architecture that combines strong pretrained unimodal encoders with a Hierarchical Gated Fusion Decoder (HiGat",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.15707v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-17-arxiv-gatefusion_hierarchical_gated_cross_modal_fusion_for_active_speaker_detection.yaml"
  },
  {
    "id": "2025-12-17-arxiv-predictive_concept_decoders_training_scalable_end_to_end_interpretability_assist",
    "date": "2025-12-17",
    "title": "Predictive Concept Decoders: Training Scalable End-to-End Interpretability Assistants",
    "authors": "Vincent Huang, Dami Choi, Daniel D. Johnson, Sarah Schwettmann, Jacob Steinhardt",
    "abstract": "Interpreting the internal activations of neural networks can produce more faithful explanations of their behavior, but is difficult due to the complex structure of activation space. Existing approaches to scalable interpretability use hand-designed agents that make and test hypotheses about how internal activations relate to external behavior. We propose to instead turn this task into an end-to-end training objective, by training interpretability assistants to accurately predict model behavior f",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.15712v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-17-arxiv-predictive_concept_decoders_training_scalable_end_to_end_interpretability_assist.yaml"
  },
  {
    "id": "2025-12-17-arxiv-skyra_ai_generated_video_detection_via_grounded_artifact_reasoning",
    "date": "2025-12-17",
    "title": "Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning",
    "authors": "Yifei Li, Wenzhao Zheng, Yanran Zhang, Runze Sun, Yu Zheng",
    "abstract": "The misuse of AI-driven video generation technologies has raised serious social concerns, highlighting the urgent need for reliable AI-generated video detectors. However, most existing methods are limited to binary classification and lack the necessary explanations for human interpretation. In this paper, we present Skyra, a specialized multimodal large language model (MLLM) that identifies human-perceivable visual artifacts in AI-generated videos and leverages them as grounded evidence for both",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.15693v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-17-arxiv-skyra_ai_generated_video_detection_via_grounded_artifact_reasoning.yaml"
  },
  {
    "id": "2025-12-17-arxiv-frontiercs_evolving_challenges_for_evolving_intelligence",
    "date": "2025-12-17",
    "title": "FrontierCS: Evolving Challenges for Evolving Intelligence",
    "authors": "Qiuyang Mang, Wenhao Chai, Zhifei Li, Huanzhi Mao, Shang Zhou",
    "abstract": "We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated. Models solve these tasks by implementing executable programs rather",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.15699v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-17-arxiv-frontiercs_evolving_challenges_for_evolving_intelligence.yaml"
  },
  {
    "id": "2025-12-17-arxiv-vlic_vision_language_models_as_perceptual_judges_for_human_aligned_image_compres",
    "date": "2025-12-17",
    "title": "VLIC: Vision-Language Models As Perceptual Judges for Human-Aligned Image Compression",
    "authors": "Kyle Sargent, Ruiqi Gao, Philipp Henzler, Charles Herrmann, Aleksander Holynski",
    "abstract": "Evaluations of image compression performance which include human preferences have generally found that naive distortion functions such as MSE are insufficiently aligned to human perception. In order to align compression models to human perception, prior work has employed differentiable perceptual losses consisting of neural networks calibrated on large-scale datasets of human psycho-visual judgments. We show that, surprisingly, state-of-the-art vision-language models (VLMs) can replicate binary ",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.15701v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-17-arxiv-vlic_vision_language_models_as_perceptual_judges_for_human_aligned_image_compres.yaml"
  },
  {
    "id": "2025-12-17-arxiv-dynamic_rebatching_for_efficient_early_exit_inference_with_drex",
    "date": "2025-12-17",
    "title": "Dynamic Rebatching for Efficient Early-Exit Inference with DREX",
    "authors": "Xuting Liu, Daniel Alexander, Siva Kesava Reddy Kakarla, Behnaz Arzani, Vincent Liu",
    "abstract": "Early-Exit (EE) is a Large Language Model (LLM) architecture that accelerates inference by allowing easier tokens to be generated using only a subset of the model's layers. However, traditional batching frameworks are ill-suited for EE LLMs, as not all requests in a batch may be ready to exit at the same time. Existing solutions either force a uniform decision on the batch, which overlooks EE opportunities, or degrade output quality by forcing premature exits. We propose Dynamic Rebatching, a so",
    "category": "Hardware/Infra",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "hardware_infra"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.15705v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-17-arxiv-dynamic_rebatching_for_efficient_early_exit_inference_with_drex.yaml"
  },
  {
    "id": "2025-12-17-arxiv-end_to_end_training_for_autoregressive_video_diffusion_via_self_resampling",
    "date": "2025-12-17",
    "title": "End-to-End Training for Autoregressive Video Diffusion via Self-Resampling",
    "authors": "Yuwei Guo, Ceyuan Yang, Hao He, Yang Zhao, Meng Wei",
    "abstract": "Autoregressive video diffusion models hold promise for world simulation but are vulnerable to exposure bias arising from the train-test mismatch. While recent works address this via post-training, they typically rely on a bidirectional teacher model or online discriminator. To achieve an end-to-end solution, we introduce Resampling Forcing, a teacher-free framework that enables training autoregressive video models from scratch and at scale. Central to our approach is a self-resampling scheme tha",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.15702v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-17-arxiv-end_to_end_training_for_autoregressive_video_diffusion_via_self_resampling.yaml"
  },
  {
    "id": "2025-12-17-arxiv-spatia_video_generation_with_updatable_spatial_memory",
    "date": "2025-12-17",
    "title": "Spatia: Video Generation with Updatable Spatial Memory",
    "authors": "Jinjing Zhao, Fangyun Wei, Zhening Liu, Hongyang Zhang, Chang Xu",
    "abstract": "Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement desig",
    "category": "Hardware/Infra",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "hardware_infra"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.15716v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-17-arxiv-spatia_video_generation_with_updatable_spatial_memory.yaml"
  },
  {
    "id": "2025-12-17-nature-network_aware_self_supervised_learning_enables_high_content_phenotypic_screening",
    "date": "2025-12-17",
    "title": "Network-aware self-supervised learning enables high-content phenotypic screening for genetic modifiers of neuronal activity dynamics",
    "authors": "Parker Grosjean, Kaivalya Shevade, Cuong Nguyen, Sarah Ancheta, Karl Mader",
    "abstract": "<p>Nature Machine Intelligence, Published online: 17 December 2025; <a href=\"https://www.nature.com/articles/s42256-025-01156-x\">doi:10.1038/s42256-025-01156-x</a></p>Grosjean et al. present a network-aware, self-supervised learning approach for screening neuronal activity dynamics. They demonstrate its applicability across a range of neural interventions.",
    "category": "Model",
    "source": "nature",
    "score": 115,
    "tags": [
      "ai",
      "research",
      "nature",
      "model"
    ],
    "links": [
      {
        "title": "View Paper",
        "url": "https://www.nature.com/articles/s42256-025-01156-x"
      },
      {
        "title": "DOI",
        "url": "https://doi.org/10.1038/s42256-025-01156-x"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-17-nature-network_aware_self_supervised_learning_enables_high_content_phenotypic_screening.yaml"
  },
  {
    "id": "2025-12-17-nature-solving_finite_element_methods_with_spiking_networks",
    "date": "2025-12-17",
    "title": "Solving finite element methods with spiking networks",
    "authors": "Wenhao Song, Zixu Wang, J. Joshua Yang",
    "abstract": "<p>Nature Machine Intelligence, Published online: 17 December 2025; <a href=\"https://www.nature.com/articles/s42256-025-01158-9\">doi:10.1038/s42256-025-01158-9</a></p>Brain-inspired computing can enhance the finite element method, a cornerstone of scientific modelling, by reducing energy costs and reframing numerical simulation through neural dynamics.",
    "category": "Model",
    "source": "nature",
    "score": 110,
    "tags": [
      "ai",
      "research",
      "nature",
      "model"
    ],
    "links": [
      {
        "title": "View Paper",
        "url": "https://www.nature.com/articles/s42256-025-01158-9"
      },
      {
        "title": "DOI",
        "url": "https://doi.org/10.1038/s42256-025-01158-9"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-17-nature-solving_finite_element_methods_with_spiking_networks.yaml"
  },
  {
    "id": "2025-12-17-arxiv-in_pursuit_of_pixel_supervision_for_visual_pre_training",
    "date": "2025-12-17",
    "title": "In Pursuit of Pixel Supervision for Visual Pre-training",
    "authors": "Lihe Yang, Shang-Wen Li, Yang Li, Xinjie Lei, Dong Wang",
    "abstract": "At the most basic level, pixels are the source of the visual information through which we perceive the world. Pixels contain information at all levels, ranging from low-level attributes to high-level concepts. Autoencoders represent a classical and long-standing paradigm for learning representations from pixels or other raw inputs. In this work, we demonstrate that autoencoder-based self-supervised learning remains competitive today and can produce strong representations for downstream tasks, wh",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.15715v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-17-arxiv-in_pursuit_of_pixel_supervision_for_visual_pre_training.yaml"
  },
  {
    "id": "2025-12-17-arxiv-mimic_video_video_action_models_for_generalizable_robot_control_beyond_vlas",
    "date": "2025-12-17",
    "title": "mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs",
    "authors": "Jonas Pai, Liam Achenbach, Victoriano Montesinos, Benedek Forrai, Oier Mees",
    "abstract": "Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data. As a result, despite improved semantic generalization, the policy must implicitly infer complex physical dynamics and temporal dependencies solely from robot trajectories. This reliance creates an unsustainable data burden, necessitating continuous, large-scale expert data collection to compensate for the lack of innate phy",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.15692v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-17-arxiv-mimic_video_video_action_models_for_generalizable_robot_control_beyond_vlas.yaml"
  },
  {
    "id": "2025-12-17-arxiv-diffusionvl_translating_any_autoregressive_models_into_diffusion_vision_language",
    "date": "2025-12-17",
    "title": "DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models",
    "authors": "Lunbin Zeng, Jingfeng Yao, Bencheng Liao, Hongyuan Tao, Wenyu Liu",
    "abstract": "In recent multimodal research, the diffusion paradigm has emerged as a promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In res",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.15713v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-17-arxiv-diffusionvl_translating_any_autoregressive_models_into_diffusion_vision_language.yaml"
  },
  {
    "id": "2025-12-17-arxiv-gaussian_pixel_codec_avatars_a_hybrid_representation_for_efficient_rendering",
    "date": "2025-12-17",
    "title": "Gaussian Pixel Codec Avatars: A Hybrid Representation for Efficient Rendering",
    "authors": "Divam Gupta, Anuj Pahuja, Nemanja Bartolovic, Tomas Simon, Forrest Iandola",
    "abstract": "We present Gaussian Pixel Codec Avatars (GPiCA), photorealistic head avatars that can be generated from multi-view images and efficiently rendered on mobile devices. GPiCA utilizes a unique hybrid representation that combines a triangle mesh and anisotropic 3D Gaussians. This combination maximizes memory and rendering efficiency while maintaining a photorealistic appearance. The triangle mesh is highly efficient in representing surface areas like facial skin, while the 3D Gaussians effectively h",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.15711v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-17-arxiv-gaussian_pixel_codec_avatars_a_hybrid_representation_for_efficient_rendering.yaml"
  },
  {
    "id": "2025-12-16-arxiv-memflow_flowing_adaptive_memory_for_consistent_and_efficient_long_video_narrativ",
    "date": "2025-12-16",
    "title": "MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives",
    "authors": "Sihui Ji, Xi Chen, Shuai Yang, Xin Tao, Pengfei Wan",
    "abstract": "The core challenge for streaming video generation is maintaining the content consistency in long context, which poses high requirement for the memory design. Most existing solutions maintain the memory by compressing historical frames with predefined strategies. However, different to-generate video chunks should refer to different historical cues, which is hard to satisfy with fixed strategies. In this work, we propose MemFlow to address this problem. Specifically, before generating the coming c",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.14699v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-16-arxiv-memflow_flowing_adaptive_memory_for_consistent_and_efficient_long_video_narrativ.yaml"
  },
  {
    "id": "2025-12-16-arxiv-native_and_compact_structured_latents_for_3d_generation",
    "date": "2025-12-16",
    "title": "Native and Compact Structured Latents for 3D Generation",
    "authors": "Jianfeng Xiang, Xiaoxue Chen, Sicheng Xu, Ruicheng Wang, Zelong Lv",
    "abstract": "Recent advancements in 3D generative modeling have significantly improved the generation realism, yet the field is still hampered by existing representations, which struggle to capture assets with complex topologies and detailed appearance. This paper present an approach for learning a structured latent representation from native 3D data to address this challenge. At its core is a new sparse voxel structure called O-Voxel, an omni-voxel representation that encodes both geometry and appearance. O",
    "category": "Hardware/Infra",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "hardware_infra"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.14692v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-16-arxiv-native_and_compact_structured_latents_for_3d_generation.yaml"
  },
  {
    "id": "2025-12-16-arxiv-early_warning_index_for_patient_deteriorations_in_hospitals",
    "date": "2025-12-16",
    "title": "Early Warning Index for Patient Deteriorations in Hospitals",
    "authors": "Dimitris Bertsimas, Yu Ma, Kimberly Villalobos Carballo, Gagan Singh, Michal Laskowski",
    "abstract": "Hospitals lack automated systems to harness the growing volume of heterogeneous clinical and operational data to effectively forecast critical events. Early identification of patients at risk for deterioration is essential not only for patient care quality monitoring but also for physician care management. However, translating varied data streams into accurate and interpretable risk assessments poses significant challenges due to inconsistent data formats. We develop a multimodal machine learnin",
    "category": "Project",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "project"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.14683v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-16-arxiv-early_warning_index_for_patient_deteriorations_in_hospitals.yaml"
  },
  {
    "id": "2025-12-16-arxiv-crisp_contact_guided_real2sim_from_monocular_video_with_planar_scene_primitives",
    "date": "2025-12-16",
    "title": "CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives",
    "authors": "Zihan Wang, Jiashun Wang, Jeff Tan, Yiwen Zhao, Jessica Hodgins",
    "abstract": "We introduce CRISP, a method that recovers simulatable human motion and scene geometry from monocular video. Prior work on joint human-scene reconstruction relies on data-driven priors and joint optimization with no physics in the loop, or recovers noisy geometry with artifacts that cause motion tracking policies with scene interactions to fail. In contrast, our key insight is to recover convex, clean, and simulation-ready geometry by fitting planar primitives to a point cloud reconstruction of ",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.14696v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-16-arxiv-crisp_contact_guided_real2sim_from_monocular_video_with_planar_scene_primitives.yaml"
  },
  {
    "id": "2025-12-16-arxiv-timelens_rethinking_video_temporal_grounding_with_multimodal_llms",
    "date": "2025-12-16",
    "title": "TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs",
    "authors": "Jun Zhang, Teng Wang, Yuying Ge, Yixiao Ge, Xinhao Li",
    "abstract": "This paper does not introduce a novel method but instead establishes a straightforward, incremental, yet essential baseline for video temporal grounding (VTG), a core capability in video understanding. While multimodal large language models (MLLMs) excel at various video understanding tasks, the recipes for optimizing them for VTG remain under-explored. In this paper, we present TimeLens, a systematic investigation into building MLLMs with strong VTG ability, along two primary dimensions: data q",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.14698v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-16-arxiv-timelens_rethinking_video_temporal_grounding_with_multimodal_llms.yaml"
  },
  {
    "id": "2025-12-16-arxiv-universal_reasoning_model",
    "date": "2025-12-16",
    "title": "Universal Reasoning Model",
    "authors": "Zitian Gao, Lynx Chen, Yihao Xiao, He Xing, Ran Tao",
    "abstract": "Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.14693v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-16-arxiv-universal_reasoning_model.yaml"
  },
  {
    "id": "2025-12-16-arxiv-chip_adaptive_compliance_for_humanoid_control_through_hindsight_perturbation",
    "date": "2025-12-16",
    "title": "CHIP: Adaptive Compliance for Humanoid Control through Hindsight Perturbation",
    "authors": "Sirui Chen, Zi-ang Cao, Zhengyi Luo, Fernando Castañeda, Chenran Li",
    "abstract": "Recent progress in humanoid robots has unlocked agile locomotion skills, including backflipping, running, and crawling. Yet it remains challenging for a humanoid robot to perform forceful manipulation tasks such as moving objects, wiping, and pushing a cart. We propose adaptive Compliance Humanoid control through hIsight Perturbation (CHIP), a plug-and-play module that enables controllable end-effector stiffness while preserving agile tracking of dynamic reference motions. CHIP is easy to implem",
    "category": "Policy",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "policy"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.14689v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-16-arxiv-chip_adaptive_compliance_for_humanoid_control_through_hindsight_perturbation.yaml"
  },
  {
    "id": "2025-12-16-arxiv-spherical_leech_quantization_for_visual_tokenization_and_generation",
    "date": "2025-12-16",
    "title": "Spherical Leech Quantization for Visual Tokenization and Generation",
    "authors": "Yue Zhao, Hanwen Jiang, Zhenlin Xu, Chutong Yang, Ehsan Adeli",
    "abstract": "Non-parametric quantization has received much attention due to its efficiency on parameters and scalability to a large codebook. In this paper, we present a unified formulation of different non-parametric quantization methods through the lens of lattice coding. The geometry of lattice codes explains the necessity of auxiliary loss terms when training auto-encoders with certain existing lookup-free quantization variants such as BSQ. As a step forward, we explore a few possible candidates, includi",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.14697v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-16-arxiv-spherical_leech_quantization_for_visual_tokenization_and_generation.yaml"
  },
  {
    "id": "2025-12-16-arxiv-mmgr_multi_modal_generative_reasoning",
    "date": "2025-12-16",
    "title": "MMGR: Multi-Modal Generative Reasoning",
    "authors": "Zefan Cai, Haoyi Qiu, Tianyi Ma, Haozhe Zhao, Gengze Zhou",
    "abstract": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework ",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.14691v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-16-arxiv-mmgr_multi_modal_generative_reasoning.yaml"
  },
  {
    "id": "2025-12-16-arxiv-spoken_dialogsum_an_emotion_rich_conversational_dataset_for_spoken_dialogue_summ",
    "date": "2025-12-16",
    "title": "Spoken DialogSum: An Emotion-Rich Conversational Dataset for Spoken Dialogue Summarization",
    "authors": "Yen-Ju Lu, Kunxiao Gao, Mingrui Liang, Helin Wang, Thomas Thebaud",
    "abstract": "Recent audio language models can follow long conversations. However, research on emotion-aware or spoken dialogue summarization is constrained by the lack of data that links speech, summaries, and paralinguistic cues. We introduce Spoken DialogSum, the first corpus aligning raw conversational audio with factual summaries, emotion-rich summaries, and utterance-level labels for speaker age, gender, and emotion. The dataset is built in two stages: first, an LLM rewrites DialogSum scripts with Switc",
    "category": "Project",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "project"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.14687v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-16-arxiv-spoken_dialogsum_an_emotion_rich_conversational_dataset_for_spoken_dialogue_summ.yaml"
  },
  {
    "id": "2025-12-15-arxiv-agentiad_tool_augmented_single_agent_for_industrial_anomaly_detection",
    "date": "2025-12-15",
    "title": "AgentIAD: Tool-Augmented Single-Agent for Industrial Anomaly Detection",
    "authors": "Junwen Miao, Penghui Du, Yi Liu, Yu Wang, Yan Wang",
    "abstract": "Industrial anomaly detection (IAD) is difficult due to the scarcity of normal reference samples and the subtle, localized nature of many defects. Single-pass vision-language models (VLMs) often overlook small abnormalities and lack explicit mechanisms to compare against canonical normal patterns. We propose AgentIAD, a tool-driven agentic framework that enables multi-stage visual inspection. The agent is equipped with a Perceptive Zoomer (PZ) for localized fine-grained analysis and a Comparative",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.13671v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-15-arxiv-agentiad_tool_augmented_single_agent_for_industrial_anomaly_detection.yaml"
  },
  {
    "id": "2025-12-15-arxiv-laser_layer_wise_scale_alignment_for_training_free_streaming_4d_reconstruction",
    "date": "2025-12-15",
    "title": "LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction",
    "authors": "Tianye Ding, Yiming Xie, Yiqing Liang, Moitreya Chatterjee, Pedro Miraldo",
    "abstract": "Recent feed-forward reconstruction models like VGGT and $π^3$ achieve impressive reconstruction quality but cannot process streaming videos due to quadratic memory complexity, limiting their practical deployment. While existing streaming methods address this through learned memory mechanisms or causal attention, they require extensive retraining and may not fully leverage the strong geometric priors of state-of-the-art offline models. We propose LASER, a training-free framework that converts an ",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.13680v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-15-arxiv-laser_layer_wise_scale_alignment_for_training_free_streaming_4d_reconstruction.yaml"
  },
  {
    "id": "2025-12-15-arxiv-a_scientific_reasoning_model_for_organic_synthesis_procedure_generation",
    "date": "2025-12-15",
    "title": "A Scientific Reasoning Model for Organic Synthesis Procedure Generation",
    "authors": "Guoqing Liu, Junren Li, Zihan Zhao, Eray Inanc, Krzysztof Maziarz",
    "abstract": "Solving computer-aided synthesis planning is essential for enabling fully automated, robot-assisted synthesis workflows and improving the efficiency of drug discovery. A key challenge, however, is bridging the gap between computational route design and practical laboratory execution, particularly the accurate prediction of viable experimental procedures for each synthesis step. In this work, we present QFANG, a scientific reasoning language model capable of generating precise, structured experim",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.13668v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-15-arxiv-a_scientific_reasoning_model_for_organic_synthesis_procedure_generation.yaml"
  },
  {
    "id": "2025-12-15-arxiv-towards_interactive_intelligence_for_digital_humans",
    "date": "2025-12-15",
    "title": "Towards Interactive Intelligence for Digital Humans",
    "authors": "Yiyi Cai, Xuangeng Chu, Xiwei Gao, Sitong Gong, Yifei Huang",
    "abstract": "We introduce Interactive Intelligence, a novel paradigm of digital human that is capable of personality-aligned expression, adaptive interaction, and self-evolution. To realize this, we present Mio (Multimodal Interactive Omni-Avatar), an end-to-end framework composed of five specialized modules: Thinker, Talker, Face Animator, Body Animator, and Renderer. This unified architecture integrates cognitive reasoning with real-time multimodal embodiment to enable fluid, consistent interaction. Furthe",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.13674v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-15-arxiv-towards_interactive_intelligence_for_digital_humans.yaml"
  },
  {
    "id": "2025-12-15-arxiv-from_code_to_field_evaluating_the_robustness_of_convolutional_neural_networks_fo",
    "date": "2025-12-15",
    "title": "From Code to Field: Evaluating the Robustness of Convolutional Neural Networks for Disease Diagnosis in Mango Leaves",
    "authors": "Gabriel Vitorino de Andrade, Saulo Roberto dos Santos, Itallo Patrick Castro Alves da Silva, Emanuel Adler Medeiros Pereira, Erick de Andrade Barboza",
    "abstract": "The validation and verification of artificial intelligence (AI) models through robustness assessment are essential to guarantee the reliable performance of intelligent systems facing real-world challenges, such as image corruptions including noise, blurring, and weather variations. Despite the global importance of mango (Mangifera indica L.), there is a lack of studies on the robustness of models for the diagnosis of disease in its leaves. This paper proposes a methodology to evaluate convolutio",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.13641v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-15-arxiv-from_code_to_field_evaluating_the_robustness_of_convolutional_neural_networks_fo.yaml"
  },
  {
    "id": "2025-12-15-arxiv-litept_lighter_yet_stronger_point_transformer",
    "date": "2025-12-15",
    "title": "LitePT: Lighter Yet Stronger Point Transformer",
    "authors": "Yuanwen Yue, Damien Robert, Jianyuan Wang, Sunghwan Hong, Jan Dirk Wegner",
    "abstract": "Modern neural architectures for 3D point cloud processing contain both convolutional layers and attention blocks, but the best way to assemble them remains unclear. We analyse the role of different computational blocks in 3D point cloud networks and find an intuitive behaviour: convolution is adequate to extract low-level geometry at high-resolution in early layers, where attention is expensive without bringing any benefits; attention captures high-level semantics and context in low-resolution, ",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.13689v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-15-arxiv-litept_lighter_yet_stronger_point_transformer.yaml"
  },
  {
    "id": "2025-12-15-arxiv-embedding_based_rankings_of_educational_resources_based_on_learning_outcome_alig",
    "date": "2025-12-15",
    "title": "Embedding-Based Rankings of Educational Resources based on Learning Outcome Alignment: Benchmarking, Expert Validation, and Learner Performance",
    "authors": "Mohammadreza Molavi, Mohammad Moein, Mohammadreza Tavakoli, Abdolali Faraji, Stefan T. Mol",
    "abstract": "As the online learning landscape evolves, the need for personalization is increasingly evident. Although educational resources are burgeoning, educators face challenges selecting materials that both align with intended learning outcomes and address diverse learner needs. Large Language Models (LLMs) are attracting growing interest for their potential to create learning resources that better support personalization, but verifying coverage of intended outcomes still requires human alignment review",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.13658v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-15-arxiv-embedding_based_rankings_of_educational_resources_based_on_learning_outcome_alig.yaml"
  },
  {
    "id": "2025-12-15-arxiv-beyond_surface_form_a_pipeline_for_semantic_analysis_in_alzheimer_s_disease_dete",
    "date": "2025-12-15",
    "title": "Beyond surface form: A pipeline for semantic analysis in Alzheimer's Disease detection from spontaneous speech",
    "authors": "Dylan Phelps, Rodrigo Wilkens, Edward Gow-Smith, Lilian Hubner, Bárbara Malcorra",
    "abstract": "Alzheimer's Disease (AD) is a progressive neurodegenerative condition that adversely affects cognitive abilities. Language-related changes can be automatically identified through the analysis of outputs from linguistic assessment tasks, such as picture description. Language models show promise as a basis for screening tools for AD, but their limited interpretability poses a challenge in distinguishing true linguistic markers of cognitive decline from surface-level textual patterns. To address th",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.13685v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-15-arxiv-beyond_surface_form_a_pipeline_for_semantic_analysis_in_alzheimer_s_disease_dete.yaml"
  },
  {
    "id": "2025-12-15-arxiv-robotracer_mastering_spatial_trace_with_reasoning_in_vision_language_models_for",
    "date": "2025-12-15",
    "title": "RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics",
    "authors": "Enshen Zhou, Cheng Chi, Yibo Li, Jingkun An, Jiayuan Zhang",
    "abstract": "Spatial tracing, as a fundamental embodied interaction ability for robots, is inherently challenging as it requires multi-step metric-grounded reasoning compounded with complex spatial referring and real-world metric measurement. However, existing methods struggle with this compositional task. To this end, we propose RoboTracer, a 3D-aware VLM that first achieves both 3D spatial referring and measuring via a universal spatial encoder and a regression-supervised decoder to enhance scale awareness",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.13660v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-15-arxiv-robotracer_mastering_spatial_trace_with_reasoning_in_vision_language_models_for.yaml"
  },
  {
    "id": "2025-12-15-arxiv-world_models_can_leverage_human_videos_for_dexterous_manipulation",
    "date": "2025-12-15",
    "title": "World Models Can Leverage Human Videos for Dexterous Manipulation",
    "authors": "Raktim Gautam Goswami, Amir Bar, David Fan, Tsung-Yen Yang, Gaoyue Zhou",
    "abstract": "Dexterous manipulation is challenging because it requires understanding how subtle hand motion influences the environment through contact with objects. We introduce DexWM, a Dexterous Manipulation World Model that predicts the next latent state of the environment conditioned on past states and dexterous actions. To overcome the scarcity of dexterous manipulation datasets, DexWM is trained on over 900 hours of human and non-dexterous robot videos. To enable fine-grained dexterity, we find that pr",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.13644v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-15-arxiv-world_models_can_leverage_human_videos_for_dexterous_manipulation.yaml"
  },
  {
    "id": "2025-12-15-arxiv-jova_unified_multimodal_learning_for_joint_video_audio_generation",
    "date": "2025-12-15",
    "title": "JoVA: Unified Multimodal Learning for Joint Video-Audio Generation",
    "authors": "Xiaohu Huang, Hao Zhou, Qiangpeng Yang, Shilei Wen, Kai Han",
    "abstract": "In this paper, we present JoVA, a unified framework for joint video-audio generation. Despite recent encouraging advances, existing methods face two critical limitations. First, most existing approaches can only generate ambient sounds and lack the capability to produce human speech synchronized with lip movements. Second, recent attempts at unified human video-audio generation typically rely on explicit fusion or modality-specific alignment modules, which introduce additional architecture desig",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.13677v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-15-arxiv-jova_unified_multimodal_learning_for_joint_video_audio_generation.yaml"
  },
  {
    "id": "2025-12-15-arxiv-towards_effective_model_editing_for_llm_personalization",
    "date": "2025-12-15",
    "title": "Towards Effective Model Editing for LLM Personalization",
    "authors": "Baixiang Huang, Limeng Cui, Jiapeng Liu, Haoran Wang, Jiawei Xu",
    "abstract": "Personalization is becoming indispensable for LLMs to align with individual user preferences and needs. Yet current approaches are often computationally expensive, data-intensive, susceptible to catastrophic forgetting, and prone to performance degradation in multi-turn interactions or when handling implicit queries. To address these challenges, we conceptualize personalization as a model editing task and introduce Personalization Editing, a framework that applies localized edits guided by clust",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.13676v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-15-arxiv-towards_effective_model_editing_for_llm_personalization.yaml"
  },
  {
    "id": "2025-12-15-arxiv-recurrent_video_masked_autoencoders",
    "date": "2025-12-15",
    "title": "Recurrent Video Masked Autoencoders",
    "authors": "Daniel Zoran, Nikhil Parthasarathy, Yi Yang, Drew A Hudson, Joao Carreira",
    "abstract": "We present Recurrent Video Masked-Autoencoders (RVM): a novel video representation learning approach that uses a transformer-based recurrent neural network to aggregate dense image features over time, effectively capturing the spatio-temporal structure of natural video data. RVM learns via an asymmetric masked prediction task requiring only a standard pixel reconstruction objective. This design yields a highly efficient ``generalist'' encoder: RVM achieves competitive performance with state-of-t",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.13684v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-15-arxiv-recurrent_video_masked_autoencoders.yaml"
  },
  {
    "id": "2025-12-12-arxiv-from_signal_to_turn_interactional_friction_in_modular_speech_to_speech_pipelines",
    "date": "2025-12-12",
    "title": "From Signal to Turn: Interactional Friction in Modular Speech-to-Speech Pipelines",
    "authors": "Titaya Mairittha, Tanakon Sawanglok, Panuwit Raden, Jirapast Buntub, Thanapat Warunee",
    "abstract": "While voice-based AI systems have achieved remarkable generative capabilities, their interactions often feel conversationally broken. This paper examines the interactional friction that emerges in modular Speech-to-Speech Retrieval-Augmented Generation (S2S-RAG) pipelines. By analyzing a representative production system, we move beyond simple latency metrics to identify three recurring patterns of conversational breakdown: (1) Temporal Misalignment, where system delays violate user expectations ",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11724v1"
      }
    ],
    "source_file": "papers/weekly/2025-12-12-arxiv-from_signal_to_turn_interactional_friction_in_modular_speech_to_speech_pipelines.yaml"
  },
  {
    "id": "2025-12-12-arxiv-cognisnn_enabling_neuron_expandability_pathway_reusability_and_dynamic_configura",
    "date": "2025-12-12",
    "title": "CogniSNN: Enabling Neuron-Expandability, Pathway-Reusability, and Dynamic-Configurability with Random Graph Architectures in Spiking Neural Networks",
    "authors": "Yongsheng Huang, Peibo Duan, Yujie Wu, Kai Sun, Zhipeng Liu",
    "abstract": "Spiking neural networks (SNNs), regarded as the third generation of artificial neural networks, are expected to bridge the gap between artificial intelligence and computational neuroscience. However, most mainstream SNN research directly adopts the rigid, chain-like hierarchical architecture of traditional artificial neural networks (ANNs), ignoring key structural characteristics of the brain. Biological neurons are stochastically interconnected, forming complex neural pathways that exhibit Neur",
    "category": "Hardware/Infra",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "hardware_infra"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11743v1"
      }
    ],
    "source_file": "papers/weekly/2025-12-12-arxiv-cognisnn_enabling_neuron_expandability_pathway_reusability_and_dynamic_configura.yaml"
  },
  {
    "id": "2025-12-12-arxiv-super_suffixes_bypassing_text_generation_alignment_and_guard_models_simultaneous",
    "date": "2025-12-12",
    "title": "Super Suffixes: Bypassing Text Generation Alignment and Guard Models Simultaneously",
    "authors": "Andrew Adiletta, Kathryn Adiletta, Kemal Derya, Berk Sunar",
    "abstract": "The rapid deployment of Large Language Models (LLMs) has created an urgent need for enhanced security and privacy measures in Machine Learning (ML). LLMs are increasingly being used to process untrusted text inputs and even generate executable code, often while having access to sensitive system controls. To address these security concerns, several companies have introduced guard models, which are smaller, specialized models designed to protect text generation models from adversarial or malicious",
    "category": "Incident",
    "source": "arxiv",
    "score": 95,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "incident"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11783v1"
      }
    ],
    "source_file": "papers/weekly/2025-12-12-arxiv-super_suffixes_bypassing_text_generation_alignment_and_guard_models_simultaneous.yaml"
  },
  {
    "id": "2025-12-12-arxiv-conditional_coverage_diagnostics_for_conformal_prediction",
    "date": "2025-12-12",
    "title": "Conditional Coverage Diagnostics for Conformal Prediction",
    "authors": "Sacha Braun, David Holzmüller, Michael I. Jordan, Francis Bach",
    "abstract": "Evaluating conditional coverage remains one of the most persistent challenges in assessing the reliability of predictive systems. Although conformal methods can give guarantees on marginal coverage, no method can guarantee to produce sets with correct conditional coverage, leaving practitioners without a clear way to interpret local deviations. To overcome sample-inefficiency and overfitting issues of existing metrics, we cast conditional coverage estimation as a classification problem. Conditio",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 95,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11779v1"
      }
    ],
    "source_file": "papers/weekly/2025-12-12-arxiv-conditional_coverage_diagnostics_for_conformal_prediction.yaml"
  },
  {
    "id": "2025-12-12-arxiv-agile_flight_emerges_from_multi_agent_competitive_racing",
    "date": "2025-12-12",
    "title": "Agile Flight Emerges from Multi-Agent Competitive Racing",
    "authors": "Vineet Pasumarti, Lorenzo Bianchi, Antonio Loquercio",
    "abstract": "Through multi-agent competition and the sparse high-level objective of winning a race, we find that both agile flight (e.g., high-speed motion pushing the platform to its physical limits) and strategy (e.g., overtaking or blocking) emerge from agents trained with reinforcement learning. We provide evidence in both simulation and the real world that this approach outperforms the common paradigm of training agents in isolation with rewards that prescribe behavior, e.g., progress on the raceline, i",
    "category": "Model",
    "source": "arxiv",
    "score": 95,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11781v1"
      }
    ],
    "source_file": "papers/weekly/2025-12-12-arxiv-agile_flight_emerges_from_multi_agent_competitive_racing.yaml"
  },
  {
    "id": "2025-12-12-arxiv-particulate_feed_forward_3d_object_articulation",
    "date": "2025-12-12",
    "title": "Particulate: Feed-Forward 3D Object Articulation",
    "authors": "Ruining Li, Yuxin Yao, Chuanxia Zheng, Christian Rupprecht, Joan Lasenby",
    "abstract": "We present Particulate, a feed-forward approach that, given a single static 3D mesh of an everyday object, directly infers all attributes of the underlying articulated structure, including its 3D parts, kinematic structure, and motion constraints. At its core is a transformer network, Part Articulation Transformer, which processes a point cloud of the input mesh using a flexible and scalable architecture to predict all the aforementioned attributes with native multi-joint support. We train the n",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11798v1"
      }
    ],
    "source_file": "papers/weekly/2025-12-12-arxiv-particulate_feed_forward_3d_object_articulation.yaml"
  },
  {
    "id": "2025-12-12-arxiv-generative_parametric_design_gpd_a_framework_for_real_time_geometry_generation_a",
    "date": "2025-12-12",
    "title": "Generative Parametric Design (GPD): A framework for real-time geometry generation and on-the-fly multiparametric approximation",
    "authors": "Mohammed El Fallaki Idrissi, Jad Mounayer, Sebastian Rodriguez, Fodil Meraghni, Francisco Chinesta",
    "abstract": "This paper presents a novel paradigm in simulation-based engineering sciences by introducing a new framework called Generative Parametric Design (GPD). The GPD framework enables the generation of new designs along with their corresponding parametric solutions given as a reduced basis. To achieve this, two Rank Reduction Autoencoders (RRAEs) are employed, one for encoding and generating the design or geometry, and the other for encoding the sparse Proper Generalized Decomposition (sPGD) mode solu",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11748v1"
      }
    ],
    "source_file": "papers/weekly/2025-12-12-arxiv-generative_parametric_design_gpd_a_framework_for_real_time_geometry_generation_a.yaml"
  },
  {
    "id": "2025-12-12-arxiv-medai_evaluating_txagent_s_therapeutic_agentic_reasoning_in_the_neurips_cure_ben",
    "date": "2025-12-12",
    "title": "MedAI: Evaluating TxAgent's Therapeutic Agentic Reasoning in the NeurIPS CURE-Bench Competition",
    "authors": "Tim Cofala, Christian Kalfar, Jingge Xiao, Johanna Schrader, Michelle Tang",
    "abstract": "Therapeutic decision-making in clinical medicine constitutes a high-stakes domain in which AI guidance interacts with complex interactions among patient characteristics, disease processes, and pharmacological agents. Tasks such as drug recommendation, treatment planning, and adverse-effect prediction demand robust, multi-step reasoning grounded in reliable biomedical knowledge. Agentic AI methods, exemplified by TxAgent, address these challenges through iterative retrieval-augmented generation (",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11682v1"
      }
    ],
    "source_file": "papers/weekly/2025-12-12-arxiv-medai_evaluating_txagent_s_therapeutic_agentic_reasoning_in_the_neurips_cure_ben.yaml"
  },
  {
    "id": "2025-12-12-arxiv-smudged_fingerprints_a_systematic_evaluation_of_the_robustness_of_ai_image_finge",
    "date": "2025-12-12",
    "title": "Smudged Fingerprints: A Systematic Evaluation of the Robustness of AI Image Fingerprints",
    "authors": "Kai Yao, Marc Juarez",
    "abstract": "Model fingerprint detection techniques have emerged as a promising approach for attributing AI-generated images to their source models, but their robustness under adversarial conditions remains largely unexplored. We present the first systematic security evaluation of these techniques, formalizing threat models that encompass both white- and black-box access and two attack goals: fingerprint removal, which erases identifying traces to evade attribution, and fingerprint forgery, which seeks to ca",
    "category": "Incident",
    "source": "arxiv",
    "score": 90,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "incident"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11771v1"
      }
    ],
    "source_file": "papers/weekly/2025-12-12-arxiv-smudged_fingerprints_a_systematic_evaluation_of_the_robustness_of_ai_image_finge.yaml"
  },
  {
    "id": "2025-12-12-arxiv-from_verification_burden_to_trusted_collaboration_design_goals_for_llm_assisted",
    "date": "2025-12-12",
    "title": "From Verification Burden to Trusted Collaboration: Design Goals for LLM-Assisted Literature Reviews",
    "authors": "Brenda Nogueira, Werner Geyer, Andrew Anderson, Toby Jia-Jun Li, Dongwhi Kim",
    "abstract": "Large Language Models (LLMs) are increasingly embedded in academic writing practices. Although numerous studies have explored how researchers employ these tools for scientific writing, their concrete implementation, limitations, and design challenges within the literature review process remain underexplored. In this paper, we report a user study with researchers across multiple disciplines to characterize current practices, benefits, and \\textit{pain points} in using LLMs to investigate related ",
    "category": "Project",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "project"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11661v1"
      }
    ],
    "source_file": "papers/weekly/2025-12-12-arxiv-from_verification_burden_to_trusted_collaboration_design_goals_for_llm_assisted.yaml"
  },
  {
    "id": "2025-12-12-arxiv-from_signal_to_turn_interactional_friction_in_modular_speech_to_speech_pipelines",
    "date": "2025-12-12",
    "title": "From Signal to Turn: Interactional Friction in Modular Speech-to-Speech Pipelines",
    "authors": "Titaya Mairittha, Tanakon Sawanglok, Panuwit Raden, Jirapast Buntub, Thanapat Warunee",
    "abstract": "While voice-based AI systems have achieved remarkable generative capabilities, their interactions often feel conversationally broken. This paper examines the interactional friction that emerges in modular Speech-to-Speech Retrieval-Augmented Generation (S2S-RAG) pipelines. By analyzing a representative production system, we move beyond simple latency metrics to identify three recurring patterns of conversational breakdown: (1) Temporal Misalignment, where system delays violate user expectations ",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11724v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-from_signal_to_turn_interactional_friction_in_modular_speech_to_speech_pipelines.yaml"
  },
  {
    "id": "2025-12-12-arxiv-cognisnn_enabling_neuron_expandability_pathway_reusability_and_dynamic_configura",
    "date": "2025-12-12",
    "title": "CogniSNN: Enabling Neuron-Expandability, Pathway-Reusability, and Dynamic-Configurability with Random Graph Architectures in Spiking Neural Networks",
    "authors": "Yongsheng Huang, Peibo Duan, Yujie Wu, Kai Sun, Zhipeng Liu",
    "abstract": "Spiking neural networks (SNNs), regarded as the third generation of artificial neural networks, are expected to bridge the gap between artificial intelligence and computational neuroscience. However, most mainstream SNN research directly adopts the rigid, chain-like hierarchical architecture of traditional artificial neural networks (ANNs), ignoring key structural characteristics of the brain. Biological neurons are stochastically interconnected, forming complex neural pathways that exhibit Neur",
    "category": "Hardware/Infra",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "hardware_infra"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11743v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-cognisnn_enabling_neuron_expandability_pathway_reusability_and_dynamic_configura.yaml"
  },
  {
    "id": "2025-12-12-arxiv-moment_based_3d_gaussian_splatting_resolving_volumetric_occlusion_with_order_ind",
    "date": "2025-12-12",
    "title": "Moment-Based 3D Gaussian Splatting: Resolving Volumetric Occlusion with Order-Independent Transmittance",
    "authors": "Jan U. Müller, Robin Tim Landsgesell, Leif Van Holland, Patrick Stotko, Reinhard Klein",
    "abstract": "The recent success of 3D Gaussian Splatting (3DGS) has reshaped novel view synthesis by enabling fast optimization and real-time rendering of high-quality radiance fields. However, it relies on simplified, order-dependent alpha blending and coarse approximations of the density integral within the rasterizer, thereby limiting its ability to render complex, overlapping semi-transparent objects. In this paper, we extend rasterization-based rendering of 3D Gaussian representations with a novel metho",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11800v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-moment_based_3d_gaussian_splatting_resolving_volumetric_occlusion_with_order_ind.yaml"
  },
  {
    "id": "2025-12-12-arxiv-anchordream_repurposing_video_diffusion_for_embodiment_aware_robot_data_synthesi",
    "date": "2025-12-12",
    "title": "AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis",
    "authors": "Junjie Ye, Rong Xue, Basile Van Hoorick, Pavel Tokmakov, Muhammad Zubair Irshad",
    "abstract": "The collection of large-scale and diverse robot demonstrations remains a major bottleneck for imitation learning, as real-world data acquisition is costly and simulators offer limited diversity and fidelity with pronounced sim-to-real gaps. While generative models present an attractive solution, existing methods often alter only visual appearances without creating new behaviors, or suffer from embodiment inconsistencies that yield implausible motions. To address these limitations, we introduce A",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11797v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-anchordream_repurposing_video_diffusion_for_embodiment_aware_robot_data_synthesi.yaml"
  },
  {
    "id": "2025-12-12-arxiv-reframing_music_driven_2d_dance_pose_generation_as_multi_channel_image_generatio",
    "date": "2025-12-12",
    "title": "Reframing Music-Driven 2D Dance Pose Generation as Multi-Channel Image Generation",
    "authors": "Yan Zhang, Han Zou, Lincong Feng, Cong Xie, Ruiqi Yu",
    "abstract": "Recent pose-to-video models can translate 2D pose sequences into photorealistic, identity-preserving dance videos, so the key challenge is to generate temporally coherent, rhythm-aligned 2D poses from music, especially under complex, high-variance in-the-wild distributions. We address this by reframing music-to-dance generation as a music-token-conditioned multi-channel image synthesis problem: 2D pose sequences are encoded as one-hot images, compressed by a pretrained image VAE, and modeled wit",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11720v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-reframing_music_driven_2d_dance_pose_generation_as_multi_channel_image_generatio.yaml"
  },
  {
    "id": "2025-12-12-arxiv-mvise_a_visual_search_engine_for_analyzing_multiplex_ihc_brain_tissue_images",
    "date": "2025-12-12",
    "title": "mViSE: A Visual Search Engine for Analyzing Multiplex IHC Brain Tissue Images",
    "authors": "Liqiang Huang, Rachel W. Mills, Saikiran Mandula, Lin Bai, Mahtab Jeyhani",
    "abstract": "Whole-slide multiplex imaging of brain tissue generates massive information-dense images that are challenging to analyze and require custom software. We present an alternative query-driven programming-free strategy using a multiplex visual search engine (mViSE) that learns the multifaceted brain tissue chemoarchitecture, cytoarchitecture, and myeloarchitecture. Our divide-and-conquer strategy organizes the data into panels of related molecular markers and uses self-supervised learning to train a",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11745v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-mvise_a_visual_search_engine_for_analyzing_multiplex_ihc_brain_tissue_images.yaml"
  },
  {
    "id": "2025-12-12-arxiv-super_suffixes_bypassing_text_generation_alignment_and_guard_models_simultaneous",
    "date": "2025-12-12",
    "title": "Super Suffixes: Bypassing Text Generation Alignment and Guard Models Simultaneously",
    "authors": "Andrew Adiletta, Kathryn Adiletta, Kemal Derya, Berk Sunar",
    "abstract": "The rapid deployment of Large Language Models (LLMs) has created an urgent need for enhanced security and privacy measures in Machine Learning (ML). LLMs are increasingly being used to process untrusted text inputs and even generate executable code, often while having access to sensitive system controls. To address these security concerns, several companies have introduced guard models, which are smaller, specialized models designed to protect text generation models from adversarial or malicious",
    "category": "Incident",
    "source": "arxiv",
    "score": 95,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "incident"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11783v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-super_suffixes_bypassing_text_generation_alignment_and_guard_models_simultaneous.yaml"
  },
  {
    "id": "2025-12-12-arxiv-editmgt_unleashing_potentials_of_masked_generative_transformers_in_image_editing",
    "date": "2025-12-12",
    "title": "EditMGT: Unleashing Potentials of Masked Generative Transformers in Image Editing",
    "authors": "Wei Chow, Linfeng Li, Lingdong Kong, Zefeng Li, Qi Xu",
    "abstract": "Recent advances in diffusion models (DMs) have achieved exceptional visual quality in image editing tasks. However, the global denoising dynamics of DMs inherently conflate local editing targets with the full-image context, leading to unintended modifications in non-target regions. In this paper, we shift our attention beyond DMs and turn to Masked Generative Transformers (MGTs) as an alternative approach to tackle this challenge. By predicting multiple masked tokens rather than holistic refinem",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11715v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-editmgt_unleashing_potentials_of_masked_generative_transformers_in_image_editing.yaml"
  },
  {
    "id": "2025-12-12-nature-deciphering_rna_ligand_binding_specificity_with_gerna_bind",
    "date": "2025-12-12",
    "title": "Deciphering RNA–ligand binding specificity with GerNA-Bind",
    "authors": "Yunpeng Xia, Jiayi Li, Yi-Ting Chu, Jiahua Rao, Jing Chen",
    "abstract": "<p>Nature Machine Intelligence, Published online: 12 December 2025; <a href=\"https://www.nature.com/articles/s42256-025-01154-z\">doi:10.1038/s42256-025-01154-z</a></p>Xia et al. introduce GerNA-Bind, a geometric deep learning framework designed to predict RNA–ligand binding specificity by integrating multistate RNA–ligand interactions.",
    "category": "Model",
    "source": "nature",
    "score": 115,
    "tags": [
      "ai",
      "research",
      "nature",
      "model"
    ],
    "links": [
      {
        "title": "View Paper",
        "url": "https://www.nature.com/articles/s42256-025-01154-z"
      },
      {
        "title": "DOI",
        "url": "https://doi.org/10.1038/s42256-025-01154-z"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-nature-deciphering_rna_ligand_binding_specificity_with_gerna_bind.yaml"
  },
  {
    "id": "2025-12-12-arxiv-conditional_coverage_diagnostics_for_conformal_prediction",
    "date": "2025-12-12",
    "title": "Conditional Coverage Diagnostics for Conformal Prediction",
    "authors": "Sacha Braun, David Holzmüller, Michael I. Jordan, Francis Bach",
    "abstract": "Evaluating conditional coverage remains one of the most persistent challenges in assessing the reliability of predictive systems. Although conformal methods can give guarantees on marginal coverage, no method can guarantee to produce sets with correct conditional coverage, leaving practitioners without a clear way to interpret local deviations. To overcome sample-inefficiency and overfitting issues of existing metrics, we cast conditional coverage estimation as a classification problem. Conditio",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 95,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11779v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-conditional_coverage_diagnostics_for_conformal_prediction.yaml"
  },
  {
    "id": "2025-12-12-nature-llm_use_in_scholarly_writing_poses_a_provenance_problem",
    "date": "2025-12-12",
    "title": "LLM use in scholarly writing poses a provenance problem",
    "authors": "Brian D. Earp, Haotian Yuan, Julian Koplin, Sebastian Porsdam Mann",
    "abstract": "<p>Nature Machine Intelligence, Published online: 12 December 2025; <a href=\"https://www.nature.com/articles/s42256-025-01159-8\">doi:10.1038/s42256-025-01159-8</a></p>LLM use in scholarly writing poses a provenance problem",
    "category": "Model",
    "source": "nature",
    "score": 110,
    "tags": [
      "ai",
      "research",
      "nature",
      "model"
    ],
    "links": [
      {
        "title": "View Paper",
        "url": "https://www.nature.com/articles/s42256-025-01159-8"
      },
      {
        "title": "DOI",
        "url": "https://doi.org/10.1038/s42256-025-01159-8"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-nature-llm_use_in_scholarly_writing_poses_a_provenance_problem.yaml"
  },
  {
    "id": "2025-12-12-arxiv-agile_flight_emerges_from_multi_agent_competitive_racing",
    "date": "2025-12-12",
    "title": "Agile Flight Emerges from Multi-Agent Competitive Racing",
    "authors": "Vineet Pasumarti, Lorenzo Bianchi, Antonio Loquercio",
    "abstract": "Through multi-agent competition and the sparse high-level objective of winning a race, we find that both agile flight (e.g., high-speed motion pushing the platform to its physical limits) and strategy (e.g., overtaking or blocking) emerge from agents trained with reinforcement learning. We provide evidence in both simulation and the real world that this approach outperforms the common paradigm of training agents in isolation with rewards that prescribe behavior, e.g., progress on the raceline, i",
    "category": "Model",
    "source": "arxiv",
    "score": 95,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11781v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-agile_flight_emerges_from_multi_agent_competitive_racing.yaml"
  },
  {
    "id": "2025-12-12-arxiv-particulate_feed_forward_3d_object_articulation",
    "date": "2025-12-12",
    "title": "Particulate: Feed-Forward 3D Object Articulation",
    "authors": "Ruining Li, Yuxin Yao, Chuanxia Zheng, Christian Rupprecht, Joan Lasenby",
    "abstract": "We present Particulate, a feed-forward approach that, given a single static 3D mesh of an everyday object, directly infers all attributes of the underlying articulated structure, including its 3D parts, kinematic structure, and motion constraints. At its core is a transformer network, Part Articulation Transformer, which processes a point cloud of the input mesh using a flexible and scalable architecture to predict all the aforementioned attributes with native multi-joint support. We train the n",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11798v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-particulate_feed_forward_3d_object_articulation.yaml"
  },
  {
    "id": "2025-12-12-arxiv-generative_parametric_design_gpd_a_framework_for_real_time_geometry_generation_a",
    "date": "2025-12-12",
    "title": "Generative Parametric Design (GPD): A framework for real-time geometry generation and on-the-fly multiparametric approximation",
    "authors": "Mohammed El Fallaki Idrissi, Jad Mounayer, Sebastian Rodriguez, Fodil Meraghni, Francisco Chinesta",
    "abstract": "This paper presents a novel paradigm in simulation-based engineering sciences by introducing a new framework called Generative Parametric Design (GPD). The GPD framework enables the generation of new designs along with their corresponding parametric solutions given as a reduced basis. To achieve this, two Rank Reduction Autoencoders (RRAEs) are employed, one for encoding and generating the design or geometry, and the other for encoding the sparse Proper Generalized Decomposition (sPGD) mode solu",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11748v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-generative_parametric_design_gpd_a_framework_for_real_time_geometry_generation_a.yaml"
  },
  {
    "id": "2025-12-12-arxiv-uncertainty_aware_domain_adaptation_for_vitiligo_segmentation_in_clinical_photog",
    "date": "2025-12-12",
    "title": "Uncertainty-Aware Domain Adaptation for Vitiligo Segmentation in Clinical Photographs",
    "authors": "Wentao Jiang, Vamsi Varra, Caitlin Perez-Stable, Harrison Zhu, Meredith Apicella",
    "abstract": "Accurately quantifying vitiligo extent in routine clinical photographs is crucial for longitudinal monitoring of treatment response. We propose a trustworthy, frequency-aware segmentation framework built on three synergistic pillars: (1) a data-efficient training strategy combining domain-adaptive pre-training on the ISIC 2019 dataset with an ROI-constrained dual-task loss to suppress background noise; (2) an architectural refinement via a ConvNeXt V2-based encoder enhanced with a novel High-Fre",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11791v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-uncertainty_aware_domain_adaptation_for_vitiligo_segmentation_in_clinical_photog.yaml"
  },
  {
    "id": "2025-12-12-arxiv-medai_evaluating_txagent_s_therapeutic_agentic_reasoning_in_the_neurips_cure_ben",
    "date": "2025-12-12",
    "title": "MedAI: Evaluating TxAgent's Therapeutic Agentic Reasoning in the NeurIPS CURE-Bench Competition",
    "authors": "Tim Cofala, Christian Kalfar, Jingge Xiao, Johanna Schrader, Michelle Tang",
    "abstract": "Therapeutic decision-making in clinical medicine constitutes a high-stakes domain in which AI guidance interacts with complex interactions among patient characteristics, disease processes, and pharmacological agents. Tasks such as drug recommendation, treatment planning, and adverse-effect prediction demand robust, multi-step reasoning grounded in reliable biomedical knowledge. Agentic AI methods, exemplified by TxAgent, address these challenges through iterative retrieval-augmented generation (",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11682v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-medai_evaluating_txagent_s_therapeutic_agentic_reasoning_in_the_neurips_cure_ben.yaml"
  },
  {
    "id": "2025-12-12-arxiv-v_rgbx_video_editing_with_accurate_controls_over_intrinsic_properties",
    "date": "2025-12-12",
    "title": "V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties",
    "authors": "Ye Fang, Tong Wu, Valentin Deschaintre, Duygu Ceylan, Iliyan Georgiev",
    "abstract": "Large-scale video generation models have shown remarkable potential in modeling photorealistic appearance and lighting interactions in real-world scenes. However, a closed-loop framework that jointly understands intrinsic scene properties (e.g., albedo, normal, material, and irradiance), leverages them for video synthesis, and supports editable intrinsic representations remains unexplored. We present V-RGBX, the first end-to-end framework for intrinsic-aware video editing. V-RGBX unifies three k",
    "category": "Industry",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "industry"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11799v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-v_rgbx_video_editing_with_accurate_controls_over_intrinsic_properties.yaml"
  },
  {
    "id": "2025-12-12-arxiv-structure_from_tracking_distilling_structure_preserving_motion_for_video_generat",
    "date": "2025-12-12",
    "title": "Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation",
    "authors": "Yang Fei, George Stoica, Jingyuan Liu, Qifeng Chen, Ranjay Krishna",
    "abstract": "Reality is a dance between rigid constraints and deformable structures. For video models, that means generating motion that preserves fidelity as well as structure. Despite progress in diffusion models, producing realistic structure-preserving motion remains challenging, especially for articulated and deformable objects such as humans and animals. Scaling training data alone, so far, has failed to resolve physically implausible transitions. Existing approaches rely on conditioning with noisy mot",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11792v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-structure_from_tracking_distilling_structure_preserving_motion_for_video_generat.yaml"
  },
  {
    "id": "2025-12-12-arxiv-weak_to_strong_generalization_enables_fully_automated_de_novo_training_of_multi",
    "date": "2025-12-12",
    "title": "Weak-to-Strong Generalization Enables Fully Automated De Novo Training of Multi-head Mask-RCNN Model for Segmenting Densely Overlapping Cell Nuclei in Multiplex Whole-slice Brain Images",
    "authors": "Lin Bai, Xiaoyang Li, Liqiang Huang, Quynh Nguyen, Hien Van Nguyen",
    "abstract": "We present a weak to strong generalization methodology for fully automated training of a multi-head extension of the Mask-RCNN method with efficient channel attention for reliable segmentation of overlapping cell nuclei in multiplex cyclic immunofluorescent (IF) whole-slide images (WSI), and present evidence for pseudo-label correction and coverage expansion, the key phenomena underlying weak to strong generalization. This method can learn to segment de novo a new class of images from a new inst",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11722v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-weak_to_strong_generalization_enables_fully_automated_de_novo_training_of_multi.yaml"
  },
  {
    "id": "2025-12-12-arxiv-svg_t2i_scaling_up_text_to_image_latent_diffusion_model_without_variational_auto",
    "date": "2025-12-12",
    "title": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
    "authors": "Minglei Shi, Haolin Wang, Borui Zhang, Wenzhao Zheng, Bohan Zeng",
    "abstract": "Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthes",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11749v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-svg_t2i_scaling_up_text_to_image_latent_diffusion_model_without_variational_auto.yaml"
  },
  {
    "id": "2025-12-12-arxiv-smudged_fingerprints_a_systematic_evaluation_of_the_robustness_of_ai_image_finge",
    "date": "2025-12-12",
    "title": "Smudged Fingerprints: A Systematic Evaluation of the Robustness of AI Image Fingerprints",
    "authors": "Kai Yao, Marc Juarez",
    "abstract": "Model fingerprint detection techniques have emerged as a promising approach for attributing AI-generated images to their source models, but their robustness under adversarial conditions remains largely unexplored. We present the first systematic security evaluation of these techniques, formalizing threat models that encompass both white- and black-box access and two attack goals: fingerprint removal, which erases identifying traces to evade attribution, and fingerprint forgery, which seeks to ca",
    "category": "Incident",
    "source": "arxiv",
    "score": 90,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "incident"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11771v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-smudged_fingerprints_a_systematic_evaluation_of_the_robustness_of_ai_image_finge.yaml"
  },
  {
    "id": "2025-12-12-arxiv-from_verification_burden_to_trusted_collaboration_design_goals_for_llm_assisted",
    "date": "2025-12-12",
    "title": "From Verification Burden to Trusted Collaboration: Design Goals for LLM-Assisted Literature Reviews",
    "authors": "Brenda Nogueira, Werner Geyer, Andrew Anderson, Toby Jia-Jun Li, Dongwhi Kim",
    "abstract": "Large Language Models (LLMs) are increasingly embedded in academic writing practices. Although numerous studies have explored how researchers employ these tools for scientific writing, their concrete implementation, limitations, and design challenges within the literature review process remain underexplored. In this paper, we report a user study with researchers across multiple disciplines to characterize current practices, benefits, and \\textit{pain points} in using LLMs to investigate related ",
    "category": "Project",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "project"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11661v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-from_verification_burden_to_trusted_collaboration_design_goals_for_llm_assisted.yaml"
  },
  {
    "id": "2025-12-10-nature-a_multimodal_cell_free_rna_language_model_for_liquid_biopsy_applications",
    "date": "2025-12-10",
    "title": "A multimodal cell-free RNA language model for liquid biopsy applications",
    "authors": "Mehran Karimzadeh, Aiden M. Sababi, Amir Momen-Roknabadi, Nae-Chyun Chen, Taylor B. Cavazos",
    "abstract": "<p>Nature Machine Intelligence, Published online: 10 December 2025; <a href=\"https://www.nature.com/articles/s42256-025-01148-x\">doi:10.1038/s42256-025-01148-x</a></p>Exai-1, a cell-free RNA foundation model that integrates sequence, structure and expression features, advances liquid biopsy diagnostics by denoising noisy data, augmenting limited datasets and improving the generalizability of cancer detection models.",
    "category": "Model",
    "source": "nature",
    "score": 115,
    "tags": [
      "ai",
      "research",
      "nature",
      "model"
    ],
    "links": [
      {
        "title": "View Paper",
        "url": "https://www.nature.com/articles/s42256-025-01148-x"
      },
      {
        "title": "DOI",
        "url": "https://doi.org/10.1038/s42256-025-01148-x"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-10-nature-a_multimodal_cell_free_rna_language_model_for_liquid_biopsy_applications.yaml"
  },
  {
    "id": "2025-12-09-nature-actor_critic_networks_with_analogue_memristors_mimicking_reward_based_learning",
    "date": "2025-12-09",
    "title": "Actor–critic networks with analogue memristors mimicking reward-based learning",
    "authors": "Kevin Portner, Till Zellweger, Flavio Martinelli, Laura Bégon-Lours, Valeria Bragaglia",
    "abstract": "<p>Nature Machine Intelligence, Published online: 09 December 2025; <a href=\"https://www.nature.com/articles/s42256-025-01149-w\">doi:10.1038/s42256-025-01149-w</a></p>A framework based on actor–critic temporal difference learning and employing a biologically plausible network architecture that mimics reward-based learning on memristors and enables full in-memory training for navigation tasks is discussed.",
    "category": "Model",
    "source": "nature",
    "score": 115,
    "tags": [
      "ai",
      "research",
      "nature",
      "model"
    ],
    "links": [
      {
        "title": "View Paper",
        "url": "https://www.nature.com/articles/s42256-025-01149-w"
      },
      {
        "title": "DOI",
        "url": "https://doi.org/10.1038/s42256-025-01149-w"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-09-nature-actor_critic_networks_with_analogue_memristors_mimicking_reward_based_learning.yaml"
  },
  {
    "id": "2025-12-09-nature-fully_analogue_reinforcement_learning_with_memristors",
    "date": "2025-12-09",
    "title": "Fully analogue reinforcement learning with memristors",
    "authors": "Yue Zhang, Xiaojuan Qi, Zhongrui Wang",
    "abstract": "<p>Nature Machine Intelligence, Published online: 09 December 2025; <a href=\"https://www.nature.com/articles/s42256-025-01157-w\">doi:10.1038/s42256-025-01157-w</a></p>Reinforcement learning has a key role in artifical intelligence (AI), but its implementation on neuromorphic hardware typically involves operations executed on conventional digital computers. A study now addresses this issue by implementing an actor–critic network fully in hardware using analogue memristors.",
    "category": "Model",
    "source": "nature",
    "score": 110,
    "tags": [
      "ai",
      "research",
      "nature",
      "model"
    ],
    "links": [
      {
        "title": "View Paper",
        "url": "https://www.nature.com/articles/s42256-025-01157-w"
      },
      {
        "title": "DOI",
        "url": "https://doi.org/10.1038/s42256-025-01157-w"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-09-nature-fully_analogue_reinforcement_learning_with_memristors.yaml"
  }
]