{
  "key_insight": "Balancing exploration and exploitation in RL with verifiable rewards (RLVR) requires rethinking reward design: spurious rewards and entropy pressure interact in counterintuitive ways, and simple remedies like reward clipping and calibrated entropy regularization can restore productive exploration and improve reasoning.",
  "problem_solved": "Addresses the fragile exploration–exploitation trade-off in RLVR for LLM reasoning, where spurious rewards and entropy-driven collapse can push models toward unhelpful or overly conservative behaviors and harm verifiable-reasoning performance.",
  "method": [
    "Analyze how spurious rewards (signals uncorrelated with ground truth), entropy minimization, and reward clipping separately and jointly affect exploration and exploitation in RLVR.",
    "Introduce practical interventions—reward clipping to limit spurious-signal amplification and calibrated entropy regularization to preserve exploration—alongside controlled experiments on LLM reasoning benchmarks.",
    "Empirically quantify trade-offs and demonstrate how combinations of clipping and entropy tuning recover meaningful exploration and improve verifiable-reasoning outcomes."
  ],
  "impact": "Provides actionable guidelines for designing RL-style training for LLMs: mitigate spurious rewards and avoid excessive entropy minimization to maintain exploration, leading to more reliable and robust reasoning. Practitioners gain simple tools (clipping + entropy tuning) to improve downstream verification performance.",
  "visual_elements": [
    "Trade-off curve: exploration vs. exploitation showing regions dominated by spurious rewards, entropy collapse, and the improved region after clipping + entropy tuning.",
    "Flow diagram of RLVR loop annotated with points where spurious rewards, entropy regularization, and clipping intervene.",
    "Bar/line charts comparing reasoning accuracy and exploration metrics across ablations: baseline, spurious reward, entropy-minimized, clipping-only, and combined intervention.",
    "Icon set: reward signal (trophy), entropy (entropy icon or scatter-to-concentrate), clip/scissors (clipping), and magnifying glass (exploration/verification)."
  ],
  "hashtags": [
    "#ReinforcementLearning",
    "#LLM",
    "#RLVR",
    "#ExplorationVsExploitation",
    "#RewardDesign"
  ],
  "paper_id": "2025-12-18-arxiv-exploration_v_s_exploitation_rethinking_rlvr_through_clipping_entropy_and_spurio",
  "paper_title": "Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward",
  "paper_category": "Model",
  "paper_date": "2025-12-18",
  "paper_authors": "Peter Chen, Xiaopeng Li, Ziniu Li, Wotao Yin, Xi Chen",
  "paper_links": [
    {
      "title": "PDF",
      "url": "https://arxiv.org/pdf/2512.16912v1"
    }
  ],
  "generated_at": "2025-12-19T06:20:19.383545"
}