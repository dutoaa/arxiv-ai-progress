[
  {
    "id": "2025-12-23-arxiv-distilling_to_hybrid_attention_models_via_kl_guided_layer_selection",
    "date": "2025-12-23",
    "title": "Distilling to Hybrid Attention Models via KL-Guided Layer Selection",
    "authors": "Yanhong Li, Songlin Yang, Shawn Tan, Mayank Mishra, Rameswar Panda",
    "abstract": "Distilling pretrained softmax attention Transformers into more efficient hybrid architectures that interleave softmax and linear attention layers is a promising approach for improving the inference efficiency of LLMs without requiring expensive pretraining from scratch. A critical factor in the conversion process is layer selection, i.e., deciding on which layers to convert to linear attention variants. This paper describes a simple and efficient recipe for layer selection that uses layer import",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.20569v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-23-arxiv-distilling_to_hybrid_attention_models_via_kl_guided_layer_selection.yaml"
  },
  {
    "id": "2025-12-23-arxiv-lead_minimizing_learner_expert_asymmetry_in_end_to_end_driving",
    "date": "2025-12-23",
    "title": "LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving",
    "authors": "Long Nguyen, Micha Fauth, Bernhard Jaeger, Daniel Dauner, Maximilian Igl",
    "abstract": "Simulators can generate virtually unlimited driving data, yet imitation learning policies in simulation still struggle to achieve robust closed-loop performance. Motivated by this gap, we empirically study how misalignment between privileged expert demonstrations and sensor-based student observations can limit the effectiveness of imitation learning. More precisely, experts have significantly higher visibility (e.g., ignoring occlusions) and far lower uncertainty (e.g., knowing other vehicles' a",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.20563v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-23-arxiv-lead_minimizing_learner_expert_asymmetry_in_end_to_end_driving.yaml"
  },
  {
    "id": "2025-12-23-arxiv-longvideoagent_multi_agent_reasoning_with_long_videos",
    "date": "2025-12-23",
    "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos",
    "authors": "Runtao Liu, Ziyi Liu, Jiaqi Tang, Yue Ma, Renjie Pi",
    "abstract": "Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent pl",
    "category": "Project",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "project"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.20618v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-23-arxiv-longvideoagent_multi_agent_reasoning_with_long_videos.yaml"
  },
  {
    "id": "2025-12-23-arxiv-emergent_temporal_abstractions_in_autoregressive_models_enable_hierarchical_rein",
    "date": "2025-12-23",
    "title": "Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning",
    "authors": "Seijin Kobayashi, Yanick Schimpf, Maximilian Schlegel, Angelika Steger, Maciej Wolczyk",
    "abstract": "Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations o",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.20605v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-23-arxiv-emergent_temporal_abstractions_in_autoregressive_models_enable_hierarchical_rein.yaml"
  },
  {
    "id": "2025-12-23-arxiv-repurposing_video_diffusion_transformers_for_robust_point_tracking",
    "date": "2025-12-23",
    "title": "Repurposing Video Diffusion Transformers for Robust Point Tracking",
    "authors": "Soowon Son, Honggyu An, Chaehyun Kim, Hyunah Ko, Jisu Nam",
    "abstract": "Point tracking aims to localize corresponding points across video frames, serving as a fundamental task for 4D reconstruction, robotics, and video editing. Existing methods commonly rely on shallow convolutional backbones such as ResNet that process frames independently, lacking temporal coherence and producing unreliable matching costs under challenging conditions. Through systematic analysis, we find that video Diffusion Transformers (DiTs), pre-trained on large-scale real-world videos with sp",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.20606v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-23-arxiv-repurposing_video_diffusion_transformers_for_robust_point_tracking.yaml"
  },
  {
    "id": "2025-12-23-arxiv-semanticgen_video_generation_in_semantic_space",
    "date": "2025-12-23",
    "title": "SemanticGen: Video Generation in Semantic Space",
    "authors": "Jianhong Bai, Xiaoshi Wu, Xintao Wang, Fu Xiao, Yuanxing Zhang",
    "abstract": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, ",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.20619v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-23-arxiv-semanticgen_video_generation_in_semantic_space.yaml"
  },
  {
    "id": "2025-12-23-arxiv-spatialtree_how_spatial_abilities_branch_out_in_mllms",
    "date": "2025-12-23",
    "title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs",
    "authors": "Yuxi Xiao, Longfei Li, Shen Yan, Xinhang Liu, Sida Peng",
    "abstract": "Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capa",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.20617v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-23-arxiv-spatialtree_how_spatial_abilities_branch_out_in_mllms.yaml"
  },
  {
    "id": "2025-12-23-arxiv-flashvlm_text_guided_visual_token_selection_for_large_multimodal_models",
    "date": "2025-12-23",
    "title": "FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models",
    "authors": "Kaitong Cai, Jusheng Zhang, Jing Yang, Yijia Fan, Pengtao Xie",
    "abstract": "Large vision-language models (VLMs) typically process hundreds or thousands of visual tokens per image or video frame, incurring quadratic attention cost and substantial redundancy. Existing token reduction methods often ignore the textual query or rely on deep attention maps, whose instability under aggressive pruning leads to degraded semantic alignment.\n  We propose FlashVLM, a text guided visual token selection framework that dynamically adapts visual inputs to the query. Instead of relying ",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.20561v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-23-arxiv-flashvlm_text_guided_visual_token_selection_for_large_multimodal_models.yaml"
  },
  {
    "id": "2025-12-23-arxiv-automated_stereotactic_radiosurgery_planning_using_a_human_in_the_loop_reasoning",
    "date": "2025-12-23",
    "title": "Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent",
    "authors": "Humza Nusrat, Luke Francisco, Bing Luo, Hassan Bagher-Ebadian, Joshua Kim",
    "abstract": "Stereotactic radiosurgery (SRS) demands precise dose shaping around critical structures, yet black-box AI systems have limited clinical adoption due to opacity concerns. We tested whether chain-of-thought reasoning improves agentic planning in a retrospective cohort of 41 patients with brain metastases treated with 18 Gy single-fraction SRS. We developed SAGE (Secure Agent for Generative Dose Expertise), an LLM-based planning agent for automated SRS treatment planning. Two variants generated pla",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.20586v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-23-arxiv-automated_stereotactic_radiosurgery_planning_using_a_human_in_the_loop_reasoning.yaml"
  },
  {
    "id": "2025-12-23-arxiv-active_intelligence_in_video_avatars_via_closed_loop_world_modeling",
    "date": "2025-12-23",
    "title": "Active Intelligence in Video Avatars via Closed-loop World Modeling",
    "authors": "Xuanhua He, Tianyu Yang, Ke Cao, Ruiqi Wu, Cheng Meng",
    "abstract": "Current video avatar generation methods excel at identity preservation and motion alignment but lack genuine agency, they cannot autonomously pursue long-term goals through adaptive environmental interaction. We address this by introducing L-IVA (Long-horizon Interactive Visual Avatar), a task and benchmark for evaluating goal-directed planning in stochastic generative environments, and ORCA (Online Reasoning and Cognitive Architecture), the first framework enabling active intelligence in video ",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.20615v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-23-arxiv-active_intelligence_in_video_avatars_via_closed_loop_world_modeling.yaml"
  },
  {
    "id": "2025-12-22-arxiv-visual_aware_cot_achieving_high_fidelity_visual_consistency_in_unified_models",
    "date": "2025-12-22",
    "title": "Visual-Aware CoT: Achieving High-Fidelity Visual Consistency in Unified Models",
    "authors": "Zixuan Ye, Quande Liu, Cong Wei, Yuanxing Zhang, Xintao Wang",
    "abstract": "Recently, the introduction of Chain-of-Thought (CoT) has largely improved the generation ability of unified models. However, it is observed that the current thinking process during generation mainly focuses on the text consistency with the text prompt, ignoring the \\textbf{visual context consistency} with the visual reference images during the multi-modal generation, e.g., multi-reference generation. The lack of such consistency results in the failure in maintaining key visual features (like hum",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.19686v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-22-arxiv-visual_aware_cot_achieving_high_fidelity_visual_consistency_in_unified_models.yaml"
  },
  {
    "id": "2025-12-22-arxiv-efficient_vision_mamba_for_mri_super_resolution_via_hybrid_selective_scanning",
    "date": "2025-12-22",
    "title": "Efficient Vision Mamba for MRI Super-Resolution via Hybrid Selective Scanning",
    "authors": "Mojtaba Safari, Shansong Wang, Vanessa L Wildman, Mingzhe Hu, Zach Eidex",
    "abstract": "Background: High-resolution MRI is critical for diagnosis, but long acquisition times limit clinical use. Super-resolution (SR) can enhance resolution post-scan, yet existing deep learning methods face fidelity-efficiency trade-offs. Purpose: To develop a computationally efficient and accurate deep learning framework for MRI SR that preserves anatomical detail for clinical integration. Materials and Methods: We propose a novel SR framework combining multi-head selective state-space models (MHSSM",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.19676v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-22-arxiv-efficient_vision_mamba_for_mri_super_resolution_via_hybrid_selective_scanning.yaml"
  },
  {
    "id": "2025-12-22-arxiv-va_variational_policy_alignment_for_pixel_aware_autoregressive_generation",
    "date": "2025-12-22",
    "title": "VA-$π$: Variational Policy Alignment for Pixel-Aware Autoregressive Generation",
    "authors": "Xinyao Liao, Qiyuan He, Kai Xu, Xiaoye Qu, Yicong Li",
    "abstract": "Autoregressive (AR) visual generation relies on tokenizers to map images to and from discrete sequences. However, tokenizers are trained to reconstruct clean images from ground-truth tokens, while AR generators are optimized only for token likelihood. This misalignment leads to generated token sequences that may decode into low-quality images, without direct supervision from the pixel space. We propose VA-$π$, a lightweight post-training framework that directly optimizes AR models with a princip",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.19680v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-22-arxiv-va_variational_policy_alignment_for_pixel_aware_autoregressive_generation.yaml"
  },
  {
    "id": "2025-12-22-arxiv-scalably_enhancing_the_clinical_validity_of_a_task_benchmark_with_physician_over",
    "date": "2025-12-22",
    "title": "Scalably Enhancing the Clinical Validity of a Task Benchmark with Physician Oversight",
    "authors": "Junze Ye, Daniel Tawfik, Alex J. Goodell, Nikhil V. Kotha, Mark K. Buyyounouski",
    "abstract": "Automating the calculation of clinical risk scores offers a significant opportunity to reduce physician administrative burden and enhance patient care. The current standard for evaluating this capability is MedCalc-Bench, a large-scale dataset constructed using LLM-based feature extraction and rule-based aggregation. However, treating such model-generated benchmarks as static oracles risks enshrining historical model errors as evaluation gold standards, a problem dangerously amplified when these",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.19691v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-22-arxiv-scalably_enhancing_the_clinical_validity_of_a_task_benchmark_with_physician_over.yaml"
  },
  {
    "id": "2025-12-22-arxiv-bottom_up_policy_optimization_your_language_model_policy_secretly_contains_inter",
    "date": "2025-12-22",
    "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies",
    "authors": "Yuqiao Tan, Minzheng Wang, Shizhu He, Huanxuan Liao, Chengfeng Zhao",
    "abstract": "Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.19673v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-22-arxiv-bottom_up_policy_optimization_your_language_model_policy_secretly_contains_inter.yaml"
  },
  {
    "id": "2025-12-22-arxiv-from_indoor_to_open_world_revealing_the_spatial_reasoning_gap_in_mllms",
    "date": "2025-12-22",
    "title": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs",
    "authors": "Mingrui Wu, Zhaozhi Wang, Fangjinhua Wang, Jiaolong Yang, Marc Pollefeys",
    "abstract": "While Multimodal Large Language Models (MLLMs) have achieved impressive performance on semantic tasks, their spatial intelligence--crucial for robust and grounded AI systems--remains underdeveloped. Existing benchmarks fall short of diagnosing this limitation: they either focus on overly simplified qualitative reasoning or rely on domain-specific indoor data, constrained by the lack of outdoor datasets with verifiable metric ground truth. To bridge this gap, we introduce a large-scale benchmark ",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.19683v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-22-arxiv-from_indoor_to_open_world_revealing_the_spatial_reasoning_gap_in_mllms.yaml"
  },
  {
    "id": "2025-12-22-arxiv-beyond_clip_knowledge_enhanced_multimodal_transformers_for_cross_modal_alignment",
    "date": "2025-12-22",
    "title": "Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis",
    "authors": "Argha Kamal Samanta, Harshika Goyal, Vasudha Joshi, Tushar Mungle, Pabitra Mitra",
    "abstract": "Diabetic retinopathy (DR) is a leading cause of preventable blindness worldwide, demanding accurate automated diagnostic systems. While general-domain vision-language models like Contrastive Language-Image Pre-Training (CLIP) perform well on natural image tasks, they struggle in medical domain applications, particularly in cross-modal retrieval for ophthalmological images. We propose a novel knowledge-enhanced joint embedding framework that integrates retinal fundus images, clinical text, and st",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.19663v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-22-arxiv-beyond_clip_knowledge_enhanced_multimodal_transformers_for_cross_modal_alignment.yaml"
  },
  {
    "id": "2025-12-22-arxiv-pushing_the_frontier_of_audiovisual_perception_with_large_scale_multimodal_corre",
    "date": "2025-12-22",
    "title": "Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning",
    "authors": "Apoorv Vyas, Heng-Jui Chang, Cheng-Fu Yang, Po-Yao Huang, Luya Gao",
    "abstract": "We introduce Perception Encoder Audiovisual, PE-AV, a new family of encoders for audio and video understanding trained with scaled contrastive learning. Built on PE, PE-AV makes several key contributions to extend representations to audio, and natively support joint embeddings across audio-video, audio-text, and video-text modalities. PE-AV's unified cross-modal embeddings enable novel tasks such as speech retrieval, and set a new state of the art across standard audio and video benchmarks. We u",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.19687v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-22-arxiv-pushing_the_frontier_of_audiovisual_perception_with_large_scale_multimodal_corre.yaml"
  },
  {
    "id": "2025-12-22-arxiv-interact2ar_full_body_human_human_interaction_generation_via_autoregressive_diff",
    "date": "2025-12-22",
    "title": "Interact2Ar: Full-Body Human-Human Interaction Generation via Autoregressive Diffusion Models",
    "authors": "Pablo Ruiz-Ponce, Sergio Escalera, José García-Rodríguez, Jiankang Deng, Rolandos Alexandros Potamias",
    "abstract": "Generating realistic human-human interactions is a challenging task that requires not only high-quality individual body and hand motions, but also coherent coordination among all interactants. Due to limitations in available data and increased learning complexity, previous methods tend to ignore hand motions, limiting the realism and expressivity of the interactions. Additionally, current diffusion-based approaches generate entire motion sequences simultaneously, limiting their ability to captur",
    "category": "Hardware/Infra",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "hardware_infra"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.19692v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-22-arxiv-interact2ar_full_body_human_human_interaction_generation_via_autoregressive_diff.yaml"
  },
  {
    "id": "2025-12-22-arxiv-zero_shot_reconstruction_of_in_scene_object_manipulation_from_video",
    "date": "2025-12-22",
    "title": "Zero-shot Reconstruction of In-Scene Object Manipulation from Video",
    "authors": "Dixuan Lin, Tianyou Wang, Zhuoyang Pan, Yufu Wang, Lingjie Liu",
    "abstract": "We build the first system to address the problem of reconstructing in-scene object manipulation from a monocular RGB video. It is challenging due to ill-posed scene reconstruction, ambiguous hand-object depth, and the need for physically plausible interactions. Existing methods operate in hand centric coordinates and ignore the scene, hindering metric accuracy and practical use. In our method, we first use data-driven foundation models to initialize the core components, including the object mesh",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.19684v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-22-arxiv-zero_shot_reconstruction_of_in_scene_object_manipulation_from_video.yaml"
  },
  {
    "id": "2025-12-22-arxiv-the_prism_hypothesis_harmonizing_semantic_and_pixel_representations_via_unified",
    "date": "2025-12-22",
    "title": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding",
    "authors": "Weichen Fan, Haiwen Diao, Quan Wang, Dahua Lin, Ziwei Liu",
    "abstract": "Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that convey",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.19693v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-22-arxiv-the_prism_hypothesis_harmonizing_semantic_and_pixel_representations_via_unified.yaml"
  },
  {
    "id": "2025-12-22-arxiv-genenv_difficulty_aligned_co_evolution_between_llm_agents_and_environment_simula",
    "date": "2025-12-22",
    "title": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators",
    "authors": "Jiacheng Guo, Ling Yang, Peter Chen, Qixin Xiao, Yinjie Wang",
    "abstract": "Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating t",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.19682v1"
      }
    ],
    "source_file": "papers/weekly/2025-W51/2025-12-22-arxiv-genenv_difficulty_aligned_co_evolution_between_llm_agents_and_environment_simula.yaml"
  },
  {
    "id": "2025-12-19-arxiv-both_semantics_and_reconstruction_matter_making_representation_encoders_ready_fo",
    "date": "2025-12-19",
    "title": "Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing",
    "authors": "Shilong Zhang, He Zhang, Zhifei Zhang, Chongjian Ge, Shuchen Xue",
    "abstract": "Modern Latent Diffusion Models (LDMs) typically operate in low-level Variational Autoencoder (VAE) latent spaces that are primarily optimized for pixel-level reconstruction. To unify vision generation and understanding, a burgeoning trend is to adopt high-dimensional features from representation encoders as generative latents. However, we empirically identify two fundamental obstacles in this paradigm: (1) the discriminative feature space lacks compact regularization, making diffusion models pro",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.17909v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-19-arxiv-both_semantics_and_reconstruction_matter_making_representation_encoders_ready_fo.yaml"
  },
  {
    "id": "2025-12-19-arxiv-visually_prompted_benchmarks_are_surprisingly_fragile",
    "date": "2025-12-19",
    "title": "Visually Prompted Benchmarks Are Surprisingly Fragile",
    "authors": "Haiwen Feng, Long Lian, Lisa Dunlap, Jiahao Shu, XuDong Wang",
    "abstract": "A key challenge in evaluating VLMs is testing models' ability to analyze visual content independently from their textual priors. Recent benchmarks such as BLINK probe visual perception through visual prompting, where questions about visual content are paired with coordinates to which the question refers, with the coordinates explicitly marked in the image itself. While these benchmarks are an important part of VLM evaluation, we find that existing models are surprisingly fragile to seemingly irr",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.17875v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-19-arxiv-visually_prompted_benchmarks_are_surprisingly_fragile.yaml"
  },
  {
    "id": "2025-12-19-arxiv-learning_vertical_coordinates_via_automatic_differentiation_of_a_dynamical_core",
    "date": "2025-12-19",
    "title": "Learning vertical coordinates via automatic differentiation of a dynamical core",
    "authors": "Tim Whittaker, Seth Taylor, Elsa Cardoso-Bihlo, Alejandro Di Luca, Alex Bihlo",
    "abstract": "Terrain-following coordinates in atmospheric models often imprint their grid structure onto the solution, particularly over steep topography, where distorted coordinate layers can generate spurious horizontal and vertical motion. Standard formulations, such as hybrid or SLEVE coordinates, mitigate these errors by using analytic decay functions controlled by heuristic scale parameters that are typically tuned by hand and fixed a priori. In this work, we propose a framework to define a parametric ",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.17877v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-19-arxiv-learning_vertical_coordinates_via_automatic_differentiation_of_a_dynamical_core.yaml"
  },
  {
    "id": "2025-12-19-arxiv-distributionally_robust_imitation_learning_layered_control_architecture_for_cert",
    "date": "2025-12-19",
    "title": "Distributionally Robust Imitation Learning: Layered Control Architecture for Certifiable Autonomy",
    "authors": "Aditya Gahlawat, Ahmed Aboudonia, Sandeep Banik, Naira Hovakimyan, Nikolai Matni",
    "abstract": "Imitation learning (IL) enables autonomous behavior by learning from expert demonstrations. While more sample-efficient than comparative alternatives like reinforcement learning, IL is sensitive to compounding errors induced by distribution shifts. There are two significant sources of distribution shifts when using IL-based feedback laws on systems: distribution shifts caused by policy error and distribution shifts due to exogenous disturbances and endogenous model errors due to lack of learning",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.17899v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-19-arxiv-distributionally_robust_imitation_learning_layered_control_architecture_for_cert.yaml"
  },
  {
    "id": "2025-12-19-arxiv-anytask_an_automated_task_and_data_generation_framework_for_advancing_sim_to_rea",
    "date": "2025-12-19",
    "title": "AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning",
    "authors": "Ran Gong, Xiaohan Zhang, Jinghuan Shang, Maria Vittoria Minniti, Jigarkumar Patel",
    "abstract": "Generalist robot learning remains constrained by data: large-scale, diverse, and high-quality interaction data are expensive to collect in the real world. While simulation has become a promising way for scaling up data collection, the related tasks, including simulation task design, task-aware scene generation, expert demonstration synthesis, and sim-to-real transfer, still demand substantial human effort. We present AnyTask, an automated framework that pairs massively parallel GPU simulation wi",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.17853v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-19-arxiv-anytask_an_automated_task_and_data_generation_framework_for_advancing_sim_to_rea.yaml"
  },
  {
    "id": "2025-12-19-arxiv-keypoint_counting_classifiers_turning_vision_transformers_into_self_explainable",
    "date": "2025-12-19",
    "title": "Keypoint Counting Classifiers: Turning Vision Transformers into Self-Explainable Models Without Training",
    "authors": "Kristoffer Wickstrøm, Teresa Dorszewski, Siyan Chen, Michael Kampffmeyer, Elisabeth Wetzer",
    "abstract": "Current approaches for designing self-explainable models (SEMs) require complicated training procedures and specific architectures which makes them impractical. With the advance of general purpose foundation models based on Vision Transformers (ViTs), this impracticability becomes even more problematic. Therefore, new methods are necessary to provide transparency and reliability to ViT-based foundation models. In this work, we present a new method for turning any well-trained ViT-based model int",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.17891v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-19-arxiv-keypoint_counting_classifiers_turning_vision_transformers_into_self_explainable.yaml"
  },
  {
    "id": "2025-12-19-arxiv-diffusion_forcing_for_multi_agent_interaction_sequence_modeling",
    "date": "2025-12-19",
    "title": "Diffusion Forcing for Multi-Agent Interaction Sequence Modeling",
    "authors": "Vongani H. Maluleke, Kie Horiuchi, Lea Wilken, Evonne Ng, Jitendra Malik",
    "abstract": "Understanding and generating multi-person interactions is a fundamental challenge with broad implications for robotics and social computing. While humans naturally coordinate in groups, modeling such interactions remains difficult due to long temporal horizons, strong inter-agent dependencies, and variable group sizes. Existing motion generation methods are largely task-specific and do not generalize to flexible multi-agent generation. We introduce MAGNet (Multi-Agent Diffusion Forcing Transform",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.17900v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-19-arxiv-diffusion_forcing_for_multi_agent_interaction_sequence_modeling.yaml"
  },
  {
    "id": "2025-12-19-arxiv-infsplign_inference_time_spatial_alignment_of_text_to_image_diffusion_models",
    "date": "2025-12-19",
    "title": "InfSplign: Inference-Time Spatial Alignment of Text-to-Image Diffusion Models",
    "authors": "Sarah Rastegar, Violeta Chatalbasheva, Sieger Falkena, Anuj Singh, Yanbo Wang",
    "abstract": "Text-to-image (T2I) diffusion models generate high-quality images but often fail to capture the spatial relations specified in text prompts. This limitation can be traced to two factors: lack of fine-grained spatial supervision in training data and inability of text embeddings to encode spatial semantics. We introduce InfSplign, a training-free inference-time method that improves spatial alignment by adjusting the noise through a compound loss in every denoising step. Proposed loss leverages dif",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.17851v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-19-arxiv-infsplign_inference_time_spatial_alignment_of_text_to_image_diffusion_models.yaml"
  },
  {
    "id": "2025-12-19-arxiv-radargen_automotive_radar_point_cloud_generation_from_cameras",
    "date": "2025-12-19",
    "title": "RadarGen: Automotive Radar Point Cloud Generation from Cameras",
    "authors": "Tomer Borreda, Fangqiang Ding, Sanja Fidler, Shengyu Huang, Or Litany",
    "abstract": "We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery. RadarGen adapts efficient image-latent diffusion to the radar domain by representing radar measurements in bird's-eye-view form that encodes spatial structure together with radar cross section (RCS) and Doppler attributes. A lightweight recovery step reconstructs point clouds from the generated maps. To better align generation with the visual scene, RadarGen incorporate",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.17897v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-19-arxiv-radargen_automotive_radar_point_cloud_generation_from_cameras.yaml"
  },
  {
    "id": "2025-12-19-arxiv-when_reasoning_meets_its_laws",
    "date": "2025-12-19",
    "title": "When Reasoning Meets Its Laws",
    "authors": "Junyu Zhang, Yifan Sun, Tianang Leng, Jingyan Shen, Liu Ziyin",
    "abstract": "Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we ext",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.17901v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-19-arxiv-when_reasoning_meets_its_laws.yaml"
  },
  {
    "id": "2025-12-18-arxiv-adatooler_v_adaptive_tool_use_for_images_and_videos",
    "date": "2025-12-18",
    "title": "AdaTooler-V: Adaptive Tool-Use for Images and Videos",
    "authors": "Chaoyang Wang, Kaituo Feng, Dongyang Chen, Zhongyu Wang, Zhixun Li",
    "abstract": "Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.16918v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-18-arxiv-adatooler_v_adaptive_tool_use_for_images_and_videos.yaml"
  },
  {
    "id": "2025-12-18-arxiv-the_world_is_your_canvas_painting_promptable_events_with_reference_images_trajec",
    "date": "2025-12-18",
    "title": "The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text",
    "authors": "Hanlin Wang, Hao Ouyang, Qiuyu Wang, Yue Yu, Yihao Meng",
    "abstract": "We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable even",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.16924v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-18-arxiv-the_world_is_your_canvas_painting_promptable_events_with_reference_images_trajec.yaml"
  },
  {
    "id": "2025-12-18-nature-multimodal_out_of_distribution_individual_uncertainty_quantification_enhances_bi",
    "date": "2025-12-18",
    "title": "Multimodal out-of-distribution individual uncertainty quantification enhances binding affinity prediction for polypharmacology",
    "authors": "Amitesh Badkul, Li Xie, Shuo Zhang, Lei Xie",
    "abstract": "<p>Nature Machine Intelligence, Published online: 18 December 2025; <a href=\"https://www.nature.com/articles/s42256-025-01151-2\">doi:10.1038/s42256-025-01151-2</a></p>Badkul et al. develop eMOSAIC, a method that improves drug discovery by accurately predicting the interaction mechanics of various compounds with multiple proteins.",
    "category": "Model",
    "source": "nature",
    "score": 110,
    "tags": [
      "ai",
      "research",
      "nature",
      "model"
    ],
    "links": [
      {
        "title": "View Paper",
        "url": "https://www.nature.com/articles/s42256-025-01151-2"
      },
      {
        "title": "DOI",
        "url": "https://doi.org/10.1038/s42256-025-01151-2"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-18-nature-multimodal_out_of_distribution_individual_uncertainty_quantification_enhances_bi.yaml"
  },
  {
    "id": "2025-12-18-nature-learning_cell_dynamics_with_neural_differential_equations",
    "date": "2025-12-18",
    "title": "Learning cell dynamics with neural differential equations",
    "authors": "Michael E. Vinyard, Anders W. Rasmussen, Ruitong Li, Allon M. Klein, Gad Getz",
    "abstract": "<p>Nature Machine Intelligence, Published online: 18 December 2025; <a href=\"https://www.nature.com/articles/s42256-025-01150-3\">doi:10.1038/s42256-025-01150-3</a></p>Vinyard et al. present a generative method to model cell dynamics using neural stochastic differential equations that learn state-dependent drift and diffusion, outperforming existing approaches and enabling perturbation studies of development and disease.",
    "category": "Model",
    "source": "nature",
    "score": 115,
    "tags": [
      "ai",
      "research",
      "nature",
      "model"
    ],
    "links": [
      {
        "title": "View Paper",
        "url": "https://www.nature.com/articles/s42256-025-01150-3"
      },
      {
        "title": "DOI",
        "url": "https://doi.org/10.1038/s42256-025-01150-3"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-18-nature-learning_cell_dynamics_with_neural_differential_equations.yaml"
  },
  {
    "id": "2025-12-18-arxiv-next_embedding_prediction_makes_strong_vision_learners",
    "date": "2025-12-18",
    "title": "Next-Embedding Prediction Makes Strong Vision Learners",
    "authors": "Sihan Xu, Ziqiao Ma, Wenhao Chai, Xuweiyi Chen, Weiyang Jin",
    "abstract": "Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gra",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.16922v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-18-arxiv-next_embedding_prediction_makes_strong_vision_learners.yaml"
  },
  {
    "id": "2025-12-18-nature-a_psychometric_framework_for_evaluating_and_shaping_personality_traits_in_large",
    "date": "2025-12-18",
    "title": "A psychometric framework for evaluating and shaping personality traits in large language models",
    "authors": "Gregory Serapio-García, Mustafa Safdari, Clément Crepy, Luning Sun, Stephen Fitz",
    "abstract": "<p>Nature Machine Intelligence, Published online: 18 December 2025; <a href=\"https://www.nature.com/articles/s42256-025-01115-6\">doi:10.1038/s42256-025-01115-6</a></p>Serapio-García, Safdari and colleagues develop a method based on psychometric tests to measure and validate personality-like traits in LLMs. Large, instruction-tuned models give reliable personality measurement results, and specific personality profiles can be mimicked in downstream tasks.",
    "category": "Evaluation",
    "source": "nature",
    "score": 115,
    "tags": [
      "ai",
      "research",
      "nature",
      "evaluation"
    ],
    "links": [
      {
        "title": "View Paper",
        "url": "https://www.nature.com/articles/s42256-025-01115-6"
      },
      {
        "title": "DOI",
        "url": "https://doi.org/10.1038/s42256-025-01115-6"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-18-nature-a_psychometric_framework_for_evaluating_and_shaping_personality_traits_in_large.yaml"
  },
  {
    "id": "2025-12-18-arxiv-stereopilot_learning_unified_and_efficient_stereo_conversion_via_generative_prio",
    "date": "2025-12-18",
    "title": "StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors",
    "authors": "Guibao Shen, Yihua Du, Wenhang Ge, Jing He, Chirui Chang",
    "abstract": "The rapid growth of stereoscopic displays, including VR headsets and 3D cinemas, has led to increasing demand for high-quality stereo video content. However, producing 3D videos remains costly and complex, while automatic Monocular-to-Stereo conversion is hindered by the limitations of the multi-stage ``Depth-Warp-Inpaint'' (DWI) pipeline. This paradigm suffers from error propagation, depth ambiguity, and format inconsistency between parallel and converged stereo configurations. To address these",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.16915v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-18-arxiv-stereopilot_learning_unified_and_efficient_stereo_conversion_via_generative_prio.yaml"
  },
  {
    "id": "2025-12-18-arxiv-easyv2v_a_high_quality_instruction_based_video_editing_framework",
    "date": "2025-12-18",
    "title": "EasyV2V: A High-quality Instruction-based Video Editing Framework",
    "authors": "Jinjie Mai, Chaoyang Wang, Guocheng Gordon Qian, Willi Menapace, Sergey Tulyakov",
    "abstract": "While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \\emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine mo",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.16920v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-18-arxiv-easyv2v_a_high_quality_instruction_based_video_editing_framework.yaml"
  },
  {
    "id": "2025-12-18-arxiv-differences_that_matter_auditing_models_for_capability_gap_discovery_and_rectifi",
    "date": "2025-12-18",
    "title": "Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification",
    "authors": "Qihao Liu, Chengzhi Mao, Yaojie Liu, Alan Yuille, Wen-Sheng Chu",
    "abstract": "Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once ",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.16921v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-18-arxiv-differences_that_matter_auditing_models_for_capability_gap_discovery_and_rectifi.yaml"
  },
  {
    "id": "2025-12-18-arxiv-exploration_v_s_exploitation_rethinking_rlvr_through_clipping_entropy_and_spurio",
    "date": "2025-12-18",
    "title": "Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward",
    "authors": "Peter Chen, Xiaopeng Li, Ziniu Li, Wotao Yin, Xi Chen",
    "abstract": "This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward ",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.16912v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-18-arxiv-exploration_v_s_exploitation_rethinking_rlvr_through_clipping_entropy_and_spurio.yaml"
  },
  {
    "id": "2025-12-18-arxiv-dvgt_driving_visual_geometry_transformer",
    "date": "2025-12-18",
    "title": "DVGT: Driving Visual Geometry Transformer",
    "authors": "Sicheng Zuo, Zixun Xie, Wenzhao Zheng, Shaoqing Xu, Fang Li",
    "abstract": "Perceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs. We first extract visual features for each image using a DINO backbone, and emp",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.16919v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-18-arxiv-dvgt_driving_visual_geometry_transformer.yaml"
  },
  {
    "id": "2025-12-18-arxiv-generative_adversarial_reasoner_enhancing_llm_reasoning_with_adversarial_reinfor",
    "date": "2025-12-18",
    "title": "Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning",
    "authors": "Qihao Liu, Luoxin Ye, Wufei Ma, Yu-Cheng Chou, Alan Yuille",
    "abstract": "Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule part",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.16917v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-18-arxiv-generative_adversarial_reasoner_enhancing_llm_reasoning_with_adversarial_reinfor.yaml"
  },
  {
    "id": "2025-12-18-arxiv-depth_any_panoramas_a_foundation_model_for_panoramic_depth_estimation",
    "date": "2025-12-18",
    "title": "Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation",
    "authors": "Xin Lin, Meixi Song, Dizhe Zhang, Wenxuan Lu, Haodong Li",
    "abstract": "In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.16913v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-18-arxiv-depth_any_panoramas_a_foundation_model_for_panoramic_depth_estimation.yaml"
  },
  {
    "id": "2025-12-17-arxiv-gatefusion_hierarchical_gated_cross_modal_fusion_for_active_speaker_detection",
    "date": "2025-12-17",
    "title": "GateFusion: Hierarchical Gated Cross-Modal Fusion for Active Speaker Detection",
    "authors": "Yu Wang, Juhyung Ha, Frangil M. Ramirez, Yuchen Wang, David J. Crandall",
    "abstract": "Active Speaker Detection (ASD) aims to identify who is currently speaking in each frame of a video. Most state-of-the-art approaches rely on late fusion to combine visual and audio features, but late fusion often fails to capture fine-grained cross-modal interactions, which can be critical for robust performance in unconstrained scenarios. In this paper, we introduce GateFusion, a novel architecture that combines strong pretrained unimodal encoders with a Hierarchical Gated Fusion Decoder (HiGat",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.15707v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-17-arxiv-gatefusion_hierarchical_gated_cross_modal_fusion_for_active_speaker_detection.yaml"
  },
  {
    "id": "2025-12-17-arxiv-predictive_concept_decoders_training_scalable_end_to_end_interpretability_assist",
    "date": "2025-12-17",
    "title": "Predictive Concept Decoders: Training Scalable End-to-End Interpretability Assistants",
    "authors": "Vincent Huang, Dami Choi, Daniel D. Johnson, Sarah Schwettmann, Jacob Steinhardt",
    "abstract": "Interpreting the internal activations of neural networks can produce more faithful explanations of their behavior, but is difficult due to the complex structure of activation space. Existing approaches to scalable interpretability use hand-designed agents that make and test hypotheses about how internal activations relate to external behavior. We propose to instead turn this task into an end-to-end training objective, by training interpretability assistants to accurately predict model behavior f",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.15712v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-17-arxiv-predictive_concept_decoders_training_scalable_end_to_end_interpretability_assist.yaml"
  },
  {
    "id": "2025-12-17-arxiv-skyra_ai_generated_video_detection_via_grounded_artifact_reasoning",
    "date": "2025-12-17",
    "title": "Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning",
    "authors": "Yifei Li, Wenzhao Zheng, Yanran Zhang, Runze Sun, Yu Zheng",
    "abstract": "The misuse of AI-driven video generation technologies has raised serious social concerns, highlighting the urgent need for reliable AI-generated video detectors. However, most existing methods are limited to binary classification and lack the necessary explanations for human interpretation. In this paper, we present Skyra, a specialized multimodal large language model (MLLM) that identifies human-perceivable visual artifacts in AI-generated videos and leverages them as grounded evidence for both",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.15693v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-17-arxiv-skyra_ai_generated_video_detection_via_grounded_artifact_reasoning.yaml"
  },
  {
    "id": "2025-12-17-arxiv-frontiercs_evolving_challenges_for_evolving_intelligence",
    "date": "2025-12-17",
    "title": "FrontierCS: Evolving Challenges for Evolving Intelligence",
    "authors": "Qiuyang Mang, Wenhao Chai, Zhifei Li, Huanzhi Mao, Shang Zhou",
    "abstract": "We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated. Models solve these tasks by implementing executable programs rather",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.15699v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-17-arxiv-frontiercs_evolving_challenges_for_evolving_intelligence.yaml"
  },
  {
    "id": "2025-12-17-arxiv-vlic_vision_language_models_as_perceptual_judges_for_human_aligned_image_compres",
    "date": "2025-12-17",
    "title": "VLIC: Vision-Language Models As Perceptual Judges for Human-Aligned Image Compression",
    "authors": "Kyle Sargent, Ruiqi Gao, Philipp Henzler, Charles Herrmann, Aleksander Holynski",
    "abstract": "Evaluations of image compression performance which include human preferences have generally found that naive distortion functions such as MSE are insufficiently aligned to human perception. In order to align compression models to human perception, prior work has employed differentiable perceptual losses consisting of neural networks calibrated on large-scale datasets of human psycho-visual judgments. We show that, surprisingly, state-of-the-art vision-language models (VLMs) can replicate binary ",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.15701v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-17-arxiv-vlic_vision_language_models_as_perceptual_judges_for_human_aligned_image_compres.yaml"
  },
  {
    "id": "2025-12-17-arxiv-dynamic_rebatching_for_efficient_early_exit_inference_with_drex",
    "date": "2025-12-17",
    "title": "Dynamic Rebatching for Efficient Early-Exit Inference with DREX",
    "authors": "Xuting Liu, Daniel Alexander, Siva Kesava Reddy Kakarla, Behnaz Arzani, Vincent Liu",
    "abstract": "Early-Exit (EE) is a Large Language Model (LLM) architecture that accelerates inference by allowing easier tokens to be generated using only a subset of the model's layers. However, traditional batching frameworks are ill-suited for EE LLMs, as not all requests in a batch may be ready to exit at the same time. Existing solutions either force a uniform decision on the batch, which overlooks EE opportunities, or degrade output quality by forcing premature exits. We propose Dynamic Rebatching, a so",
    "category": "Hardware/Infra",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "hardware_infra"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.15705v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-17-arxiv-dynamic_rebatching_for_efficient_early_exit_inference_with_drex.yaml"
  },
  {
    "id": "2025-12-17-arxiv-end_to_end_training_for_autoregressive_video_diffusion_via_self_resampling",
    "date": "2025-12-17",
    "title": "End-to-End Training for Autoregressive Video Diffusion via Self-Resampling",
    "authors": "Yuwei Guo, Ceyuan Yang, Hao He, Yang Zhao, Meng Wei",
    "abstract": "Autoregressive video diffusion models hold promise for world simulation but are vulnerable to exposure bias arising from the train-test mismatch. While recent works address this via post-training, they typically rely on a bidirectional teacher model or online discriminator. To achieve an end-to-end solution, we introduce Resampling Forcing, a teacher-free framework that enables training autoregressive video models from scratch and at scale. Central to our approach is a self-resampling scheme tha",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.15702v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-17-arxiv-end_to_end_training_for_autoregressive_video_diffusion_via_self_resampling.yaml"
  },
  {
    "id": "2025-12-17-arxiv-spatia_video_generation_with_updatable_spatial_memory",
    "date": "2025-12-17",
    "title": "Spatia: Video Generation with Updatable Spatial Memory",
    "authors": "Jinjing Zhao, Fangyun Wei, Zhening Liu, Hongyang Zhang, Chang Xu",
    "abstract": "Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement desig",
    "category": "Hardware/Infra",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "hardware_infra"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.15716v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-17-arxiv-spatia_video_generation_with_updatable_spatial_memory.yaml"
  },
  {
    "id": "2025-12-17-nature-network_aware_self_supervised_learning_enables_high_content_phenotypic_screening",
    "date": "2025-12-17",
    "title": "Network-aware self-supervised learning enables high-content phenotypic screening for genetic modifiers of neuronal activity dynamics",
    "authors": "Parker Grosjean, Kaivalya Shevade, Cuong Nguyen, Sarah Ancheta, Karl Mader",
    "abstract": "<p>Nature Machine Intelligence, Published online: 17 December 2025; <a href=\"https://www.nature.com/articles/s42256-025-01156-x\">doi:10.1038/s42256-025-01156-x</a></p>Grosjean et al. present a network-aware, self-supervised learning approach for screening neuronal activity dynamics. They demonstrate its applicability across a range of neural interventions.",
    "category": "Model",
    "source": "nature",
    "score": 115,
    "tags": [
      "ai",
      "research",
      "nature",
      "model"
    ],
    "links": [
      {
        "title": "View Paper",
        "url": "https://www.nature.com/articles/s42256-025-01156-x"
      },
      {
        "title": "DOI",
        "url": "https://doi.org/10.1038/s42256-025-01156-x"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-17-nature-network_aware_self_supervised_learning_enables_high_content_phenotypic_screening.yaml"
  },
  {
    "id": "2025-12-17-nature-solving_finite_element_methods_with_spiking_networks",
    "date": "2025-12-17",
    "title": "Solving finite element methods with spiking networks",
    "authors": "Wenhao Song, Zixu Wang, J. Joshua Yang",
    "abstract": "<p>Nature Machine Intelligence, Published online: 17 December 2025; <a href=\"https://www.nature.com/articles/s42256-025-01158-9\">doi:10.1038/s42256-025-01158-9</a></p>Brain-inspired computing can enhance the finite element method, a cornerstone of scientific modelling, by reducing energy costs and reframing numerical simulation through neural dynamics.",
    "category": "Model",
    "source": "nature",
    "score": 110,
    "tags": [
      "ai",
      "research",
      "nature",
      "model"
    ],
    "links": [
      {
        "title": "View Paper",
        "url": "https://www.nature.com/articles/s42256-025-01158-9"
      },
      {
        "title": "DOI",
        "url": "https://doi.org/10.1038/s42256-025-01158-9"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-17-nature-solving_finite_element_methods_with_spiking_networks.yaml"
  },
  {
    "id": "2025-12-17-arxiv-in_pursuit_of_pixel_supervision_for_visual_pre_training",
    "date": "2025-12-17",
    "title": "In Pursuit of Pixel Supervision for Visual Pre-training",
    "authors": "Lihe Yang, Shang-Wen Li, Yang Li, Xinjie Lei, Dong Wang",
    "abstract": "At the most basic level, pixels are the source of the visual information through which we perceive the world. Pixels contain information at all levels, ranging from low-level attributes to high-level concepts. Autoencoders represent a classical and long-standing paradigm for learning representations from pixels or other raw inputs. In this work, we demonstrate that autoencoder-based self-supervised learning remains competitive today and can produce strong representations for downstream tasks, wh",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.15715v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-17-arxiv-in_pursuit_of_pixel_supervision_for_visual_pre_training.yaml"
  },
  {
    "id": "2025-12-17-arxiv-mimic_video_video_action_models_for_generalizable_robot_control_beyond_vlas",
    "date": "2025-12-17",
    "title": "mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs",
    "authors": "Jonas Pai, Liam Achenbach, Victoriano Montesinos, Benedek Forrai, Oier Mees",
    "abstract": "Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data. As a result, despite improved semantic generalization, the policy must implicitly infer complex physical dynamics and temporal dependencies solely from robot trajectories. This reliance creates an unsustainable data burden, necessitating continuous, large-scale expert data collection to compensate for the lack of innate phy",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.15692v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-17-arxiv-mimic_video_video_action_models_for_generalizable_robot_control_beyond_vlas.yaml"
  },
  {
    "id": "2025-12-17-arxiv-diffusionvl_translating_any_autoregressive_models_into_diffusion_vision_language",
    "date": "2025-12-17",
    "title": "DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models",
    "authors": "Lunbin Zeng, Jingfeng Yao, Bencheng Liao, Hongyuan Tao, Wenyu Liu",
    "abstract": "In recent multimodal research, the diffusion paradigm has emerged as a promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In res",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.15713v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-17-arxiv-diffusionvl_translating_any_autoregressive_models_into_diffusion_vision_language.yaml"
  },
  {
    "id": "2025-12-17-arxiv-gaussian_pixel_codec_avatars_a_hybrid_representation_for_efficient_rendering",
    "date": "2025-12-17",
    "title": "Gaussian Pixel Codec Avatars: A Hybrid Representation for Efficient Rendering",
    "authors": "Divam Gupta, Anuj Pahuja, Nemanja Bartolovic, Tomas Simon, Forrest Iandola",
    "abstract": "We present Gaussian Pixel Codec Avatars (GPiCA), photorealistic head avatars that can be generated from multi-view images and efficiently rendered on mobile devices. GPiCA utilizes a unique hybrid representation that combines a triangle mesh and anisotropic 3D Gaussians. This combination maximizes memory and rendering efficiency while maintaining a photorealistic appearance. The triangle mesh is highly efficient in representing surface areas like facial skin, while the 3D Gaussians effectively h",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.15711v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-17-arxiv-gaussian_pixel_codec_avatars_a_hybrid_representation_for_efficient_rendering.yaml"
  },
  {
    "id": "2025-12-16-arxiv-memflow_flowing_adaptive_memory_for_consistent_and_efficient_long_video_narrativ",
    "date": "2025-12-16",
    "title": "MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives",
    "authors": "Sihui Ji, Xi Chen, Shuai Yang, Xin Tao, Pengfei Wan",
    "abstract": "The core challenge for streaming video generation is maintaining the content consistency in long context, which poses high requirement for the memory design. Most existing solutions maintain the memory by compressing historical frames with predefined strategies. However, different to-generate video chunks should refer to different historical cues, which is hard to satisfy with fixed strategies. In this work, we propose MemFlow to address this problem. Specifically, before generating the coming c",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.14699v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-16-arxiv-memflow_flowing_adaptive_memory_for_consistent_and_efficient_long_video_narrativ.yaml"
  },
  {
    "id": "2025-12-16-arxiv-native_and_compact_structured_latents_for_3d_generation",
    "date": "2025-12-16",
    "title": "Native and Compact Structured Latents for 3D Generation",
    "authors": "Jianfeng Xiang, Xiaoxue Chen, Sicheng Xu, Ruicheng Wang, Zelong Lv",
    "abstract": "Recent advancements in 3D generative modeling have significantly improved the generation realism, yet the field is still hampered by existing representations, which struggle to capture assets with complex topologies and detailed appearance. This paper present an approach for learning a structured latent representation from native 3D data to address this challenge. At its core is a new sparse voxel structure called O-Voxel, an omni-voxel representation that encodes both geometry and appearance. O",
    "category": "Hardware/Infra",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "hardware_infra"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.14692v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-16-arxiv-native_and_compact_structured_latents_for_3d_generation.yaml"
  },
  {
    "id": "2025-12-16-arxiv-early_warning_index_for_patient_deteriorations_in_hospitals",
    "date": "2025-12-16",
    "title": "Early Warning Index for Patient Deteriorations in Hospitals",
    "authors": "Dimitris Bertsimas, Yu Ma, Kimberly Villalobos Carballo, Gagan Singh, Michal Laskowski",
    "abstract": "Hospitals lack automated systems to harness the growing volume of heterogeneous clinical and operational data to effectively forecast critical events. Early identification of patients at risk for deterioration is essential not only for patient care quality monitoring but also for physician care management. However, translating varied data streams into accurate and interpretable risk assessments poses significant challenges due to inconsistent data formats. We develop a multimodal machine learnin",
    "category": "Project",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "project"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.14683v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-16-arxiv-early_warning_index_for_patient_deteriorations_in_hospitals.yaml"
  },
  {
    "id": "2025-12-16-arxiv-crisp_contact_guided_real2sim_from_monocular_video_with_planar_scene_primitives",
    "date": "2025-12-16",
    "title": "CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives",
    "authors": "Zihan Wang, Jiashun Wang, Jeff Tan, Yiwen Zhao, Jessica Hodgins",
    "abstract": "We introduce CRISP, a method that recovers simulatable human motion and scene geometry from monocular video. Prior work on joint human-scene reconstruction relies on data-driven priors and joint optimization with no physics in the loop, or recovers noisy geometry with artifacts that cause motion tracking policies with scene interactions to fail. In contrast, our key insight is to recover convex, clean, and simulation-ready geometry by fitting planar primitives to a point cloud reconstruction of ",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.14696v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-16-arxiv-crisp_contact_guided_real2sim_from_monocular_video_with_planar_scene_primitives.yaml"
  },
  {
    "id": "2025-12-16-arxiv-timelens_rethinking_video_temporal_grounding_with_multimodal_llms",
    "date": "2025-12-16",
    "title": "TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs",
    "authors": "Jun Zhang, Teng Wang, Yuying Ge, Yixiao Ge, Xinhao Li",
    "abstract": "This paper does not introduce a novel method but instead establishes a straightforward, incremental, yet essential baseline for video temporal grounding (VTG), a core capability in video understanding. While multimodal large language models (MLLMs) excel at various video understanding tasks, the recipes for optimizing them for VTG remain under-explored. In this paper, we present TimeLens, a systematic investigation into building MLLMs with strong VTG ability, along two primary dimensions: data q",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.14698v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-16-arxiv-timelens_rethinking_video_temporal_grounding_with_multimodal_llms.yaml"
  },
  {
    "id": "2025-12-16-arxiv-universal_reasoning_model",
    "date": "2025-12-16",
    "title": "Universal Reasoning Model",
    "authors": "Zitian Gao, Lynx Chen, Yihao Xiao, He Xing, Ran Tao",
    "abstract": "Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.14693v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-16-arxiv-universal_reasoning_model.yaml"
  },
  {
    "id": "2025-12-16-arxiv-chip_adaptive_compliance_for_humanoid_control_through_hindsight_perturbation",
    "date": "2025-12-16",
    "title": "CHIP: Adaptive Compliance for Humanoid Control through Hindsight Perturbation",
    "authors": "Sirui Chen, Zi-ang Cao, Zhengyi Luo, Fernando Castañeda, Chenran Li",
    "abstract": "Recent progress in humanoid robots has unlocked agile locomotion skills, including backflipping, running, and crawling. Yet it remains challenging for a humanoid robot to perform forceful manipulation tasks such as moving objects, wiping, and pushing a cart. We propose adaptive Compliance Humanoid control through hIsight Perturbation (CHIP), a plug-and-play module that enables controllable end-effector stiffness while preserving agile tracking of dynamic reference motions. CHIP is easy to implem",
    "category": "Policy",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "policy"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.14689v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-16-arxiv-chip_adaptive_compliance_for_humanoid_control_through_hindsight_perturbation.yaml"
  },
  {
    "id": "2025-12-16-arxiv-spherical_leech_quantization_for_visual_tokenization_and_generation",
    "date": "2025-12-16",
    "title": "Spherical Leech Quantization for Visual Tokenization and Generation",
    "authors": "Yue Zhao, Hanwen Jiang, Zhenlin Xu, Chutong Yang, Ehsan Adeli",
    "abstract": "Non-parametric quantization has received much attention due to its efficiency on parameters and scalability to a large codebook. In this paper, we present a unified formulation of different non-parametric quantization methods through the lens of lattice coding. The geometry of lattice codes explains the necessity of auxiliary loss terms when training auto-encoders with certain existing lookup-free quantization variants such as BSQ. As a step forward, we explore a few possible candidates, includi",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.14697v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-16-arxiv-spherical_leech_quantization_for_visual_tokenization_and_generation.yaml"
  },
  {
    "id": "2025-12-16-arxiv-mmgr_multi_modal_generative_reasoning",
    "date": "2025-12-16",
    "title": "MMGR: Multi-Modal Generative Reasoning",
    "authors": "Zefan Cai, Haoyi Qiu, Tianyi Ma, Haozhe Zhao, Gengze Zhou",
    "abstract": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework ",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.14691v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-16-arxiv-mmgr_multi_modal_generative_reasoning.yaml"
  },
  {
    "id": "2025-12-16-arxiv-spoken_dialogsum_an_emotion_rich_conversational_dataset_for_spoken_dialogue_summ",
    "date": "2025-12-16",
    "title": "Spoken DialogSum: An Emotion-Rich Conversational Dataset for Spoken Dialogue Summarization",
    "authors": "Yen-Ju Lu, Kunxiao Gao, Mingrui Liang, Helin Wang, Thomas Thebaud",
    "abstract": "Recent audio language models can follow long conversations. However, research on emotion-aware or spoken dialogue summarization is constrained by the lack of data that links speech, summaries, and paralinguistic cues. We introduce Spoken DialogSum, the first corpus aligning raw conversational audio with factual summaries, emotion-rich summaries, and utterance-level labels for speaker age, gender, and emotion. The dataset is built in two stages: first, an LLM rewrites DialogSum scripts with Switc",
    "category": "Project",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "project"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.14687v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-16-arxiv-spoken_dialogsum_an_emotion_rich_conversational_dataset_for_spoken_dialogue_summ.yaml"
  },
  {
    "id": "2025-12-15-arxiv-agentiad_tool_augmented_single_agent_for_industrial_anomaly_detection",
    "date": "2025-12-15",
    "title": "AgentIAD: Tool-Augmented Single-Agent for Industrial Anomaly Detection",
    "authors": "Junwen Miao, Penghui Du, Yi Liu, Yu Wang, Yan Wang",
    "abstract": "Industrial anomaly detection (IAD) is difficult due to the scarcity of normal reference samples and the subtle, localized nature of many defects. Single-pass vision-language models (VLMs) often overlook small abnormalities and lack explicit mechanisms to compare against canonical normal patterns. We propose AgentIAD, a tool-driven agentic framework that enables multi-stage visual inspection. The agent is equipped with a Perceptive Zoomer (PZ) for localized fine-grained analysis and a Comparative",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.13671v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-15-arxiv-agentiad_tool_augmented_single_agent_for_industrial_anomaly_detection.yaml"
  },
  {
    "id": "2025-12-15-arxiv-laser_layer_wise_scale_alignment_for_training_free_streaming_4d_reconstruction",
    "date": "2025-12-15",
    "title": "LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction",
    "authors": "Tianye Ding, Yiming Xie, Yiqing Liang, Moitreya Chatterjee, Pedro Miraldo",
    "abstract": "Recent feed-forward reconstruction models like VGGT and $π^3$ achieve impressive reconstruction quality but cannot process streaming videos due to quadratic memory complexity, limiting their practical deployment. While existing streaming methods address this through learned memory mechanisms or causal attention, they require extensive retraining and may not fully leverage the strong geometric priors of state-of-the-art offline models. We propose LASER, a training-free framework that converts an ",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.13680v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-15-arxiv-laser_layer_wise_scale_alignment_for_training_free_streaming_4d_reconstruction.yaml"
  },
  {
    "id": "2025-12-15-arxiv-a_scientific_reasoning_model_for_organic_synthesis_procedure_generation",
    "date": "2025-12-15",
    "title": "A Scientific Reasoning Model for Organic Synthesis Procedure Generation",
    "authors": "Guoqing Liu, Junren Li, Zihan Zhao, Eray Inanc, Krzysztof Maziarz",
    "abstract": "Solving computer-aided synthesis planning is essential for enabling fully automated, robot-assisted synthesis workflows and improving the efficiency of drug discovery. A key challenge, however, is bridging the gap between computational route design and practical laboratory execution, particularly the accurate prediction of viable experimental procedures for each synthesis step. In this work, we present QFANG, a scientific reasoning language model capable of generating precise, structured experim",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.13668v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-15-arxiv-a_scientific_reasoning_model_for_organic_synthesis_procedure_generation.yaml"
  },
  {
    "id": "2025-12-15-arxiv-towards_interactive_intelligence_for_digital_humans",
    "date": "2025-12-15",
    "title": "Towards Interactive Intelligence for Digital Humans",
    "authors": "Yiyi Cai, Xuangeng Chu, Xiwei Gao, Sitong Gong, Yifei Huang",
    "abstract": "We introduce Interactive Intelligence, a novel paradigm of digital human that is capable of personality-aligned expression, adaptive interaction, and self-evolution. To realize this, we present Mio (Multimodal Interactive Omni-Avatar), an end-to-end framework composed of five specialized modules: Thinker, Talker, Face Animator, Body Animator, and Renderer. This unified architecture integrates cognitive reasoning with real-time multimodal embodiment to enable fluid, consistent interaction. Furthe",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.13674v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-15-arxiv-towards_interactive_intelligence_for_digital_humans.yaml"
  },
  {
    "id": "2025-12-15-arxiv-from_code_to_field_evaluating_the_robustness_of_convolutional_neural_networks_fo",
    "date": "2025-12-15",
    "title": "From Code to Field: Evaluating the Robustness of Convolutional Neural Networks for Disease Diagnosis in Mango Leaves",
    "authors": "Gabriel Vitorino de Andrade, Saulo Roberto dos Santos, Itallo Patrick Castro Alves da Silva, Emanuel Adler Medeiros Pereira, Erick de Andrade Barboza",
    "abstract": "The validation and verification of artificial intelligence (AI) models through robustness assessment are essential to guarantee the reliable performance of intelligent systems facing real-world challenges, such as image corruptions including noise, blurring, and weather variations. Despite the global importance of mango (Mangifera indica L.), there is a lack of studies on the robustness of models for the diagnosis of disease in its leaves. This paper proposes a methodology to evaluate convolutio",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.13641v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-15-arxiv-from_code_to_field_evaluating_the_robustness_of_convolutional_neural_networks_fo.yaml"
  },
  {
    "id": "2025-12-15-arxiv-litept_lighter_yet_stronger_point_transformer",
    "date": "2025-12-15",
    "title": "LitePT: Lighter Yet Stronger Point Transformer",
    "authors": "Yuanwen Yue, Damien Robert, Jianyuan Wang, Sunghwan Hong, Jan Dirk Wegner",
    "abstract": "Modern neural architectures for 3D point cloud processing contain both convolutional layers and attention blocks, but the best way to assemble them remains unclear. We analyse the role of different computational blocks in 3D point cloud networks and find an intuitive behaviour: convolution is adequate to extract low-level geometry at high-resolution in early layers, where attention is expensive without bringing any benefits; attention captures high-level semantics and context in low-resolution, ",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.13689v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-15-arxiv-litept_lighter_yet_stronger_point_transformer.yaml"
  },
  {
    "id": "2025-12-15-arxiv-embedding_based_rankings_of_educational_resources_based_on_learning_outcome_alig",
    "date": "2025-12-15",
    "title": "Embedding-Based Rankings of Educational Resources based on Learning Outcome Alignment: Benchmarking, Expert Validation, and Learner Performance",
    "authors": "Mohammadreza Molavi, Mohammad Moein, Mohammadreza Tavakoli, Abdolali Faraji, Stefan T. Mol",
    "abstract": "As the online learning landscape evolves, the need for personalization is increasingly evident. Although educational resources are burgeoning, educators face challenges selecting materials that both align with intended learning outcomes and address diverse learner needs. Large Language Models (LLMs) are attracting growing interest for their potential to create learning resources that better support personalization, but verifying coverage of intended outcomes still requires human alignment review",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.13658v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-15-arxiv-embedding_based_rankings_of_educational_resources_based_on_learning_outcome_alig.yaml"
  },
  {
    "id": "2025-12-15-arxiv-beyond_surface_form_a_pipeline_for_semantic_analysis_in_alzheimer_s_disease_dete",
    "date": "2025-12-15",
    "title": "Beyond surface form: A pipeline for semantic analysis in Alzheimer's Disease detection from spontaneous speech",
    "authors": "Dylan Phelps, Rodrigo Wilkens, Edward Gow-Smith, Lilian Hubner, Bárbara Malcorra",
    "abstract": "Alzheimer's Disease (AD) is a progressive neurodegenerative condition that adversely affects cognitive abilities. Language-related changes can be automatically identified through the analysis of outputs from linguistic assessment tasks, such as picture description. Language models show promise as a basis for screening tools for AD, but their limited interpretability poses a challenge in distinguishing true linguistic markers of cognitive decline from surface-level textual patterns. To address th",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.13685v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-15-arxiv-beyond_surface_form_a_pipeline_for_semantic_analysis_in_alzheimer_s_disease_dete.yaml"
  },
  {
    "id": "2025-12-15-arxiv-robotracer_mastering_spatial_trace_with_reasoning_in_vision_language_models_for",
    "date": "2025-12-15",
    "title": "RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics",
    "authors": "Enshen Zhou, Cheng Chi, Yibo Li, Jingkun An, Jiayuan Zhang",
    "abstract": "Spatial tracing, as a fundamental embodied interaction ability for robots, is inherently challenging as it requires multi-step metric-grounded reasoning compounded with complex spatial referring and real-world metric measurement. However, existing methods struggle with this compositional task. To this end, we propose RoboTracer, a 3D-aware VLM that first achieves both 3D spatial referring and measuring via a universal spatial encoder and a regression-supervised decoder to enhance scale awareness",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.13660v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-15-arxiv-robotracer_mastering_spatial_trace_with_reasoning_in_vision_language_models_for.yaml"
  },
  {
    "id": "2025-12-15-arxiv-world_models_can_leverage_human_videos_for_dexterous_manipulation",
    "date": "2025-12-15",
    "title": "World Models Can Leverage Human Videos for Dexterous Manipulation",
    "authors": "Raktim Gautam Goswami, Amir Bar, David Fan, Tsung-Yen Yang, Gaoyue Zhou",
    "abstract": "Dexterous manipulation is challenging because it requires understanding how subtle hand motion influences the environment through contact with objects. We introduce DexWM, a Dexterous Manipulation World Model that predicts the next latent state of the environment conditioned on past states and dexterous actions. To overcome the scarcity of dexterous manipulation datasets, DexWM is trained on over 900 hours of human and non-dexterous robot videos. To enable fine-grained dexterity, we find that pr",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.13644v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-15-arxiv-world_models_can_leverage_human_videos_for_dexterous_manipulation.yaml"
  },
  {
    "id": "2025-12-15-arxiv-jova_unified_multimodal_learning_for_joint_video_audio_generation",
    "date": "2025-12-15",
    "title": "JoVA: Unified Multimodal Learning for Joint Video-Audio Generation",
    "authors": "Xiaohu Huang, Hao Zhou, Qiangpeng Yang, Shilei Wen, Kai Han",
    "abstract": "In this paper, we present JoVA, a unified framework for joint video-audio generation. Despite recent encouraging advances, existing methods face two critical limitations. First, most existing approaches can only generate ambient sounds and lack the capability to produce human speech synchronized with lip movements. Second, recent attempts at unified human video-audio generation typically rely on explicit fusion or modality-specific alignment modules, which introduce additional architecture desig",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.13677v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-15-arxiv-jova_unified_multimodal_learning_for_joint_video_audio_generation.yaml"
  },
  {
    "id": "2025-12-15-arxiv-towards_effective_model_editing_for_llm_personalization",
    "date": "2025-12-15",
    "title": "Towards Effective Model Editing for LLM Personalization",
    "authors": "Baixiang Huang, Limeng Cui, Jiapeng Liu, Haoran Wang, Jiawei Xu",
    "abstract": "Personalization is becoming indispensable for LLMs to align with individual user preferences and needs. Yet current approaches are often computationally expensive, data-intensive, susceptible to catastrophic forgetting, and prone to performance degradation in multi-turn interactions or when handling implicit queries. To address these challenges, we conceptualize personalization as a model editing task and introduce Personalization Editing, a framework that applies localized edits guided by clust",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.13676v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-15-arxiv-towards_effective_model_editing_for_llm_personalization.yaml"
  },
  {
    "id": "2025-12-15-arxiv-recurrent_video_masked_autoencoders",
    "date": "2025-12-15",
    "title": "Recurrent Video Masked Autoencoders",
    "authors": "Daniel Zoran, Nikhil Parthasarathy, Yi Yang, Drew A Hudson, Joao Carreira",
    "abstract": "We present Recurrent Video Masked-Autoencoders (RVM): a novel video representation learning approach that uses a transformer-based recurrent neural network to aggregate dense image features over time, effectively capturing the spatio-temporal structure of natural video data. RVM learns via an asymmetric masked prediction task requiring only a standard pixel reconstruction objective. This design yields a highly efficient ``generalist'' encoder: RVM achieves competitive performance with state-of-t",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.13684v1"
      }
    ],
    "source_file": "papers/weekly/2025-W50/2025-12-15-arxiv-recurrent_video_masked_autoencoders.yaml"
  },
  {
    "id": "2025-12-12-arxiv-from_signal_to_turn_interactional_friction_in_modular_speech_to_speech_pipelines",
    "date": "2025-12-12",
    "title": "From Signal to Turn: Interactional Friction in Modular Speech-to-Speech Pipelines",
    "authors": "Titaya Mairittha, Tanakon Sawanglok, Panuwit Raden, Jirapast Buntub, Thanapat Warunee",
    "abstract": "While voice-based AI systems have achieved remarkable generative capabilities, their interactions often feel conversationally broken. This paper examines the interactional friction that emerges in modular Speech-to-Speech Retrieval-Augmented Generation (S2S-RAG) pipelines. By analyzing a representative production system, we move beyond simple latency metrics to identify three recurring patterns of conversational breakdown: (1) Temporal Misalignment, where system delays violate user expectations ",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11724v1"
      }
    ],
    "source_file": "papers/weekly/2025-12-12-arxiv-from_signal_to_turn_interactional_friction_in_modular_speech_to_speech_pipelines.yaml"
  },
  {
    "id": "2025-12-12-arxiv-cognisnn_enabling_neuron_expandability_pathway_reusability_and_dynamic_configura",
    "date": "2025-12-12",
    "title": "CogniSNN: Enabling Neuron-Expandability, Pathway-Reusability, and Dynamic-Configurability with Random Graph Architectures in Spiking Neural Networks",
    "authors": "Yongsheng Huang, Peibo Duan, Yujie Wu, Kai Sun, Zhipeng Liu",
    "abstract": "Spiking neural networks (SNNs), regarded as the third generation of artificial neural networks, are expected to bridge the gap between artificial intelligence and computational neuroscience. However, most mainstream SNN research directly adopts the rigid, chain-like hierarchical architecture of traditional artificial neural networks (ANNs), ignoring key structural characteristics of the brain. Biological neurons are stochastically interconnected, forming complex neural pathways that exhibit Neur",
    "category": "Hardware/Infra",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "hardware_infra"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11743v1"
      }
    ],
    "source_file": "papers/weekly/2025-12-12-arxiv-cognisnn_enabling_neuron_expandability_pathway_reusability_and_dynamic_configura.yaml"
  },
  {
    "id": "2025-12-12-arxiv-super_suffixes_bypassing_text_generation_alignment_and_guard_models_simultaneous",
    "date": "2025-12-12",
    "title": "Super Suffixes: Bypassing Text Generation Alignment and Guard Models Simultaneously",
    "authors": "Andrew Adiletta, Kathryn Adiletta, Kemal Derya, Berk Sunar",
    "abstract": "The rapid deployment of Large Language Models (LLMs) has created an urgent need for enhanced security and privacy measures in Machine Learning (ML). LLMs are increasingly being used to process untrusted text inputs and even generate executable code, often while having access to sensitive system controls. To address these security concerns, several companies have introduced guard models, which are smaller, specialized models designed to protect text generation models from adversarial or malicious",
    "category": "Incident",
    "source": "arxiv",
    "score": 95,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "incident"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11783v1"
      }
    ],
    "source_file": "papers/weekly/2025-12-12-arxiv-super_suffixes_bypassing_text_generation_alignment_and_guard_models_simultaneous.yaml"
  },
  {
    "id": "2025-12-12-arxiv-conditional_coverage_diagnostics_for_conformal_prediction",
    "date": "2025-12-12",
    "title": "Conditional Coverage Diagnostics for Conformal Prediction",
    "authors": "Sacha Braun, David Holzmüller, Michael I. Jordan, Francis Bach",
    "abstract": "Evaluating conditional coverage remains one of the most persistent challenges in assessing the reliability of predictive systems. Although conformal methods can give guarantees on marginal coverage, no method can guarantee to produce sets with correct conditional coverage, leaving practitioners without a clear way to interpret local deviations. To overcome sample-inefficiency and overfitting issues of existing metrics, we cast conditional coverage estimation as a classification problem. Conditio",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 95,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11779v1"
      }
    ],
    "source_file": "papers/weekly/2025-12-12-arxiv-conditional_coverage_diagnostics_for_conformal_prediction.yaml"
  },
  {
    "id": "2025-12-12-arxiv-agile_flight_emerges_from_multi_agent_competitive_racing",
    "date": "2025-12-12",
    "title": "Agile Flight Emerges from Multi-Agent Competitive Racing",
    "authors": "Vineet Pasumarti, Lorenzo Bianchi, Antonio Loquercio",
    "abstract": "Through multi-agent competition and the sparse high-level objective of winning a race, we find that both agile flight (e.g., high-speed motion pushing the platform to its physical limits) and strategy (e.g., overtaking or blocking) emerge from agents trained with reinforcement learning. We provide evidence in both simulation and the real world that this approach outperforms the common paradigm of training agents in isolation with rewards that prescribe behavior, e.g., progress on the raceline, i",
    "category": "Model",
    "source": "arxiv",
    "score": 95,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11781v1"
      }
    ],
    "source_file": "papers/weekly/2025-12-12-arxiv-agile_flight_emerges_from_multi_agent_competitive_racing.yaml"
  },
  {
    "id": "2025-12-12-arxiv-particulate_feed_forward_3d_object_articulation",
    "date": "2025-12-12",
    "title": "Particulate: Feed-Forward 3D Object Articulation",
    "authors": "Ruining Li, Yuxin Yao, Chuanxia Zheng, Christian Rupprecht, Joan Lasenby",
    "abstract": "We present Particulate, a feed-forward approach that, given a single static 3D mesh of an everyday object, directly infers all attributes of the underlying articulated structure, including its 3D parts, kinematic structure, and motion constraints. At its core is a transformer network, Part Articulation Transformer, which processes a point cloud of the input mesh using a flexible and scalable architecture to predict all the aforementioned attributes with native multi-joint support. We train the n",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11798v1"
      }
    ],
    "source_file": "papers/weekly/2025-12-12-arxiv-particulate_feed_forward_3d_object_articulation.yaml"
  },
  {
    "id": "2025-12-12-arxiv-generative_parametric_design_gpd_a_framework_for_real_time_geometry_generation_a",
    "date": "2025-12-12",
    "title": "Generative Parametric Design (GPD): A framework for real-time geometry generation and on-the-fly multiparametric approximation",
    "authors": "Mohammed El Fallaki Idrissi, Jad Mounayer, Sebastian Rodriguez, Fodil Meraghni, Francisco Chinesta",
    "abstract": "This paper presents a novel paradigm in simulation-based engineering sciences by introducing a new framework called Generative Parametric Design (GPD). The GPD framework enables the generation of new designs along with their corresponding parametric solutions given as a reduced basis. To achieve this, two Rank Reduction Autoencoders (RRAEs) are employed, one for encoding and generating the design or geometry, and the other for encoding the sparse Proper Generalized Decomposition (sPGD) mode solu",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11748v1"
      }
    ],
    "source_file": "papers/weekly/2025-12-12-arxiv-generative_parametric_design_gpd_a_framework_for_real_time_geometry_generation_a.yaml"
  },
  {
    "id": "2025-12-12-arxiv-medai_evaluating_txagent_s_therapeutic_agentic_reasoning_in_the_neurips_cure_ben",
    "date": "2025-12-12",
    "title": "MedAI: Evaluating TxAgent's Therapeutic Agentic Reasoning in the NeurIPS CURE-Bench Competition",
    "authors": "Tim Cofala, Christian Kalfar, Jingge Xiao, Johanna Schrader, Michelle Tang",
    "abstract": "Therapeutic decision-making in clinical medicine constitutes a high-stakes domain in which AI guidance interacts with complex interactions among patient characteristics, disease processes, and pharmacological agents. Tasks such as drug recommendation, treatment planning, and adverse-effect prediction demand robust, multi-step reasoning grounded in reliable biomedical knowledge. Agentic AI methods, exemplified by TxAgent, address these challenges through iterative retrieval-augmented generation (",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11682v1"
      }
    ],
    "source_file": "papers/weekly/2025-12-12-arxiv-medai_evaluating_txagent_s_therapeutic_agentic_reasoning_in_the_neurips_cure_ben.yaml"
  },
  {
    "id": "2025-12-12-arxiv-smudged_fingerprints_a_systematic_evaluation_of_the_robustness_of_ai_image_finge",
    "date": "2025-12-12",
    "title": "Smudged Fingerprints: A Systematic Evaluation of the Robustness of AI Image Fingerprints",
    "authors": "Kai Yao, Marc Juarez",
    "abstract": "Model fingerprint detection techniques have emerged as a promising approach for attributing AI-generated images to their source models, but their robustness under adversarial conditions remains largely unexplored. We present the first systematic security evaluation of these techniques, formalizing threat models that encompass both white- and black-box access and two attack goals: fingerprint removal, which erases identifying traces to evade attribution, and fingerprint forgery, which seeks to ca",
    "category": "Incident",
    "source": "arxiv",
    "score": 90,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "incident"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11771v1"
      }
    ],
    "source_file": "papers/weekly/2025-12-12-arxiv-smudged_fingerprints_a_systematic_evaluation_of_the_robustness_of_ai_image_finge.yaml"
  },
  {
    "id": "2025-12-12-arxiv-from_verification_burden_to_trusted_collaboration_design_goals_for_llm_assisted",
    "date": "2025-12-12",
    "title": "From Verification Burden to Trusted Collaboration: Design Goals for LLM-Assisted Literature Reviews",
    "authors": "Brenda Nogueira, Werner Geyer, Andrew Anderson, Toby Jia-Jun Li, Dongwhi Kim",
    "abstract": "Large Language Models (LLMs) are increasingly embedded in academic writing practices. Although numerous studies have explored how researchers employ these tools for scientific writing, their concrete implementation, limitations, and design challenges within the literature review process remain underexplored. In this paper, we report a user study with researchers across multiple disciplines to characterize current practices, benefits, and \\textit{pain points} in using LLMs to investigate related ",
    "category": "Project",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "project"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11661v1"
      }
    ],
    "source_file": "papers/weekly/2025-12-12-arxiv-from_verification_burden_to_trusted_collaboration_design_goals_for_llm_assisted.yaml"
  },
  {
    "id": "2025-12-12-arxiv-from_signal_to_turn_interactional_friction_in_modular_speech_to_speech_pipelines",
    "date": "2025-12-12",
    "title": "From Signal to Turn: Interactional Friction in Modular Speech-to-Speech Pipelines",
    "authors": "Titaya Mairittha, Tanakon Sawanglok, Panuwit Raden, Jirapast Buntub, Thanapat Warunee",
    "abstract": "While voice-based AI systems have achieved remarkable generative capabilities, their interactions often feel conversationally broken. This paper examines the interactional friction that emerges in modular Speech-to-Speech Retrieval-Augmented Generation (S2S-RAG) pipelines. By analyzing a representative production system, we move beyond simple latency metrics to identify three recurring patterns of conversational breakdown: (1) Temporal Misalignment, where system delays violate user expectations ",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11724v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-from_signal_to_turn_interactional_friction_in_modular_speech_to_speech_pipelines.yaml"
  },
  {
    "id": "2025-12-12-arxiv-cognisnn_enabling_neuron_expandability_pathway_reusability_and_dynamic_configura",
    "date": "2025-12-12",
    "title": "CogniSNN: Enabling Neuron-Expandability, Pathway-Reusability, and Dynamic-Configurability with Random Graph Architectures in Spiking Neural Networks",
    "authors": "Yongsheng Huang, Peibo Duan, Yujie Wu, Kai Sun, Zhipeng Liu",
    "abstract": "Spiking neural networks (SNNs), regarded as the third generation of artificial neural networks, are expected to bridge the gap between artificial intelligence and computational neuroscience. However, most mainstream SNN research directly adopts the rigid, chain-like hierarchical architecture of traditional artificial neural networks (ANNs), ignoring key structural characteristics of the brain. Biological neurons are stochastically interconnected, forming complex neural pathways that exhibit Neur",
    "category": "Hardware/Infra",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "hardware_infra"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11743v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-cognisnn_enabling_neuron_expandability_pathway_reusability_and_dynamic_configura.yaml"
  },
  {
    "id": "2025-12-12-arxiv-moment_based_3d_gaussian_splatting_resolving_volumetric_occlusion_with_order_ind",
    "date": "2025-12-12",
    "title": "Moment-Based 3D Gaussian Splatting: Resolving Volumetric Occlusion with Order-Independent Transmittance",
    "authors": "Jan U. Müller, Robin Tim Landsgesell, Leif Van Holland, Patrick Stotko, Reinhard Klein",
    "abstract": "The recent success of 3D Gaussian Splatting (3DGS) has reshaped novel view synthesis by enabling fast optimization and real-time rendering of high-quality radiance fields. However, it relies on simplified, order-dependent alpha blending and coarse approximations of the density integral within the rasterizer, thereby limiting its ability to render complex, overlapping semi-transparent objects. In this paper, we extend rasterization-based rendering of 3D Gaussian representations with a novel metho",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11800v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-moment_based_3d_gaussian_splatting_resolving_volumetric_occlusion_with_order_ind.yaml"
  },
  {
    "id": "2025-12-12-arxiv-anchordream_repurposing_video_diffusion_for_embodiment_aware_robot_data_synthesi",
    "date": "2025-12-12",
    "title": "AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis",
    "authors": "Junjie Ye, Rong Xue, Basile Van Hoorick, Pavel Tokmakov, Muhammad Zubair Irshad",
    "abstract": "The collection of large-scale and diverse robot demonstrations remains a major bottleneck for imitation learning, as real-world data acquisition is costly and simulators offer limited diversity and fidelity with pronounced sim-to-real gaps. While generative models present an attractive solution, existing methods often alter only visual appearances without creating new behaviors, or suffer from embodiment inconsistencies that yield implausible motions. To address these limitations, we introduce A",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11797v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-anchordream_repurposing_video_diffusion_for_embodiment_aware_robot_data_synthesi.yaml"
  },
  {
    "id": "2025-12-12-arxiv-reframing_music_driven_2d_dance_pose_generation_as_multi_channel_image_generatio",
    "date": "2025-12-12",
    "title": "Reframing Music-Driven 2D Dance Pose Generation as Multi-Channel Image Generation",
    "authors": "Yan Zhang, Han Zou, Lincong Feng, Cong Xie, Ruiqi Yu",
    "abstract": "Recent pose-to-video models can translate 2D pose sequences into photorealistic, identity-preserving dance videos, so the key challenge is to generate temporally coherent, rhythm-aligned 2D poses from music, especially under complex, high-variance in-the-wild distributions. We address this by reframing music-to-dance generation as a music-token-conditioned multi-channel image synthesis problem: 2D pose sequences are encoded as one-hot images, compressed by a pretrained image VAE, and modeled wit",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11720v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-reframing_music_driven_2d_dance_pose_generation_as_multi_channel_image_generatio.yaml"
  },
  {
    "id": "2025-12-12-arxiv-mvise_a_visual_search_engine_for_analyzing_multiplex_ihc_brain_tissue_images",
    "date": "2025-12-12",
    "title": "mViSE: A Visual Search Engine for Analyzing Multiplex IHC Brain Tissue Images",
    "authors": "Liqiang Huang, Rachel W. Mills, Saikiran Mandula, Lin Bai, Mahtab Jeyhani",
    "abstract": "Whole-slide multiplex imaging of brain tissue generates massive information-dense images that are challenging to analyze and require custom software. We present an alternative query-driven programming-free strategy using a multiplex visual search engine (mViSE) that learns the multifaceted brain tissue chemoarchitecture, cytoarchitecture, and myeloarchitecture. Our divide-and-conquer strategy organizes the data into panels of related molecular markers and uses self-supervised learning to train a",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11745v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-mvise_a_visual_search_engine_for_analyzing_multiplex_ihc_brain_tissue_images.yaml"
  },
  {
    "id": "2025-12-12-arxiv-super_suffixes_bypassing_text_generation_alignment_and_guard_models_simultaneous",
    "date": "2025-12-12",
    "title": "Super Suffixes: Bypassing Text Generation Alignment and Guard Models Simultaneously",
    "authors": "Andrew Adiletta, Kathryn Adiletta, Kemal Derya, Berk Sunar",
    "abstract": "The rapid deployment of Large Language Models (LLMs) has created an urgent need for enhanced security and privacy measures in Machine Learning (ML). LLMs are increasingly being used to process untrusted text inputs and even generate executable code, often while having access to sensitive system controls. To address these security concerns, several companies have introduced guard models, which are smaller, specialized models designed to protect text generation models from adversarial or malicious",
    "category": "Incident",
    "source": "arxiv",
    "score": 95,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "incident"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11783v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-super_suffixes_bypassing_text_generation_alignment_and_guard_models_simultaneous.yaml"
  },
  {
    "id": "2025-12-12-arxiv-editmgt_unleashing_potentials_of_masked_generative_transformers_in_image_editing",
    "date": "2025-12-12",
    "title": "EditMGT: Unleashing Potentials of Masked Generative Transformers in Image Editing",
    "authors": "Wei Chow, Linfeng Li, Lingdong Kong, Zefeng Li, Qi Xu",
    "abstract": "Recent advances in diffusion models (DMs) have achieved exceptional visual quality in image editing tasks. However, the global denoising dynamics of DMs inherently conflate local editing targets with the full-image context, leading to unintended modifications in non-target regions. In this paper, we shift our attention beyond DMs and turn to Masked Generative Transformers (MGTs) as an alternative approach to tackle this challenge. By predicting multiple masked tokens rather than holistic refinem",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11715v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-editmgt_unleashing_potentials_of_masked_generative_transformers_in_image_editing.yaml"
  },
  {
    "id": "2025-12-12-nature-deciphering_rna_ligand_binding_specificity_with_gerna_bind",
    "date": "2025-12-12",
    "title": "Deciphering RNA–ligand binding specificity with GerNA-Bind",
    "authors": "Yunpeng Xia, Jiayi Li, Yi-Ting Chu, Jiahua Rao, Jing Chen",
    "abstract": "<p>Nature Machine Intelligence, Published online: 12 December 2025; <a href=\"https://www.nature.com/articles/s42256-025-01154-z\">doi:10.1038/s42256-025-01154-z</a></p>Xia et al. introduce GerNA-Bind, a geometric deep learning framework designed to predict RNA–ligand binding specificity by integrating multistate RNA–ligand interactions.",
    "category": "Model",
    "source": "nature",
    "score": 115,
    "tags": [
      "ai",
      "research",
      "nature",
      "model"
    ],
    "links": [
      {
        "title": "View Paper",
        "url": "https://www.nature.com/articles/s42256-025-01154-z"
      },
      {
        "title": "DOI",
        "url": "https://doi.org/10.1038/s42256-025-01154-z"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-nature-deciphering_rna_ligand_binding_specificity_with_gerna_bind.yaml"
  },
  {
    "id": "2025-12-12-arxiv-conditional_coverage_diagnostics_for_conformal_prediction",
    "date": "2025-12-12",
    "title": "Conditional Coverage Diagnostics for Conformal Prediction",
    "authors": "Sacha Braun, David Holzmüller, Michael I. Jordan, Francis Bach",
    "abstract": "Evaluating conditional coverage remains one of the most persistent challenges in assessing the reliability of predictive systems. Although conformal methods can give guarantees on marginal coverage, no method can guarantee to produce sets with correct conditional coverage, leaving practitioners without a clear way to interpret local deviations. To overcome sample-inefficiency and overfitting issues of existing metrics, we cast conditional coverage estimation as a classification problem. Conditio",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 95,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11779v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-conditional_coverage_diagnostics_for_conformal_prediction.yaml"
  },
  {
    "id": "2025-12-12-nature-llm_use_in_scholarly_writing_poses_a_provenance_problem",
    "date": "2025-12-12",
    "title": "LLM use in scholarly writing poses a provenance problem",
    "authors": "Brian D. Earp, Haotian Yuan, Julian Koplin, Sebastian Porsdam Mann",
    "abstract": "<p>Nature Machine Intelligence, Published online: 12 December 2025; <a href=\"https://www.nature.com/articles/s42256-025-01159-8\">doi:10.1038/s42256-025-01159-8</a></p>LLM use in scholarly writing poses a provenance problem",
    "category": "Model",
    "source": "nature",
    "score": 110,
    "tags": [
      "ai",
      "research",
      "nature",
      "model"
    ],
    "links": [
      {
        "title": "View Paper",
        "url": "https://www.nature.com/articles/s42256-025-01159-8"
      },
      {
        "title": "DOI",
        "url": "https://doi.org/10.1038/s42256-025-01159-8"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-nature-llm_use_in_scholarly_writing_poses_a_provenance_problem.yaml"
  },
  {
    "id": "2025-12-12-arxiv-agile_flight_emerges_from_multi_agent_competitive_racing",
    "date": "2025-12-12",
    "title": "Agile Flight Emerges from Multi-Agent Competitive Racing",
    "authors": "Vineet Pasumarti, Lorenzo Bianchi, Antonio Loquercio",
    "abstract": "Through multi-agent competition and the sparse high-level objective of winning a race, we find that both agile flight (e.g., high-speed motion pushing the platform to its physical limits) and strategy (e.g., overtaking or blocking) emerge from agents trained with reinforcement learning. We provide evidence in both simulation and the real world that this approach outperforms the common paradigm of training agents in isolation with rewards that prescribe behavior, e.g., progress on the raceline, i",
    "category": "Model",
    "source": "arxiv",
    "score": 95,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11781v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-agile_flight_emerges_from_multi_agent_competitive_racing.yaml"
  },
  {
    "id": "2025-12-12-arxiv-particulate_feed_forward_3d_object_articulation",
    "date": "2025-12-12",
    "title": "Particulate: Feed-Forward 3D Object Articulation",
    "authors": "Ruining Li, Yuxin Yao, Chuanxia Zheng, Christian Rupprecht, Joan Lasenby",
    "abstract": "We present Particulate, a feed-forward approach that, given a single static 3D mesh of an everyday object, directly infers all attributes of the underlying articulated structure, including its 3D parts, kinematic structure, and motion constraints. At its core is a transformer network, Part Articulation Transformer, which processes a point cloud of the input mesh using a flexible and scalable architecture to predict all the aforementioned attributes with native multi-joint support. We train the n",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11798v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-particulate_feed_forward_3d_object_articulation.yaml"
  },
  {
    "id": "2025-12-12-arxiv-generative_parametric_design_gpd_a_framework_for_real_time_geometry_generation_a",
    "date": "2025-12-12",
    "title": "Generative Parametric Design (GPD): A framework for real-time geometry generation and on-the-fly multiparametric approximation",
    "authors": "Mohammed El Fallaki Idrissi, Jad Mounayer, Sebastian Rodriguez, Fodil Meraghni, Francisco Chinesta",
    "abstract": "This paper presents a novel paradigm in simulation-based engineering sciences by introducing a new framework called Generative Parametric Design (GPD). The GPD framework enables the generation of new designs along with their corresponding parametric solutions given as a reduced basis. To achieve this, two Rank Reduction Autoencoders (RRAEs) are employed, one for encoding and generating the design or geometry, and the other for encoding the sparse Proper Generalized Decomposition (sPGD) mode solu",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11748v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-generative_parametric_design_gpd_a_framework_for_real_time_geometry_generation_a.yaml"
  },
  {
    "id": "2025-12-12-arxiv-uncertainty_aware_domain_adaptation_for_vitiligo_segmentation_in_clinical_photog",
    "date": "2025-12-12",
    "title": "Uncertainty-Aware Domain Adaptation for Vitiligo Segmentation in Clinical Photographs",
    "authors": "Wentao Jiang, Vamsi Varra, Caitlin Perez-Stable, Harrison Zhu, Meredith Apicella",
    "abstract": "Accurately quantifying vitiligo extent in routine clinical photographs is crucial for longitudinal monitoring of treatment response. We propose a trustworthy, frequency-aware segmentation framework built on three synergistic pillars: (1) a data-efficient training strategy combining domain-adaptive pre-training on the ISIC 2019 dataset with an ROI-constrained dual-task loss to suppress background noise; (2) an architectural refinement via a ConvNeXt V2-based encoder enhanced with a novel High-Fre",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11791v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-uncertainty_aware_domain_adaptation_for_vitiligo_segmentation_in_clinical_photog.yaml"
  },
  {
    "id": "2025-12-12-arxiv-medai_evaluating_txagent_s_therapeutic_agentic_reasoning_in_the_neurips_cure_ben",
    "date": "2025-12-12",
    "title": "MedAI: Evaluating TxAgent's Therapeutic Agentic Reasoning in the NeurIPS CURE-Bench Competition",
    "authors": "Tim Cofala, Christian Kalfar, Jingge Xiao, Johanna Schrader, Michelle Tang",
    "abstract": "Therapeutic decision-making in clinical medicine constitutes a high-stakes domain in which AI guidance interacts with complex interactions among patient characteristics, disease processes, and pharmacological agents. Tasks such as drug recommendation, treatment planning, and adverse-effect prediction demand robust, multi-step reasoning grounded in reliable biomedical knowledge. Agentic AI methods, exemplified by TxAgent, address these challenges through iterative retrieval-augmented generation (",
    "category": "Evaluation",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "evaluation"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11682v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-medai_evaluating_txagent_s_therapeutic_agentic_reasoning_in_the_neurips_cure_ben.yaml"
  },
  {
    "id": "2025-12-12-arxiv-v_rgbx_video_editing_with_accurate_controls_over_intrinsic_properties",
    "date": "2025-12-12",
    "title": "V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties",
    "authors": "Ye Fang, Tong Wu, Valentin Deschaintre, Duygu Ceylan, Iliyan Georgiev",
    "abstract": "Large-scale video generation models have shown remarkable potential in modeling photorealistic appearance and lighting interactions in real-world scenes. However, a closed-loop framework that jointly understands intrinsic scene properties (e.g., albedo, normal, material, and irradiance), leverages them for video synthesis, and supports editable intrinsic representations remains unexplored. We present V-RGBX, the first end-to-end framework for intrinsic-aware video editing. V-RGBX unifies three k",
    "category": "Industry",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "industry"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11799v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-v_rgbx_video_editing_with_accurate_controls_over_intrinsic_properties.yaml"
  },
  {
    "id": "2025-12-12-arxiv-structure_from_tracking_distilling_structure_preserving_motion_for_video_generat",
    "date": "2025-12-12",
    "title": "Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation",
    "authors": "Yang Fei, George Stoica, Jingyuan Liu, Qifeng Chen, Ranjay Krishna",
    "abstract": "Reality is a dance between rigid constraints and deformable structures. For video models, that means generating motion that preserves fidelity as well as structure. Despite progress in diffusion models, producing realistic structure-preserving motion remains challenging, especially for articulated and deformable objects such as humans and animals. Scaling training data alone, so far, has failed to resolve physically implausible transitions. Existing approaches rely on conditioning with noisy mot",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11792v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-structure_from_tracking_distilling_structure_preserving_motion_for_video_generat.yaml"
  },
  {
    "id": "2025-12-12-arxiv-weak_to_strong_generalization_enables_fully_automated_de_novo_training_of_multi",
    "date": "2025-12-12",
    "title": "Weak-to-Strong Generalization Enables Fully Automated De Novo Training of Multi-head Mask-RCNN Model for Segmenting Densely Overlapping Cell Nuclei in Multiplex Whole-slice Brain Images",
    "authors": "Lin Bai, Xiaoyang Li, Liqiang Huang, Quynh Nguyen, Hien Van Nguyen",
    "abstract": "We present a weak to strong generalization methodology for fully automated training of a multi-head extension of the Mask-RCNN method with efficient channel attention for reliable segmentation of overlapping cell nuclei in multiplex cyclic immunofluorescent (IF) whole-slide images (WSI), and present evidence for pseudo-label correction and coverage expansion, the key phenomena underlying weak to strong generalization. This method can learn to segment de novo a new class of images from a new inst",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11722v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-weak_to_strong_generalization_enables_fully_automated_de_novo_training_of_multi.yaml"
  },
  {
    "id": "2025-12-12-arxiv-svg_t2i_scaling_up_text_to_image_latent_diffusion_model_without_variational_auto",
    "date": "2025-12-12",
    "title": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
    "authors": "Minglei Shi, Haolin Wang, Borui Zhang, Wenzhao Zheng, Bohan Zeng",
    "abstract": "Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthes",
    "category": "Model",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "model"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11749v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-svg_t2i_scaling_up_text_to_image_latent_diffusion_model_without_variational_auto.yaml"
  },
  {
    "id": "2025-12-12-arxiv-smudged_fingerprints_a_systematic_evaluation_of_the_robustness_of_ai_image_finge",
    "date": "2025-12-12",
    "title": "Smudged Fingerprints: A Systematic Evaluation of the Robustness of AI Image Fingerprints",
    "authors": "Kai Yao, Marc Juarez",
    "abstract": "Model fingerprint detection techniques have emerged as a promising approach for attributing AI-generated images to their source models, but their robustness under adversarial conditions remains largely unexplored. We present the first systematic security evaluation of these techniques, formalizing threat models that encompass both white- and black-box access and two attack goals: fingerprint removal, which erases identifying traces to evade attribution, and fingerprint forgery, which seeks to ca",
    "category": "Incident",
    "source": "arxiv",
    "score": 90,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "incident"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11771v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-smudged_fingerprints_a_systematic_evaluation_of_the_robustness_of_ai_image_finge.yaml"
  },
  {
    "id": "2025-12-12-arxiv-from_verification_burden_to_trusted_collaboration_design_goals_for_llm_assisted",
    "date": "2025-12-12",
    "title": "From Verification Burden to Trusted Collaboration: Design Goals for LLM-Assisted Literature Reviews",
    "authors": "Brenda Nogueira, Werner Geyer, Andrew Anderson, Toby Jia-Jun Li, Dongwhi Kim",
    "abstract": "Large Language Models (LLMs) are increasingly embedded in academic writing practices. Although numerous studies have explored how researchers employ these tools for scientific writing, their concrete implementation, limitations, and design challenges within the literature review process remain underexplored. In this paper, we report a user study with researchers across multiple disciplines to characterize current practices, benefits, and \\textit{pain points} in using LLMs to investigate related ",
    "category": "Project",
    "source": "arxiv",
    "score": 100,
    "tags": [
      "ai",
      "research",
      "arxiv",
      "project"
    ],
    "links": [
      {
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.11661v1"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-12-arxiv-from_verification_burden_to_trusted_collaboration_design_goals_for_llm_assisted.yaml"
  },
  {
    "id": "2025-12-10-nature-a_multimodal_cell_free_rna_language_model_for_liquid_biopsy_applications",
    "date": "2025-12-10",
    "title": "A multimodal cell-free RNA language model for liquid biopsy applications",
    "authors": "Mehran Karimzadeh, Aiden M. Sababi, Amir Momen-Roknabadi, Nae-Chyun Chen, Taylor B. Cavazos",
    "abstract": "<p>Nature Machine Intelligence, Published online: 10 December 2025; <a href=\"https://www.nature.com/articles/s42256-025-01148-x\">doi:10.1038/s42256-025-01148-x</a></p>Exai-1, a cell-free RNA foundation model that integrates sequence, structure and expression features, advances liquid biopsy diagnostics by denoising noisy data, augmenting limited datasets and improving the generalizability of cancer detection models.",
    "category": "Model",
    "source": "nature",
    "score": 115,
    "tags": [
      "ai",
      "research",
      "nature",
      "model"
    ],
    "links": [
      {
        "title": "View Paper",
        "url": "https://www.nature.com/articles/s42256-025-01148-x"
      },
      {
        "title": "DOI",
        "url": "https://doi.org/10.1038/s42256-025-01148-x"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-10-nature-a_multimodal_cell_free_rna_language_model_for_liquid_biopsy_applications.yaml"
  },
  {
    "id": "2025-12-09-nature-actor_critic_networks_with_analogue_memristors_mimicking_reward_based_learning",
    "date": "2025-12-09",
    "title": "Actor–critic networks with analogue memristors mimicking reward-based learning",
    "authors": "Kevin Portner, Till Zellweger, Flavio Martinelli, Laura Bégon-Lours, Valeria Bragaglia",
    "abstract": "<p>Nature Machine Intelligence, Published online: 09 December 2025; <a href=\"https://www.nature.com/articles/s42256-025-01149-w\">doi:10.1038/s42256-025-01149-w</a></p>A framework based on actor–critic temporal difference learning and employing a biologically plausible network architecture that mimics reward-based learning on memristors and enables full in-memory training for navigation tasks is discussed.",
    "category": "Model",
    "source": "nature",
    "score": 115,
    "tags": [
      "ai",
      "research",
      "nature",
      "model"
    ],
    "links": [
      {
        "title": "View Paper",
        "url": "https://www.nature.com/articles/s42256-025-01149-w"
      },
      {
        "title": "DOI",
        "url": "https://doi.org/10.1038/s42256-025-01149-w"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-09-nature-actor_critic_networks_with_analogue_memristors_mimicking_reward_based_learning.yaml"
  },
  {
    "id": "2025-12-09-nature-fully_analogue_reinforcement_learning_with_memristors",
    "date": "2025-12-09",
    "title": "Fully analogue reinforcement learning with memristors",
    "authors": "Yue Zhang, Xiaojuan Qi, Zhongrui Wang",
    "abstract": "<p>Nature Machine Intelligence, Published online: 09 December 2025; <a href=\"https://www.nature.com/articles/s42256-025-01157-w\">doi:10.1038/s42256-025-01157-w</a></p>Reinforcement learning has a key role in artifical intelligence (AI), but its implementation on neuromorphic hardware typically involves operations executed on conventional digital computers. A study now addresses this issue by implementing an actor–critic network fully in hardware using analogue memristors.",
    "category": "Model",
    "source": "nature",
    "score": 110,
    "tags": [
      "ai",
      "research",
      "nature",
      "model"
    ],
    "links": [
      {
        "title": "View Paper",
        "url": "https://www.nature.com/articles/s42256-025-01157-w"
      },
      {
        "title": "DOI",
        "url": "https://doi.org/10.1038/s42256-025-01157-w"
      }
    ],
    "source_file": "papers/weekly/2025-W49/2025-12-09-nature-fully_analogue_reinforcement_learning_with_memristors.yaml"
  }
]