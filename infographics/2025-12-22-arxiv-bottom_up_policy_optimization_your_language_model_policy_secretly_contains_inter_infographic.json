{
  "key_insight": "Large language models do not act as a single monolithic policy; their transformer residual stream and hidden-state composition reveal multiple internal, layer- or module-level policies that can be identified and optimized separately.",
  "problem_solved": "Current RL approaches treat an LLM as one unified policy, which hides how decisions form across layers and modules and prevents precise, targeted policy optimization and interpretability.",
  "method": [
    "Decompose the LLM policy using the intrinsic split of the Transformer residual stream to isolate per-layer/module policy contributions.",
    "Exploit equivalences in hidden-state composition to reconstruct and evaluate these internal policies bottom-up.",
    "Use the decomposed internal policies to drive targeted optimization and analysis instead of optimizing the whole model as a single black box."
  ],
  "impact": "This enables more precise RL-style updates, faster troubleshooting, and finer-grained interpretability for LLM behavior, letting practitioners optimize or constrain specific internal decision-makers without retraining entire models.",
  "visual_elements": [
    "Layered Transformer diagram highlighting residual stream splits and arrows showing separate internal policies per layer/module",
    "Flowchart showing bottom-up extraction: hidden states -> internal policy reconstruction -> targeted optimization",
    "Before/after bar chart comparing global vs. decomposed optimization efficiency or interpretability metrics",
    "Icon set: magnifying glass (analysis), gear (optimization), shield (safety/control)"
  ],
  "hashtags": [
    "#LLM",
    "#ReinforcementLearning",
    "#Interpretability",
    "#Transformers",
    "#PolicyOptimization"
  ],
  "paper_id": "2025-12-22-arxiv-bottom_up_policy_optimization_your_language_model_policy_secretly_contains_inter",
  "paper_title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies",
  "paper_category": "Model",
  "paper_date": "2025-12-22",
  "paper_authors": "Yuqiao Tan, Minzheng Wang, Shizhu He, Huanxuan Liao, Chengfeng Zhao",
  "paper_links": [
    {
      "title": "PDF",
      "url": "https://arxiv.org/pdf/2512.19673v1"
    }
  ],
  "generated_at": "2025-12-24T06:25:10.293638"
}