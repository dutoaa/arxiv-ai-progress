{
  "key_insight": "State-of-the-art vision-language models can reliably replicate binary human preference judgments and serve as perceptual judges, enabling image-compression systems to be aligned to human perception without large-scale costly psychophysical datasets.",
  "problem_solved": "Conventional distortion metrics (e.g., MSE) and earlier perceptual losses are poorly aligned with human judgments; collecting and calibrating large human preference datasets for perceptual losses is slow and expensive.",
  "method": [
    "Use pretrained vision-language models to predict pairwise human preferences between compressed image variants, effectively acting as a perceptual judge.",
    "Convert VLM outputs into a differentiable or ranking loss/signal (or use them as an evaluation metric) to steer compression model training toward human-aligned quality.",
    "Optionally calibrate or fine-tune the VLM on a small set of human judgments to improve agreement and then integrate it into the compression training/evaluation pipeline."
  ],
  "impact": "Provides a scalable, cheaper route to train and evaluate image compression models that better match human perceptual preferences, reducing reliance on extensive human studies and improving subjective image quality in practice.",
  "visual_elements": [
    "Pipeline diagram: original image → compressor → two compressed variants → VLM judge vs. human vote (showing alignment)",
    "Bar/line chart: correlation or agreement between MSE, prior perceptual losses, and VLM judgments with human preferences",
    "Before/after image grid: examples where VLM-aligned compression preserves perceptual quality better than MSE-optimized compression",
    "Icon set: stopwatch/coin representing reduced time/cost for human studies, and a judge/gavel icon representing VLM as perceptual judge"
  ],
  "hashtags": [
    "#VisionLanguageModels",
    "#ImageCompression",
    "#PerceptualLoss",
    "#HumanCenteredAI",
    "#VLIC"
  ],
  "paper_id": "2025-12-17-arxiv-vlic_vision_language_models_as_perceptual_judges_for_human_aligned_image_compres",
  "paper_title": "VLIC: Vision-Language Models As Perceptual Judges for Human-Aligned Image Compression",
  "paper_category": "Model",
  "paper_date": "2025-12-17",
  "paper_authors": "Kyle Sargent, Ruiqi Gao, Philipp Henzler, Charles Herrmann, Aleksander Holynski",
  "paper_links": [
    {
      "title": "PDF",
      "url": "https://arxiv.org/pdf/2512.15701v1"
    }
  ],
  "generated_at": "2025-12-22T06:25:19.191094"
}