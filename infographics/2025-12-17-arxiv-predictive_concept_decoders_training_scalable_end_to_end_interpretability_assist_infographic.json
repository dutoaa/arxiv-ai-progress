{
  "key_insight": "Train end-to-end \"interpretability assistants\" (Predictive Concept Decoders) that learn to predict a model's behavior from its internal activations, replacing hand-designed hypothesis testers with scalable, learned probes that produce more faithful explanations.",
  "problem_solved": "Bridges the gap between complex internal activation space and external model behavior by providing a scalable, trainable way to extract human-interpretable concepts and predictions from hidden activations—avoiding ad-hoc, hand-crafted interpretability agents.",
  "method": [
    "Define predictive decoders that map internal activations to model behavior or human-interpretable concepts and train them end-to-end with an objective that rewards accurate prediction of the target model's outputs.",
    "Apply the decoders across layers and units to discover concept-aligned directions in activation space, allowing systematic testing and aggregation of explanations instead of manual hypothesis testing.",
    "Evaluate learned assistants on fidelity (how well they predict behavior), scalability across models/layers, and usefulness for downstream interpretability tasks (e.g., debugging or concept discovery)."
  ],
  "impact": "Provides practitioners a scalable, faithfulness-focused interpretability toolset that can speed up debugging, model analysis, and alignment work by turning opaque activations into actionable, testable concepts across large models.",
  "visual_elements": [
    "Diagram: model forward pass with a plug-in predictive concept decoder that maps activations to predicted outputs/concepts",
    "Bar/line chart: fidelity and scalability comparison between hand-designed agents vs learned decoders across layers or model sizes",
    "Layer-wise heatmap or t-SNE/UMAP projection: discovered concept directions and their activation strengths across inputs",
    "Flowchart: training loop for the interpretability assistant (collect activations → train decoder → evaluate predictions → refine)"
  ],
  "hashtags": [
    "#Interpretability",
    "#ExplainableAI",
    "#ConceptDecoders",
    "#ScalableAI",
    "#ModelAnalysis"
  ],
  "paper_id": "2025-12-17-arxiv-predictive_concept_decoders_training_scalable_end_to_end_interpretability_assist",
  "paper_title": "Predictive Concept Decoders: Training Scalable End-to-End Interpretability Assistants",
  "paper_category": "Model",
  "paper_date": "2025-12-17",
  "paper_authors": "Vincent Huang, Dami Choi, Daniel D. Johnson, Sarah Schwettmann, Jacob Steinhardt",
  "paper_links": [
    {
      "title": "PDF",
      "url": "https://arxiv.org/pdf/2512.15712v1"
    }
  ],
  "generated_at": "2025-12-18T23:57:12.372610"
}