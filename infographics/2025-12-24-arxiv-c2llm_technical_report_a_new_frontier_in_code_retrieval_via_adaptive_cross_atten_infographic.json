{
  "key_insight": "C2LLM introduces a Pooling by Multihead Attention (PMA) module that converts token-level causal representations into a single, information-rich sequence embedding, removing the EOS bottleneck in code retrieval. The approach yields compact contrastive code embeddings in 0.5B and 7B variants built on Qwen-2.5-Coder.",
  "problem_solved": "Standard EOS or simple pooling loses important token-level information from causal LLM representations, limiting retrieval quality; C2LLM enables aggregation from all tokens so embeddings better capture whole-sequence semantics for code search.",
  "method": [
    "Introduce a PMA module that uses multihead attention to pool token embeddings into a fixed-length sequence embedding while leveraging the LLM's causal representations.",
    "Train embeddings contrastively so similar code snippets are pulled together and dissimilar ones pushed apart for effective retrieval.",
    "Release two model sizes (0.5B and 7B) based on Qwen-2.5-Coder backbones to balance efficiency and performance."
  ],
  "impact": "Provides practitioners with higher-fidelity, retrieval-ready code embeddings that improve code search and downstream tooling without discarding causal token information—scalable options for both lightweight and higher-capacity deployments.",
  "visual_elements": [
    "Architecture flow: tokens → LLM causal representations → PMA multihead attention → sequence embedding",
    "Bar/line chart: retrieval accuracy (or MRR) comparing PMA pooling vs EOS/simple pooling",
    "Attention heatmap: PMA weights across tokens to show where pooling focuses for different code examples",
    "Model-size infographic: icons comparing 0.5B vs 7B trade-offs (latency, memory, accuracy)"
  ],
  "hashtags": [
    "#CodeRetrieval",
    "#ContrastiveLearning",
    "#CodeEmbeddings",
    "#LargeLanguageModels",
    "#AttentionPooling"
  ],
  "paper_id": "2025-12-24-arxiv-c2llm_technical_report_a_new_frontier_in_code_retrieval_via_adaptive_cross_atten",
  "paper_title": "C2LLM Technical Report: A New Frontier in Code Retrieval via Adaptive Cross-Attention Pooling",
  "paper_category": "Model",
  "paper_date": "2025-12-24",
  "paper_authors": "Jin Qin, Zihan Liao, Ziyin Zhang, Hang Yu, Peng Di",
  "paper_links": [
    {
      "title": "PDF",
      "url": "https://arxiv.org/pdf/2512.21332v1"
    }
  ],
  "generated_at": "2025-12-26T06:22:46.877699"
}