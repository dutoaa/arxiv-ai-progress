{
  "key_insight": "Use per-layer KL divergence between teacher and student attention distributions to automatically pick which Transformer layers can be converted from softmax to linear attention, enabling compact hybrid attention models that preserve performance while improving inference efficiency.",
  "problem_solved": "How to efficiently distill pretrained softmax-attention Transformers into hybrid models (interleaving softmax and linear attention) by deciding which layers to convert without expensive trial-and-error or full retraining.",
  "method": [
    "Compute a layerwise importance score using KL divergence between teacher and student attention outputs to measure sensitivity to conversion.",
    "Rank layers by KL score and convert low-sensitivity layers to linear attention to form a hybrid softmax/linear architecture.",
    "Distill the hybrid model from the pretrained teacher using standard KD losses, keeping high-sensitivity layers as softmax attention to preserve accuracy."
  ],
  "impact": "Provides a simple, scalable recipe to create faster, memory-light LLM variants with minimal accuracy loss, letting practitioners trade compute for latency efficiently without pretraining from scratch.",
  "visual_elements": [
    "Schematic of a Transformer stack showing selected layers converted to linear attention (color-coded softmax vs linear).",
    "Bar chart of per-layer KL divergence used for ranking and selection (high to low).",
    "Line/curve showing accuracy vs. inference speed (or FLOPs) for baseline, hybrid variants, and full linear/softmax extremes.",
    "Side-by-side attention-map heatmaps comparing teacher softmax attention and student linear/converted layers."
  ],
  "hashtags": [
    "#ModelCompression",
    "#KnowledgeDistillation",
    "#EfficientTransformers",
    "#HybridAttention",
    "#LLMEfficiency"
  ],
  "paper_id": "2025-12-23-arxiv-distilling_to_hybrid_attention_models_via_kl_guided_layer_selection",
  "paper_title": "Distilling to Hybrid Attention Models via KL-Guided Layer Selection",
  "paper_category": "Model",
  "paper_date": "2025-12-23",
  "paper_authors": "Yanhong Li, Songlin Yang, Shawn Tan, Mayank Mishra, Rameswar Panda",
  "paper_links": [
    {
      "title": "PDF",
      "url": "https://arxiv.org/pdf/2512.20569v1"
    }
  ],
  "generated_at": "2025-12-27T06:23:40.773727"
}