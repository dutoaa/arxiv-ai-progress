{
  "key_insight": "Introducing closed-loop world modeling enables video avatars to act with genuine agency: they can perceive, plan, and adapt over long horizons to pursue goals in stochastic visual environments. The paper provides a benchmark (L-IVA) and a practical architecture (ORCA) to evaluate and enable this capability.",
  "problem_solved": "Current video-avatar systems are reactive and identity-preserving but lack the ability to autonomously plan and adapt to achieve long-term goals in dynamic, uncertain environments.",
  "method": [
    "L-IVA benchmark: a long-horizon interactive visual-avatar task suite to evaluate goal-directed planning and robustness under stochastic generative dynamics.",
    "ORCA framework: an online reasoning + cognitive architecture that integrates a closed-loop world model (predictive perception), goal-conditioned planner, and action synthesis to iteratively interact with the environment.",
    "Evaluation: quantitative metrics for goal success, temporal consistency, and adaptability, plus qualitative video rollouts to validate long-horizon behavior."
  ],
  "impact": "Provides the first practical pathway and standardized benchmark for building and evaluating avatars that exhibit autonomous, goal-directed behavior—advancing research in interactive agents, virtual assistants, and embodied AI. Practitioners gain tools to measure and train long-horizon decision-making in generative visual systems.",
  "visual_elements": [
    "Closed-loop system diagram: Perception → World Model → Planner → Action → Environment → Perception (loop highlighted).",
    "Benchmark timeline: side-by-side timelines comparing reactive avatars versus ORCA-driven long-horizon goal pursuit.",
    "Performance chart: bar or radar plot showing metrics (goal success rate, adaptability, identity fidelity, temporal consistency) across methods.",
    "Example storyboard: sequential video frames showing an avatar detecting changes, planning, and successfully completing a goal under uncertainty."
  ],
  "hashtags": [
    "#VideoAvatars",
    "#ActiveAI",
    "#WorldModeling",
    "#LongHorizonPlanning",
    "#InteractiveAgents"
  ],
  "paper_id": "2025-12-23-arxiv-active_intelligence_in_video_avatars_via_closed_loop_world_modeling",
  "paper_title": "Active Intelligence in Video Avatars via Closed-loop World Modeling",
  "paper_category": "Model",
  "paper_date": "2025-12-23",
  "paper_authors": "Xuanhua He, Tianyu Yang, Ke Cao, Ruiqi Wu, Cheng Meng",
  "paper_links": [
    {
      "title": "PDF",
      "url": "https://arxiv.org/pdf/2512.20615v1"
    }
  ],
  "generated_at": "2025-12-24T06:22:40.958774"
}