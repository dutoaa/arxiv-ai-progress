{
  "key_insight": "Video diffusion models already internalize optical cues of transparency (refraction, reflection, transmission); repurposing their learned temporal and photometric knowledge enables accurate depth and normal estimation for transparent objects.",
  "problem_solved": "Transparent and reflective objects break assumptions of stereo, ToF and discriminative monocular methods, producing holes and temporally unstable depth/normal estimates; this work recovers reliable geometry for such challenging materials.",
  "method": [
    "Construct TransPhy3D: a large synthetic video corpus of transparent/reflective scenes (11k+ sequences) to expose models to diverse transparent phenomena.",
    "Leverage modern video diffusion models' internal representations of transparency — extract or finetune their temporal/photometric priors to predict per-frame depth and surface normals.",
    "Train a downstream estimator that combines diffusion-derived cues with geometric priors to produce dense, temporally stable depth and normal maps for transparent objects."
  ],
  "impact": "Provides a practical route to robust geometry estimation for transparent materials using existing generative models and synthetic data, improving perception reliability for robotics, AR/VR, and 3D reconstruction.",
  "visual_elements": [
    "Before/after comparison: RGB frame, baseline depth (holes/noisy), TransPhy3D depth & normals (dense, stable) side-by-side.",
    "Pipeline diagram: Video diffusion model → feature extraction → geometry estimator → depth & normal outputs.",
    "Dataset snapshot: montage of synthetic scenes showing refraction/reflection variations with a small bar chart listing 11k+ sequences and diversity attributes.",
    "Temporal stability plot: per-frame depth error over a sequence comparing baseline vs. TransPhy3D-based method."
  ],
  "hashtags": [
    "#TransparentObjects",
    "#DepthEstimation",
    "#VideoDiffusion",
    "#SyntheticData",
    "#3DComputerVision"
  ],
  "paper_id": "2025-12-29-arxiv-diffusion_knows_transparency_repurposing_video_diffusion_for_transparent_object",
  "paper_title": "Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation",
  "paper_category": "Model",
  "paper_date": "2025-12-29",
  "paper_authors": "Shaocong Xu, Songlin Wei, Qizhe Wei, Zheng Geng, Hong Li",
  "paper_links": [
    {
      "title": "PDF",
      "url": "https://arxiv.org/pdf/2512.23705v1"
    }
  ],
  "generated_at": "2025-12-30T06:22:22.007388"
}