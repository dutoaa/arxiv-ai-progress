{
  "key_insight": "Use KL-divergence to identify which Transformer layers can be converted from softmax to linear attention, enabling compact hybrid attention models that retain performance while improving inference efficiency.",
  "problem_solved": "How to efficiently convert pretrained softmax-attention Transformers into hybrid architectures (mixing softmax and linear attention) without expensive re-pretraining, by deciding which layers to replace to minimize accuracy loss.",
  "method": [
    "Distill a pretrained softmax-attention teacher into candidate hybrid students that interleave softmax and linear attention layers.",
    "Compute layer-wise KL-guided importance scores (KL between teacher and student distributions/outputs) to rank layers by sensitivity.",
    "Select and convert low-importance layers to linear attention while keeping high-importance layers as softmax attention, then fine-tune the distilled hybrid model."
  ],
  "impact": "Provides a simple, low-cost recipe to build faster, memory-efficient Transformer variants from existing pretrained models—useful for deploying LLMs with reduced compute and latency while preserving accuracy.",
  "visual_elements": [
    "Diagram of a Transformer stack showing softmax vs linear attention layers with colors indicating KL-based importance",
    "Bar chart of layer-wise KL scores used to rank and select layers for conversion",
    "Tradeoff plot: accuracy (or perplexity) vs latency/compute comparing original, fully-linear, and KL-guided hybrid models",
    "Flowchart of the distillation pipeline: pretrained teacher → KL scoring → layer selection → hybrid student → fine-tune"
  ],
  "hashtags": [
    "#ModelDistillation",
    "#EfficientAI",
    "#Transformers",
    "#HybridAttention",
    "#ModelCompression"
  ],
  "paper_id": "2025-12-23-arxiv-distilling_to_hybrid_attention_models_via_kl_guided_layer_selection",
  "paper_title": "Distilling to Hybrid Attention Models via KL-Guided Layer Selection",
  "paper_category": "Model",
  "paper_date": "2025-12-23",
  "paper_authors": "Yanhong Li, Songlin Yang, Shawn Tan, Mayank Mishra, Rameswar Panda",
  "paper_links": [
    {
      "title": "PDF",
      "url": "https://arxiv.org/pdf/2512.20569v1"
    }
  ],
  "generated_at": "2025-12-25T06:24:36.734568"
}