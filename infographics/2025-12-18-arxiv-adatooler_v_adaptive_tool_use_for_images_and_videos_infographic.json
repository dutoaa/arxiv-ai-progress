{
  "key_insight": "AdaTooler-V adaptively decides when to invoke vision tools for images and videos, avoiding unnecessary tool calls and thereby improving accuracy and inference efficiency.",
  "problem_solved": "Addresses blind tool-use in open-source multimodal LLMs, where models call vision tools even when they are unnecessary—causing higher latency, compute cost, and degraded performance.",
  "method": [
    "Introduce a lightweight adaptive decision module (gating/classifier) that predicts whether a visual query requires external vision-tool invocation before executing it.",
    "Integrate the decision module into multimodal interleaved chain-of-thought interactions so the model only calls vision tools for queries that benefit from explicit visual reasoning.",
    "Train with cost-aware and performance-aware objectives (e.g., penalties for unnecessary calls and supervision from annotated decisions) to balance accuracy and inference overhead."
  ],
  "impact": "Reduces inference cost and latency while improving downstream task performance for MLLMs working with images and videos—making deployment more practical and scalable for real-world applications.",
  "visual_elements": [
    "Flowchart showing per-query decision: text/video input → adaptive gate → (no tool) or (invoke vision tool) → final answer",
    "Bar chart comparing metrics (accuracy, latency, tool-invocation rate) between baseline (blind tool-use) and AdaTooler-V",
    "Side-by-side example: two prompts (one that needs vision tool, one that doesn't) showing different model behaviors and outputs",
    "Icon set representing images, videos, decision gate, and cost (compute/latency) for quick visual cues"
  ],
  "hashtags": [
    "#MultimodalLLM",
    "#AdaptiveToolUse",
    "#EfficientAI",
    "#VisionModels",
    "#AIInference"
  ],
  "paper_id": "2025-12-18-arxiv-adatooler_v_adaptive_tool_use_for_images_and_videos",
  "paper_title": "AdaTooler-V: Adaptive Tool-Use for Images and Videos",
  "paper_category": "Model",
  "paper_date": "2025-12-18",
  "paper_authors": "Chaoyang Wang, Kaituo Feng, Dongyang Chen, Zhongyu Wang, Zhixun Li",
  "paper_links": [
    {
      "title": "PDF",
      "url": "https://arxiv.org/pdf/2512.16918v1"
    }
  ],
  "generated_at": "2025-12-19T03:53:59.962613"
}