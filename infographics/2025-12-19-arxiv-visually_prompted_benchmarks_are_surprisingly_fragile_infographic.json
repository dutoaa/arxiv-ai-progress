{
  "key_insight": "Benchmarks that use visual prompting (e.g., explicit coordinate markers) can give misleadingly strong results because VLMs are surprisingly fragile to small, seemingly irrelevant changes in those visual prompts. Minor alterations to marker appearance or placement cause large performance drops.",
  "problem_solved": "Identifies and diagnoses a key evaluation gap: determining whether VLMs truly analyze visual content or instead exploit prompt artifacts and textual priors in visually-prompted benchmarks.",
  "method": [
    "Benchmark analysis: evaluate multiple VLMs on existing visually-prompted datasets (e.g., BLINK) to establish baseline behavior.",
    "Systematic perturbations: modify visual prompt cues (marker style, color, size, occlusion, displacement, removal) and measure resulting performance degradation.",
    "Failure-mode attribution: quantify how much model answers rely on prompt artifacts vs. genuine image understanding and characterize common brittleness patterns."
  ],
  "impact": "Reveals that common visual-prompt benchmarks may overestimate VLM visual reasoning, motivating more robust benchmark design and careful interpretation of model capabilities for researchers and practitioners.",
  "visual_elements": [
    "Side-by-side image pair showing original marked image vs. perturbed marker (color/shape/shift) with large accuracy delta annotation",
    "Bar chart of accuracy drop per perturbation type (marker color, size, occlusion, removal, shift)",
    "Flow diagram of the evaluation pipeline: dataset → perturbation → model suite → failure analysis",
    "Icon set summarizing failure modes: 'spurious cue reliance', 'sensitivity to marker style', 'occlusion vulnerability', 'textual prior leakage'"
  ],
  "hashtags": [
    "#VisionLanguageModels",
    "#Benchmarking",
    "#Robustness",
    "#VLMs",
    "#ModelEvaluation"
  ],
  "paper_id": "2025-12-19-arxiv-visually_prompted_benchmarks_are_surprisingly_fragile",
  "paper_title": "Visually Prompted Benchmarks Are Surprisingly Fragile",
  "paper_category": "Evaluation",
  "paper_date": "2025-12-19",
  "paper_authors": "Haiwen Feng, Long Lian, Lisa Dunlap, Jiahao Shu, XuDong Wang",
  "paper_links": [
    {
      "title": "PDF",
      "url": "https://arxiv.org/pdf/2512.17875v1"
    }
  ],
  "generated_at": "2025-12-23T06:25:06.180878"
}