{
  "key_insight": "Visually prompted benchmarks that mark coordinates directly on images (e.g., BLINK-style prompts) can give a misleading view of model vision capabilities: small, semantically irrelevant changes to the visual prompt cause large performance drops, revealing fragility in current VLM evaluations.",
  "problem_solved": "Evaluates whether visual prompting truly isolates visual understanding from textual priors, and reveals that common benchmark designs can be brittle and overestimate model competence.",
  "method": [
    "Systematic stress tests of visually prompted benchmarks: apply controlled perturbations to the visual prompt (marker style, position jitter, occlusion, distractors).",
    "Cross-model evaluation to measure performance sensitivity and identify failure modes where visual cues—not visual reasoning—drive answers.",
    "Quantitative analysis of performance degradation and recommendations for more robust benchmark design and reporting."
  ],
  "impact": "Highlights that many VLM evaluation results may reflect sensitivity to prompt artifacts rather than genuine visual reasoning; practitioners should adopt harder, more robust benchmarks and report sensitivity analyses to get a truthful picture of model capabilities.",
  "visual_elements": [
    "Side-by-side image examples showing original vs. perturbed visual prompts (different marker shapes, occlusions) with model answers and correctness labels.",
    "Bar chart or line plot showing accuracy drop across perturbation types and magnitudes for several VLMs.",
    "Heatmap or sensitivity matrix mapping models × perturbation types to failure severity.",
    "Pipeline diagram illustrating the evaluation protocol: dataset → visual prompt perturbation → model inference → robustness metrics."
  ],
  "hashtags": [
    "#VisionLanguageModels",
    "#Benchmarking",
    "#Robustness",
    "#VLMs",
    "#Evaluation"
  ],
  "paper_id": "2025-12-19-arxiv-visually_prompted_benchmarks_are_surprisingly_fragile",
  "paper_title": "Visually Prompted Benchmarks Are Surprisingly Fragile",
  "paper_category": "Evaluation",
  "paper_date": "2025-12-19",
  "paper_authors": "Haiwen Feng, Long Lian, Lisa Dunlap, Jiahao Shu, XuDong Wang",
  "paper_links": [
    {
      "title": "PDF",
      "url": "https://arxiv.org/pdf/2512.17875v1"
    }
  ],
  "generated_at": "2025-12-22T06:22:14.337130"
}