{
  "key_insight": "C2LLM introduces a Pooling by Multihead Attention (PMA) module that converts token-level causal representations from a Qwen-2.5-Coder backbone into high-quality sequence embeddings, removing the EOS-token bottleneck and enabling stronger contrastive code embeddings in 0.5B and 7B variants.",
  "problem_solved": "EOS-based pooling and naive averaging underuse the LLM's learned causal token representations, creating an information bottleneck for code retrieval and semantic search; C2LLM addresses this by aggregating information from all tokens while preserving pretrained causal features.",
  "method": [
    "Build on Qwen-2.5-Coder backbones to retain causal pretraining representations in 0.5B and 7B model sizes.",
    "Insert a Pooling by Multihead Attention (PMA) module to compute sequence embeddings by attending over all token embeddings instead of relying on EOS or simple pooling.",
    "Train the models with contrastive objectives to produce retrieval-ready code embeddings that distinguish similar vs. dissimilar code examples."
  ],
  "impact": "Practitioners get more discriminative, retrieval-ready code embeddings that improve semantic code search, snippet/clone retrieval, and downstream code understanding tasks, with a drop-in pooling module that leverages existing LLM backbones.",
  "visual_elements": [
    "Architecture diagram: Qwen-2.5-Coder backbone -> PMA pooling -> embedding head (showing 0.5B and 7B variants).",
    "Before vs After illustration: EOS-token pooling (information bottleneck) contrasted with PMA attention pooling across tokens.",
    "Performance chart: retrieval metrics (e.g., recall@k / MRR) comparing C2LLM vs EOS pooling baselines.",
    "Attention heatmap over a code snippet showing PMA attending to key tokens (identifiers, API calls, control flow)."
  ],
  "hashtags": [
    "#CodeEmbeddings",
    "#ContrastiveLearning",
    "#LargeLanguageModels",
    "#CodeSearch",
    "#RepresentationLearning"
  ],
  "paper_id": "2025-12-24-arxiv-c2llm_technical_report_a_new_frontier_in_code_retrieval_via_adaptive_cross_atten",
  "paper_title": "C2LLM Technical Report: A New Frontier in Code Retrieval via Adaptive Cross-Attention Pooling",
  "paper_category": "Model",
  "paper_date": "2025-12-24",
  "paper_authors": "Jin Qin, Zihan Liao, Ziyin Zhang, Hang Yu, Peng Di",
  "paper_links": [
    {
      "title": "PDF",
      "url": "https://arxiv.org/pdf/2512.21332v1"
    }
  ],
  "generated_at": "2025-12-27T06:21:32.032372"
}